@comment{x-kbibtex-encoding=utf-8}

@article{Couso-Moral-2011,
	abstract = {The theory of sets of desirable gambles is a very general model which covers most of the existing theories for imprecise probability as special cases; it has a clear and simple axiomatic justification; and mathematical definitions are natural and intuitive. However, much work remains to be done until the theory of desirable gambles can be considered as generally applicable to reasoning tasks as other approaches to imprecise probability are. This paper gives an overview of some of the fundamental concepts for reasoning with uncertainty expressed in terms of desirable gambles in the finite case, provides a characterization of regular extension, and studies the nature of maximally coherent sets of desirable gambles, which correspond to finite sequences of probability distributions, each one of them defined on the set where the previous one assigns probability zero.},
	author = {Inés Couso and Serafín Moral},
	doi = {10.1016/j.ijar.2011.04.004},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Couso-Moral-2011.pdf},
	number = {7},
	pages = {1034–1055},
	title = {Sets of desirable gambles: conditioning, representation, and precise probabilities},
	volume = {52},
	year = {2011}
}

@incollection{Quaeghebeur-2012-itip,
	author = {Erik Quaeghebeur},
	booktitle = {Introduction to Imprecise Probabilities},
	editor = {Frank P. A. Coolen and Thomas Augustin and Gert {De Cooman} and Matthias C. M. Troffaes},
	publisher = {Wiley},
	title = {Desirability},
	year = {at the editor}
}

@incollection{Morishima-1964-Perron-Frobenius,
	author = {Michio Morishima},
	booktitle = {Equilibrium, stability, and growth},
	chapter = {Appendix},
	edition = {1967},
	localfile = {inbook/Morishima-1964-Perron-Frobenius.pdf},
	note = {ook op papier},
	pages = {195–215},
	publisher = {Oxford University Press},
	title = {Generalizations of the Perron–Frobenius Theorem for nonnegative square matrices},
	year = {1964}
}

@article{DeCooman-Quaeghebeur-2010-Kyburg,
	abstract = {Sets of desirable gambles constitute a quite general type of
uncertainty model with an interesting geometrical interpretation. We give a
general discussion of such models and their rationality criteria. We study
exchangeability assessments for them, and prove counterparts of de Finetti's
finite and infinite representation theorems. We show that the finite
representation in terms of count vectors has a very nice geometrical
interpretation, and that the representation in terms of frequency vectors is
tied up with multivariate Bernstein (basis) polynomials. We also lay bare the
relationships between the representations of updated exchangeable models, and
discuss conservative inference (natural extension) under exchangeability and the
extension of exchangeable sequences.},
	author = {Gert {De Cooman} and Erik Quaeghebeur},
	doi = {10.1016/j.ijar.2010.12.002},
	journal = {International Journal of Approximate Reasoning},
	title = {Exchangeability and sets of desirable gambles},
	year = {in press}
}

@incollection{Dayhoff-Schwartz-Orcutt-1978,
	author = {M. O. Dayhoff and R. M. Schwartz and B. C. Orcutt},
	booktitle = {Atlas of Protein Sequence and Structure},
	chapter = {22},
	editor = {M. O. Dayhoff},
	localfile = {inbook/Dayhoff-Schwartz-Orcutt-1978.pdf},
	pages = {345–352},
	publisher = {National Biomedical Research Foundation},
	title = {A Model of Evolutionary Change in Proteins},
	year = {1978}
}

@misc{Quaeghebeur-2010-SSS,
	author = {Erik Quaeghebeur},
	howpublished = {Lecture at the 4th SIPTA Summer School, Durham, UK},
	title = {Inference \& Desirability},
	url = {http://users.ugent.be/~equaeghe/#EQ-2010-SSS},
	year = {2010}
}

@article{Buckley-1995,
	abstract = {We propose a new solution concept for fuzzy programming
problems. It is based on our new method of solving fuzzy equations [10]. For
simplicity we discuss in detail only fuzzy linear programming in this paper. We
define, and obtain the basic properties of the joint solution (a fuzzy vector in
R^n) and the optimal value of the objective function (a fuzzy number). Three
examples are presented illustrating these concepts.},
	author = {J. J. Buckley},
	doi = {10.1016/0165-0114(94)00353-9},
	issn = {0165-0114},
	journal = {Fuzzy Sets and Systems},
	keywords = {mathematical programming},
	localfile = {article/Buckley-1995.pdf},
	number = {2},
	pages = {215–220},
	title = {Joint solution to fuzzy programming problems},
	volume = {72},
	year = {1995}
}

@article{Peterson-1972-Radon,
	author = {B. B. Peterson},
	journal = {The American Mathematical Monthly},
	localfile = {article/Peterson-1972-Radon.pdf},
	number = {9},
	pages = {949–963},
	title = {The geometry of Radon's theorem},
	url = {http://www.jstor.org/stable/2318065},
	volume = {79},
	year = {1972}
}

@book{BenHaim-2006-info-gap,
	author = {Yakov Ben-Haim},
	edition = {2},
	publisher = {Academic Press},
	title = {Info-Gap Decision Theory: Decisions Under Severe Uncertainty},
	year = {2006}
}

@inproceedings{Mevel-Finesso-2000,
	annote = {ook op papier},
	author = {Laurent Mevel and Lorenzo Finesso},
	booktitle = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
	title = {Bayesian estimation of Hidden Markov Models},
	year = {2000}
}

@article{Fagiuoli-Zaffalon-1998-2U,
	abstract = {This paper addresses the problem of computing posterior
probabilities in a discrete Bayesian network where the conditional distributions
of the model belong to convex sets. The computation on a general Bayesian
network with convex sets of conditional distributions is formalized as a global
optimization problem. It is shown that such a problem can be reduced to a
combinatorial problem, suitable to exact algorithmic solutions. An exact
propagation algorithm for the updating of a polytree with binary variables is
derived. The overall complexity is linear to the size of the network, when the
maximum number of parents is fixed.},
	author = {Enrico Fagiuoli and Marco Zaffalon},
	doi = {10.1016/S0004-3702(98)00089-7},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Bayesian networks; convex sets; credal sets; intervals; uncertain reasoning},
	localfile = {article/Fagiuoli-Zaffalon-1998-2U.pdf},
	month = nov,
	number = {1},
	pages = {77–107},
	publisher = {Elsevier},
	title = {2U: an exact interval propagation algorithm for polytrees with binary variables},
	volume = {106},
	year = {1998}
}

@article{Edwards-1983-Pascal,
	author = {A. W. F. Edwards},
	journal = {International Statistical Review},
	localfile = {article/Edwards-1983-Pascal.pdf},
	pages = {73–79},
	title = {Pascal's Problem: The ‘Gambler's Ruin’},
	url = {http://www.jstor.org/stable/1402732},
	volume = {51},
	year = {1983}
}

@book{Friedman-1989feron,
	annote = {geannoteerde uittreksels},
	author = {James W. Friedman},
	keywords = {game theory; mathematical economics},
	publisher = {Oxford University Press},
	title = {Game Theory with Applications to Economics},
	year = {1989}
}

@article{Hipp-1974,
	author = {Christian Hipp},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Hipp-1974.pdf},
	month = nov,
	number = {6},
	pages = {1283–1292},
	title = {Sufficient statistics and exponential families},
	url = {http://www.jstor.org/stable/2958344},
	volume = {2},
	year = {1974}
}

@article{Dempster-1968,
	abstract = {Procedures of statistical inference are described which
generalize Bayesian inference in specific ways. Probability is used in such a
way that in general only bounds may be placed on the probabilities of given
events, and probability systems of this kind are suggested both for sample
information and for prior information. These systems are then combined using a
specified rule. Illustrations are given for inferences about trinomial
probabilities, and for inferences about a monotone sequence of binomial p\_i.
Finally, some comments are made on the general class of models which produce
upper and lower probabilities, and on the specific models which underlie the
suggested inference procedures.},
	annote = {with discussion},
	author = {Arthur P. Dempster},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Dempster-1968.pdf},
	number = {2},
	pages = {205–247},
	title = {A generalization of Bayesian inference},
	url = {http://www.jstor.org/stable/2984504},
	volume = {30},
	year = {1968}
}

@article{Koopman-1940-ams,
	author = {B. O. Koopman},
	journal = {Bulletin of the American Mathematical Society},
	localfile = {article/Koopman-1940-ams.pdf},
	number = {10},
	pages = {763–774},
	title = {The bases of probability},
	url = {http://www.ams.org/bull/1940-46-10/S0002-9904-1940-07294-5/S0002-9904-1940-0729
4-5.pdf; http://projecteuclid.org/euclid.bams/1183503229},
	volume = {46},
	year = {1940}
}

@article{Cozman-2005-graphical,
	abstract = {This paper presents an overview of graphical models that can
handle imprecision in probability values. The paper first reviews basic concepts
and presents a brief historical account of the field. The main characteristics
of the credal network model are then discussed, as this model has received
considerable attention in the literature.},
	author = {Fabio Gagliardi Cozman},
	doi = {10.1016/j.ijar.2004.10.003},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Credal network; Graphical models; Sets of probability distributions; imprecise probability},
	localfile = {article/Cozman-2005-graphical.pdf},
	number = {2-3},
	pages = {167–184},
	title = {Graphical models for imprecise probabilities},
	volume = {39},
	year = {2005}
}

@book{Liu-2007-uncertainty-theory,
	author = {Baoding Liu},
	edition = {3},
	title = {Uncertainty Theory},
	year = {2007}
}

@article{Ha-etal-1998,
	author = {Vu A. Ha and AnHai Doan and Van H. Vu and Peter Haddawy},
	doi = {10.1023/A:1018936829318},
	journal = {Annals of Mathematics and Artificial Intelligence},
	localfile = {article/Ha-etal-1998.pdf},
	pages = {1–21},
	title = {Geometric foundations for interval-based probabilities},
	volume = {24},
	year = {1998}
}

@phdthesis{Augustin-1998-phdthesis-parts,
	annote = {Extracts},
	author = {Thomas Augustin},
	isbn = {978-3-52511411-7},
	school = {LMU München},
	title = {Optimale Tests bei Intervallwahrscheinlichkeit},
	year = {1998}
}

@article{Daboni-1975,
	author = {Luciano Daboni},
	journal = {Rendiconti di matematica},
	localfile = {article/Daboni-1975.pdf},
	pages = {399–412},
	title = {Caratterizzatione delle successioni (funzioni) completamente monotone in termini di rappresentabilià delle funzioni di sopravvivenza di particolari intervalli scambiabli tra successi (arrivi) contigui},
	volume = {8},
	year = {1975}
}

@article{Zaffalon-2002-ncc,
	abstract = {Convex sets of probability distributions are also called
credal sets. They generalize probability theory by relaxing the requirement that
probability values be precise. Classification, i.e. assigning class labels to
instances described by a set of attributes, is an important domain of
application of Bayesian methods, where the naive Bayes classifier has a
surprisingly good performance. This paper proposes a new method of
classification which involves extending the naive Bayes classifier to credal
sets. Exact and effective solution procedures for naive credal classification
are derived, and the related dominance criteria are discussed. Credal
classiffcation appears as a new method, based on more realistic assumptions and
in the direction of more reliable inferences.},
	annote = {ook op papier},
	author = {Marco Zaffalon},
	doi = {10.1016/S0378-3758(01)00201-4},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Classification; Credal sets; Imprecise probabilities; Naive Bayes classifier; Pattern recognition},
	localfile = {article/Zaffalon-2002-ncc.pdf},
	number = {1},
	pages = {5–21},
	publisher = {Elsevier},
	title = {The naive credal classifier},
	volume = {105},
	year = {2002}
}

@article{Zaffalon-2005-environmental,
	abstract = {Classifiers that aim at doing credible predictions should
rely on carefully elicited prior knowledge. Often this is not available so they
should start learning from data in condition of near-ignorance. This paper shows
empirically, on an agricultural data set, that established methods of
classification do not always adhere to this principle. Traditional ways to
represent prior ignorance are shown to have an overwhelming weight compared to
the information in the data, producing overconfident predictions. This point is
crucial for problems, such as environmental ones, where prior knowledge is often
scarce and even the data may not be known precisely. Credal classification, and
in particular the naive credal classifier, is proposed as more faithful ways to
cope with the ignorance problem. With credal classification, conditions of
ignorance may limit the power of the inferences, not the credibility of the
predictions.},
	author = {Marco Zaffalon},
	doi = {10.1016/j.envsoft.2004.10.006},
	journal = {Environmental Modelling \& Software},
	number = {8},
	pages = {1003–1012},
	title = {Credible classification for environmental problems},
	volume = {20},
	year = {2005}
}

@book{Kuznetsov-1991,
	address = {Moscow},
	author = {Kuznetsov},
	title = {No Title},
	year = {1991}
}

@book{Zabell-2005,
	address = {Cambridge, United Kingdom},
	author = {Sandy L. Zabell},
	publisher = {Cambridge University Press},
	series = {Cambridge Studies in Probability, Induction, and Decision Theory},
	title = {Symmetry and Its Discontents: Essay on the History of Inductive Probability},
	year = {2005}
}

@article{Combarro-Miranda-2008-polytope,
	abstract = {In this paper we deal with the problem of studying the
structure of the polytope of non-additive measures for finite referential sets.
We give a necessary and sufficient condition for two extreme points of this
polytope to be adjacent. We also show that it is possible to find out in
polynomial time whether two vertices are adjacent. These results can be extended
to the polytope given by the convex hull of monotone Boolean functions. We also
give some results about the facets and edges of the polytope of non-additive
measures; we prove that the diameter of the polytope is 3 for referentials of
three elements or more. Finally, we show that the polytope is combinatorial and
study the corresponding properties; more concretely, we show that the graph of
non-additive measures is Hamilton connected if the cardinality of the
referential set is not 2.},
	author = {Elías F. Combarro and Pedro Miranda},
	doi = {10.1016/j.fss.2007.12.021},
	journal = {Fuzzy Sets and Systems},
	keywords = {Adjacency; Combinatorial polytopes; Complexity; Diameter; Monotone Boolean functions; Non-additive measures; Stack filters},
	localfile = {article/Combarro-Miranda-2008-polytope.pdf},
	number = {16},
	pages = {2145–2162},
	title = {On the polytope of non-additive measures},
	url = {http://www.sciencedirect.com/science/article/B6V05-4RM881N-1/1/385e96ea2df064e7
ab875367eafbf9f9},
	volume = {159},
	year = {2008}
}

@article{Tatcher-1964,
	abstract = {Given the number of successes in a random sample, prediction
limits can be determined for the number which will be observed in a second
sample, in a way which does not depend on any assumption or inference about the
unknown proportion in the population. Such "confidence limits" for the
prediction are found to correspond to Bayesian solutions based on two particular
prior distributions, and are related to Laplace's rule of succession. The
results suggest a possible type of "prediction strategy".},
	author = {A. R. Thatcher},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Tatcher-1964.pdf},
	number = {2},
	pages = {176–210},
	title = {Relationships between Bayesian and confidence limits for predictions},
	url = {http://www.jstor.org/stable/2984417},
	volume = {26},
	year = {1964}
}

@article{Aughenbaugh-Herrmann-2009,
	abstract = {This paper considers the problem of choosing between an
existing component whose reliability is well established and a new component
that has an unknown reliability. In some scenarios, the designer may have some
initial beliefs about the new component's reliability. The designer may also
have the opportunity to obtain more information and to update these beliefs.
Then, based on these updated beliefs, the designer must make a decision between
the two components. This paper examines the statistical approaches for updating
reliability assessments and the decision policy that the designer uses. We
consider four statistical approaches for modeling the uncertainty about the new
component and updating assessments of its reliability: A classical approach, a
precise Bayesian approach, a robust Bayesian approach, and an imprecise
probability approach. The paper investigates the impact of different approaches
on the decision between the components and compares them. In particular, given
that the test results are random, the paper considers the likelihood of making a
correct decision with each statistical approach under different scenarios of
available information and true reliability. In this way, the emphasis is on
practical comparisons of the policies rather than on philosophical arguments.},
	author = {J. M. Aughenbaugh and J. W. Herrmann},
	doi = {10.1080/15598608.2009.10411926},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bayesian statistics; Imprecise probabilities; Reliability assessment},
	localfile = {article/Aughenbaugh-Herrmann-2009.pdf},
	month = mar,
	number = {1},
	pages = {289–303},
	publisher = {Taylor \& Francis},
	title = {Reliability-Based Decision Making: A Comparison of Statistical Approaches},
	volume = {3},
	year = {2009}
}

@book{Handbook-Beta-2003,
	editor = {A. K. Gupta and S. Nadarajah},
	publisher = {Marcel Dekker},
	title = {Handbook of the Beta Distribution and its Applications},
	year = {2003}
}

@article{Kozine-Utkin-2002,
	abstract = {The requirement that precise state and transition
probabilities be available is often not realistic because of cost, technical
difficulties or the uniqueness of the situation under study. Expert judgements,
generic data, heterogeneous and partial information on the occurrences of events
may be sources of the probability assessments. All this source information
cannot produce precise probabilities of interest without having to introduce
drastic assumptions often of quite an arbitrary nature. in this paper the theory
of interval-valued coherent previsions is employed to generalise discrete Markov
chains to interval-valued probabilities. A general procedure of interval-valued
probability elicitation is analysed as well. In addition, examples are
provided.},
	annote = {ook offprint},
	author = {Igor O. Kozine and Lev V. Utkin},
	doi = {10.1023/A:1014745904458},
	journal = {Reliable Computing},
	localfile = {article/Kozine-Utkin-2002.pdf},
	pages = {97–113},
	title = {Interval-Valued Finite Markov Chains},
	volume = {8},
	year = {2002}
}

@inproceedings{Chrisman-1996,
	author = {Lonnie Chrisman},
	booktitle = {Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence},
	publisher = {Morgan Kaufmann},
	title = {Propagation of 2-Monotone Lower Probabilities on an Undirected Graph},
	year = {1996}
}

@article{DeFinetti-1933c,
	author = {Bruno de Finetti},
	journal = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
	localfile = {article/DeFinetti-1933c.pdf},
	pages = {279–284},
	title = {Sulla legge di distribuzione dei valori in una successione di numeri aleatori equivalenti},
	volume = {18},
	year = {1933}
}

@article{Gilbert-DeCooman-Kerre-2003,
	abstract = {Probability assessments of events are often linguistic in
nature. We model them by means of possibilistic probabilities (a version of
Zadeh's fuzzy probabilities with a behavioural interpretation) with a suitable
shape for practical implementation (on a computer). Employing the tools of
interval analysis and the theory of imprecise probabilities we argue that the
verification of coherence for these possibilistic probabilities, the corrections
of non-coherent to coherent possibilistic probabilities and their extension to
other events and gambles can be performed by finite and exact algorithms. The
model can furthermore be transformed into an imprecise first-order model, useful
for decision making and statistical inference.},
	annote = {ook op papier},
	author = {L. Gilbert and Gert {De Cooman} and Etienne E. Kerre},
	doi = {10.1007/s00500-002-0217-3},
	journal = {Soft Computing},
	keywords = {Fuzzy number; Fuzzy probability; Linguistic uncertainty; Lowest prevision; Possibility measure},
	localfile = {article/Gilbert-DeCooman-Kerre-2003.pdf},
	pages = {304–309},
	title = {Practical implementation of possibilistic probability mass functions},
	volume = {7},
	year = {2003}
}

@article{DeFinetti-1933a,
	author = {Bruno de Finetti},
	journal = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
	localfile = {article/DeFinetti-1933a.pdf},
	pages = {279–284},
	title = {Classi di numeri aleatori equivalenti},
	volume = {18},
	year = {1933}
}

@article{Munch-Krogh-2006,
	abstract = {BACKGROUND: The number of sequenced eukaryotic genomes is
rapidly increasing. This means that over time it will be hard to keep supplying
customised gene finders for each genome. This calls for procedures to
automatically generate species-specific gene finders and to re-train them as the
quantity and quality of reliable gene annotation grows. RESULTS: We present a
procedure, Agene, that automatically generates a species-specific gene predictor
from a set of reliable mRNA sequences and a genome. We apply a Hidden Markov
model (HMM) that implements explicit length distribution modelling for all gene
structure blocks using acyclic discrete phase type distributions. The state
structure of the each HMM is generated dynamically from an array of sub-models
to include only gene features represented in the training set. CONCLUSION:
Acyclic discrete phase type distributions are well suited to model sequence
length distributions. The performance of each individual gene predictor on each
individual genome is comparable to the best of the manually optimised
species-specific gene finders. It is shown that species-specific gene finders
are superior to gene finders trained on other species.},
	author = {Kasper Munch and Anders Krogh},
	doi = {10.1186/1471-2105-7-263},
	issn = {1471-2105},
	journal = {BMC Bioinformatics},
	localfile = {article/Munch-Krogh-2006.pdf},
	number = {1},
	pages = {263},
	publisher = {BioMed Central Ltd},
	title = {Automatic generation of gene finders for eukaryotic species},
	volume = {7},
	year = {2006}
}

@article{Basu-Pereira-1983b,
	abstract = {The theory of conditional independence is explained and the
relations between ancillarity, sufficiency and statistical independence are
discussed in depth. Some related concepts like specific sufficiency, bounded
completeness, and splitting sets are also studied in some details by using the
language of conditional independence.},
	annote = {ook op papier},
	author = {D. Basu and Carlos A. B. Pereira},
	journal = {Sankhya Series A},
	keywords = {(strong) identification; Conditional independence; Markov property; ancillarity; measuable separability; specific sufficiency; splitting sets; sufficiency; variation independence},
	localfile = {article/Basu-Pereira-1983b.pdf},
	number = {3},
	pages = {324–337},
	title = {Conditional independence in statistics},
	url = {http://www.jstor.org/stable/25050444},
	volume = {45},
	year = {1983}
}

@article{Klee-1951,
	author = {Victor L. Jr. Klee},
	doi = {10.1215/S0012-7094-51-01835-2},
	journal = {Duke Mathematical Journal},
	number = {2},
	pages = {443–466},
	title = {Convex sets in linear spaces},
	volume = {18},
	year = {1951}
}

@article{Miranda-Grabisch-Gil-2002,
	archiveprefix = {arXiv},
	arxivid = {0804.2642},
	author = {Pedro Miranda and Michel Grabisch and Pedro Gil},
	doi = {10.1142/S0218488502001867},
	eprint = {0804.2642},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	localfile = {article/Miranda-Grabisch-Gil-2002.pdf},
	number = {Supplementary Issue 1},
	pages = {105–123},
	title = {p-Symmetric fuzzy measures},
	volume = {10},
	year = {2002}
}

@incollection{VanDorp-Mazzuchi-2003,
	author = {J. René {Van Dorp} and Thomas A. Mazzuchi},
	booktitle = {Handbook of the Beta Distribution and its Applications},
	editor = {A. K. Gupta and S. Nadarajah},
	localfile = {inbook/VanDorp-Mazzuchi-2003.pdf},
	pages = {283–316},
	publisher = {Marcel Dekker},
	title = {Parameter Specification of the Beta Distribution and its Dirichlet Extensions Utilizing Quantiles},
	year = {2003}
}

@article{Fodor-Marichal-Roubens-1995,
	author = {Janos Fodor and Jean-Luc Marichal and Marc Roubens},
	journal = {IEEE Transactions on Fuzzy Systems},
	localfile = {article/Fodor-Marichal-Roubens-1995.pdf},
	number = {2},
	pages = {236–240},
	title = {Characterization of the Ordered Weighted Averaging Operators},
	volume = {3},
	year = {1995}
}

@article{Pericchi-Walley-1991,
	author = {Luis Raúl Pericchi and Peter Walley},
	journal = {International Statistical Review},
	localfile = {article/Pericchi-Walley-1991.pdf},
	number = {1},
	pages = {1–23},
	title = {Robust Bayesian credible intervals and prior ignorance},
	url = {http://www.jstor.org/stable/1403571},
	volume = {58},
	year = {1991}
}

@article{Fishburn-1980,
	abstract = {Stochastic dominance orders of all finite degrees are
defined on the set of distribution functions on the nonnegative real numbers in
terms of integrals of the distributions. It is proved that if F strictly
nth-degree stochastically dominates G, and if the moments of F and G through
order n are finite with $\mu \_{F}^{k}-∈t x^{k}dF(x)$, then $(\mu
\_{F}^{1},…,\mu \_{F}^{n})\neq (\mu \_{G}^{n},…,\mu \_{G}^{n})$ and
$(-1)^{k-1}\mu \_{F}^{k}>(-1)^{k-1}\mu \_{G}^{k}$ for the smallest k for which
$\mu \_{F}^{k}\neq \mu \_{G}^{k}$.},
	author = {Peter C. Fishburn},
	issn = {0364-765X},
	journal = {Mathematics of Operations Research},
	localfile = {article/Fishburn-1980.pdf},
	number = {1},
	pages = {94–100},
	publisher = {INFORMS},
	title = {Stochastic dominance and moments of distributions},
	url = {http://www.jstor.org/stable/3689397},
	volume = {5},
	year = {1980}
}

@article{Walley-1996-IDM,
	abstract = {A new method is proposed for making inferences from
multinomial data in cases where there is no prior information. A paradigm is the
problem of predicting the colour of the next marble to be drawn from a bag whose
contents are (initially) completely unknown. In such problems we may be unable
to formulate a sample space because we do not know what outcomes are possible.
This suggests an invariance principle: inferences based on observations should
not depend on the sample space in which the observations and future events of
interest are represented. Objective Bayesian methods do not satisfy this
principle. This paper describes a statistical model, called the imprecise
Dirichlet model, for drawing coherent inferences from multinomial data.
Inferences are expressed in terms of posterior upper and lower probabilities.
The probabilities are initially vacuous, reflecting prior ignorance, but they
become more precise as the number of observations increases. This model does
satisfy the invariance principle. Two sets of data are analysed in detail. In
the first example one red marble is observed in six drawings from a bag.
Inferences from the imprecise Dirichlet model are compared with objective
Bayesian and frequentist inferences. The second example is an analysis of data
from medical trials which compared two treatments for cardiorespiratory failure
in newborn babies. There are two problems: to draw conclusions about which
treatment is more effective and to decide when the randomized trials should be
terminated. This example shows how the imprecise Dirichlet model can be used to
analyse data in the form of a contingency table.},
	annote = {with discussion geannoteerde kopie},
	author = {Peter Walley},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Walley-1996-IDM.pdf},
	number = {1},
	pages = {3–57},
	title = {Inferences from multinomial data: learning about a bag of marbles},
	url = {http://www.jstor.org/stable/2346164},
	volume = {58},
	year = {1996}
}

@article{Bushell-1986-Hilbert-metric,
	abstract = {The Cayley-Hilbert metric is defined for a real Banach space
containing a closed cone. By restricting the domain of a particular type of
positive nonlinear operator, the Banach contraction-mapping theorem is used to
prove the existence of a unique fixed point of the operator with explicit upper
and lower bounds. Applications to quasilinear elliptic partial differential
equations and to matrix theory are considered.},
	author = {P. J. Bushell},
	doi = {10.1016/0024-3795(86)90319-8},
	journal = {Linear Algebra and its Applications},
	localfile = {article/Bushell-1986-Hilbert-metric.pdf},
	pages = {271–280},
	publisher = {Elsevier},
	title = {The Cayley-Hilbert metric and positive operators},
	volume = {84},
	year = {1986}
}

@misc{DeCooman-2004-summer,
	annote = {Slides voor inleidende presentatie SIPTA summer school},
	author = {Gert {De Cooman}},
	title = {Coherent lower and upper previsions (and their behavioural interpretation)},
	year = {2004}
}

@book{deFinetti-1992,
	author = {Bruno de Finetti},
	editor = {Paola Monari and Daniela Cocchi},
	publisher = {CLUEB, Bologna},
	title = {Probabilità e Induzione – Induction and Probability},
	url = {http://diglib.cib.unibo.it/diglib.php?inv=35&term_ptnum=1&format=jpg},
	volume = {52},
	year = {1992}
}

@article{Bot-Lorenz-Wanka-2010,
	author = {Radu Ioan Bot and Nicole Lorenz and Gert Wanka},
	doi = {10.4134/JKMS.2010.47.1.017},
	journal = {Journal of The Korean Mathematical Society},
	localfile = {article/Bot-Lorenz-Wanka-2010.pdf},
	pages = {17–28},
	title = {Duality for linear chance-constrained optimization problems},
	volume = {47},
	year = {2010}
}

@book{Levi-1980,
	address = {London},
	author = {Isaac Levi},
	publisher = {MIT Press},
	title = {The Enterprise of Knowledge},
	year = {1980}
}

@article{Lambrakis1969,
	author = {D. P. Lambrakis},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Ericson-1969.pdf},
	number = {2},
	pages = {234–245},
	title = {Experiments with mixtures: an alternative to the simplex-lattice design},
	url = {http://www.jstor.org/stable/2984207},
	volume = {31},
	year = {1969}
}

@article{Rosenthal-1995-Markov-rate,
	abstract = {This is an expository paper that presents various ideas
related to nonasymptotic rates of convergence for Markov chains. Such rates are
of great importance for stochastic algorithms that are widely used in statistics
and in computer science. They also have applications to analysis of card
shuffling and other areas.In this paper, we attempt to describe various
mathematical techniques that have been used to bound such rates of convergence.
In particular, we describe eigenvalue analysis, random walks on groups,
coupling, and minorization conditions. Connections are made to modern areas of
research wherever possible. Elements of linear algebra, probability theory,
group theory, and measure theory are used, but efforts are made to keep the
presentation elementary and accessible.},
	author = {Jeffrey S. Rosenthal},
	doi = {10.1137/1037083},
	journal = {SIAM Review},
	keywords = {Markov chain; coupling; eigenvalue; random walk on group},
	localfile = {article/Rosenthal-1995-Markov-rate.pdf},
	number = {3},
	pages = {387–405},
	title = {Convergence rates for Markov chains},
	url = {http://www.jstor.org/stable/2132659},
	volume = {37},
	year = {1995}
}

@misc{Doumont-2001-website,
	author = {Jean-luc Doumont},
	title = {Designing Web sites},
	year = {2001}
}

@inproceedings{Capotorti-Zagoraiou-2006,
	address = {Paris},
	author = {Andrea Capotorti and Maroussa Zagoraiou},
	booktitle = {Proceedings of the Eleventh International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
	title = {Implicit Degree of Support for Finite Lower-Upper Conditional Probabilities Extensions},
	year = {2006}
}

@article{Bloch-Watson-1967,
	abstract = {Lindley [6] studies the topic in our title. By using
Fisher's conditional-Poisson approach to the multinomial and the logarithmic
transformation of gamma variables to normality, he showed that linear contrasts
in the logarithms of the cell probabilities $\theta$\_i are asymptotically
jointly normal and suggested that the approximation can be improved by applying
a "correction" to the sample. By studying the asymptotic series for the joint
distribution in Section 2 an improved correction procedure is found below. A
more detailed expansion is given in Section 3 for the distribution of a single
contrast in the \log $\theta$\_i. In many problems a linear function of the
$\theta$\_i is of interest. The exact distribution is obtained and is of a form
familiar in the theory of serial correlation coefficients. A beta approximation
is given. For three cells, a numerical example is given to show the merit of
this approximation. A genetic linkage example is considered which requires the
joint distribution of two linear functions of the $\theta$\_i. The exact joint
distribution is found but is too involved for practical use. A normal
approximation leads to Lindley's results [7].},
	author = {Daniel A. Bloch and Geoffrey S. Watson},
	journal = {The Annals of Mathematical Statistics},
	localfile = {article/Bloch-Watson-1967.pdf},
	number = {5},
	pages = {1423–1435},
	title = {A Bayesian study of the multinomial distribution},
	url = {http://www.jstor.org/stable/2238958},
	volume = {38},
	year = {1967}
}

@misc{Good-2003,
	author = {I. J. Good},
	title = {The accumulation of imprecise weights of evidence},
	year = {2003}
}

@article{Zabell-1982,
	abstract = {How do Bayesians justify using conjugate priors on grounds
other than mathematical convenience? In the 1920's the Cambridge philosopher
William Ernest Johnson in effect characterized symmetric Dirichlet priors for
multinomial sampling in terms of a natural and easily assessed subjective
condition. Johnson's proof can be generalized to include asymmetric Dirichlet
priors and those finitely exchangeable sequences with linear posterior
expectation of success. Some interesting open problems that Johnson's result
raises, and its historical and philosophical background, are also discussed.},
	annote = {ook op papier},
	author = {Sandy L. Zabell},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Zabell-1982.pdf},
	number = {4},
	pages = {1091–1099},
	publisher = {Institute of Mathematical Statistics},
	title = {W. E. Johnson's “sufficientness” postulate},
	volume = {10},
	year = {1982}
}

@article{Cifarelli-Regazzini-1996,
	abstract = {This paper summarizes the scientific activity of de Finetti
in probability and statistics. It falls into three sections: Section 1 includes
an essential biography of de Finetti and a survey of the basic features of the
scientific milieu in which he took the first steps of his scientific career;
Section 2 concerns de Finetti's work in probability: (a) foundations, (b)
processes with independent increments, (c) sequences of exchangeable random
variables, and (d) contributions which fall within other fields; Section 3 deals
with de Finetti's contributions to statistics: (a) description of frequency
distributions, (b) induction and statistics, (c) probability and induction, and
(d) objectivistic schools and theory of decision. Many recent developments of de
Finetti's work are mentioned here and briefly described.},
	author = {Donato Michele Cifarelli and Eugenio Regazzini},
	doi = {10.1214/ss},
	journal = {Statistical Science},
	keywords = {Associative mean; Bayes-Laplace paradigm; Bayesian nonparametric statistics; Glivenko-Cantelli theorem; completely additive probabilities; correlation and monotone dependence; exchangeable and partially exchangeable random var; finitely additive probabilities; gambler's ruin; infinitely decomposable laws; predictive inference; prevision; principle of coherence processes with independent; reasoning by induction; statistical decision; subjective probability; utility function},
	localfile = {article/Cifarelli-Regazzini-1996.pdf},
	number = {4},
	pages = {253–282},
	title = {De Finetti's Contribution to probability and Statistics},
	volume = {11},
	year = {1996}
}

@proceedings{ISIPTA-2003,
	address = {Waterloo, Ontario, Canada},
	booktitle = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Jean-Marc Bernard and Teddy Seidenfeld and Marco Zaffalon},
	location = {Lugano, Switzerland},
	publisher = {Carleton Scientific},
	series = {Proceedings in Informatics},
	title = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
	volume = {18},
	year = {2003}
}

@article{Rommelfanger-2004,
	abstract = {Classical mathematical programming models require
well-defined coefficients and right hand sides. In order to avoid a non
satisfying modeling usually a broad information gathering and processing is
necessary. In case of real problems some model parameters can be only roughly
estimated. While in case of classical models the vague data is replaced by
"average data", fuzzy models offer the opportunity to model subjective
imaginations of the decision maker as precisely as a decision maker will be able
to describe it. Thus the risk of applying a wrong model of the reality and
selecting solutions which do not reflect the real problem can be clearly
reduced. The modeling of real problems by means of deterministic and stochastic
models requires extensive information processing. On the other hand we know that
an optimum solution is finally defined only by few restrictions. Especially in
case of larger systems we notice afterwards that most of the information is
useless. The dilemma of data processing is due to the fact that first we have to
calculate the solution in order to define, whether the information must be
well-defined or whether vague data may be sufficient. Based on multicriteria
programming problems it should be demonstrated that the dilemma of data
processing in case of real programming problems can be handled adequately by
modeling them as fuzzy system combined with an interactive problem-solving.
Describing the real problem by means of a fuzzy system first of all only the
available information or such information which can be achieved easily will be
considered. Then we try to develop an optimum solution. With reference to the
cost-benefit relation further information can be gathered in order to describe
the solution more precisely. Furthermore it should be pointed out that some
interactive fuzzy solution algorithms, e.g. FULPAL provide the opportunity to
solve mixed integer multicriteria programming models as well.},
	author = {Heinrich Rommelfanger},
	doi = {10.1007/s10700-004-4200-6},
	issn = {1568-4539},
	journal = {Fuzzy Optimization and Decision Making},
	localfile = {article/Rommelfanger-2004.pdf},
	pages = {295–309},
	publisher = {Springer Netherlands},
	title = {The Advantages of Fuzzy Optimization Models in Practical Use},
	volume = {3},
	year = {2004}
}

@incollection{Mukerji-Tallon-2003,
	author = {Sujoy Mukerji and Jean-Marc Tallon},
	booktitle = {Uncertainty in Economic Theory: A collection of essays in honor of David Schmeidler's 65th birthday},
	editor = {I. Gilboa},
	publisher = {Routledge},
	title = {An overview of economic applications of David Schmeidler's models of decision making under uncertainty},
	year = {2004}
}

@article{Smith-1961,
	abstract = {It is suggested that the strength of a person's beliefs may
be tested by finding at what odds he is prepared to bet on them. This leads to a
system of numerical "medial personal probabilities" obeying the classical laws
of probability. However, these do not have precisely defined values, but are
contained within specified intervals. The appropriate method of inference is
Bayes's Theorem. This leads to generally accepted statistical procedures in
large samples, except that the "weight of evidence" and not significance level
is the measure of conviction in a significance test. Under very general
conditions decisions are made by maximizing expected utility.},
	annote = {geannoteerde kopie op papier},
	author = {Cedric A. B. Smith},
	issn = {0035-9246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Smith-1961.pdf},
	number = {1},
	pages = {1–37},
	publisher = {Blackwell Publishing for the Royal Statistical Society},
	title = {Consistency in statistical inference and decision},
	url = {http://www.jstor.org/stable/2983842},
	volume = {23},
	year = {1961}
}

@book{Kemeny-Snell-1976-markov,
	address = {New York and Berlin and Heidelberg and Tokyo},
	author = {John G. Kemeny and J. Laurie Snell},
	edition = {2},
	publisher = {Springer},
	series = {Undergraduate Texts in Mathematics},
	title = {Finite Markov Chains},
	url = {http://books.google.com/books?id=0bTK5uWzbYwC},
	year = {1976}
}

@inproceedings{Lee-Varaiya-2000-sysensig,
	annote = {ook op papier},
	author = {Edward A. Lee and Pravin Varaiya},
	booktitle = {Proceedings of the First Signal Processing Workshop},
	title = {Introducing signals and syetems – The Berkeley approach},
	year = {2000}
}

@article{Williams-1978,
	annote = {Review of Shafer-1976},
	author = {Peter M. Williams},
	journal = {The British Journal for the Philosophy of Science},
	localfile = {article/Williams-1978.pdf},
	number = {4},
	pages = {375–387},
	title = {On a new theory of epistemic probability},
	url = {http://www.jstor.org/stable/687102},
	volume = {29},
	year = {1978}
}

@article{Luce-VonWinterfeld-1994,
	abstract = {Descriptive and normative modeling of decision making under
risk and uncertainty have grown apart over the past decade. Psychological models
attempt to accommodate the numerous violations of rationality axioms, including
independence and transitivity. Meanwhile, normatively oriented decision analysts
continue to insist on the applied usefulness of the subjective expected utility
(SEU) model. As this gap has widened, two facts have remained largely
unobserved. First, most people in real situations attempt to behave in accord
with the most basic rationality principles, even though they are likely to fail
in more complex situations. Second, the SEU model is likely to provide
consistent and rational answers to decision problems within a given problem
structure, but may not be invariant across structures. Thus, people may be more
rational than the psychological literature gives them credit for, and
applications of the SEU model may be susceptible to some violations of
invariance principles. This paper attempts to search out the common ground
between the normative, descriptive, and prescriptive modeling by exploring three
types of axioms concerning structural rationality, preference rationality, and
quasi-rationality. Normatively the first two are mandatory and the last,
suspect. Descriptively, all have been questioned, but often the inferences
involved have confounded preference and structural rationality. We propose a
prescriptive view that entails full compliance with preference rationality,
modifications of structural rationality, and acceptance of quasi-rationality to
the extent of granting a primary role to the status quo and the decomposition of
decision problems into gains and losses.},
	author = {R. Duncan Luce and Detlof von Winterfeldt},
	doi = {10.1287/mnsc.40.2.263},
	journal = {Management Science},
	keywords = {Decision Analysis; Prescriptive Utility; Rank-Dependent Utility; Sign-Dependent Utility},
	localfile = {article/Luce-VonWinterfeld-1994.pdf},
	number = {2},
	pages = {263–279},
	title = {What common ground exists for descriptive, prescriptive, and normative utility theories?},
	url = {http://www.jstor.org/stable/2632765},
	volume = {40},
	year = {1994}
}

@article{Whittle-1955,
	abstract = {An exact formula (8) is derived for the probability
distribution of an observed set of transition totals. This expression furnishes
asymptotic expressions for the likelihood (24), for the covariances of
transition totals (27), and for the distribution of Bartlett's goodness-of-fit
statistic (31). Formulae are also derived for the expectations of some sample
functions related to the factorial moments of the transition totals (34) and for
the lower moments of the estimated transition probabilities (42), (44). It is
shown that the Markov chain has properties similar to those of a set of
independent multinomial distributions.},
	annote = {geannoteerde kopie},
	author = {Peter Whittle},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Whittle-1955.pdf},
	number = {2},
	pages = {235–242},
	title = {Some distribution and moment formulae for the Markov chain},
	url = {http://www.jstor.org/stable/2983957},
	volume = {17},
	year = {1955}
}

@techreport{Deshpande-Karypis-2000,
	author = {Mukund Deshpande and George Karypis},
	institution = {University of Minnesota, Department of Computer Science},
	number = {00-056},
	title = {Selective Markov Models for Predicting Web-Page Accesses},
	year = {2000}
}

@phdthesis{Degrauwe-2007-PhD,
	author = {Daan Degrauwe},
	school = {Katholieke Universiteit Leuven},
	title = {Uncertainty propagation in structural analysis by fuzzy numbers},
	year = {2007}
}

@article{GuitierrezPena-Smith-1995,
	annote = {met errata, ook op papier},
	author = {Eduardo Gutiérrez-Peña and Adrian F. M. Smith},
	journal = {Journal of the American Statistical Association},
	keywords = {Bayesian inference; Conjugate prior; Jeffreys's prior; Quadratic variance function},
	localfile = {article/GutierrezPena-Smith-1995.pdf},
	number = {432},
	pages = {1347–1356},
	title = {Conjugate parameterizations for natural exponential families},
	url = {http://www.jstor.org/stable/2291525},
	volume = {90},
	year = {1995}
}

@article{Krein-Milman-1940,
	author = {M. Krein and D. Milman},
	journal = {Studia Mathematica},
	localfile = {article/Krein-Milman-1940.pdf},
	pages = {133–138},
	title = {On extreme points of regular convex sets},
	volume = {9},
	year = {1940}
}

@article{Tukey-1986-sunset,
	author = {John W. Tukey},
	journal = {The American Statistician},
	localfile = {article/Tukey-1986-sunset.pdf},
	number = {1},
	pages = {72–76},
	title = {Sunset salvo},
	url = {http://www.jstor.org/stable/2683137},
	volume = {40},
	year = {1986}
}

@article{Hammer-1955,
	author = {Preston C. Hammer},
	doi = {10.1215/S0012-7094-55-02209-2},
	journal = {Duke Mathematical Journal},
	localfile = {article/Hammer-1955.pdf},
	number = {1},
	pages = {103–106},
	publisher = {Duke University Press},
	title = {Maximal convex sets},
	volume = {22},
	year = {1955}
}

@proceedings{Cowles-1951,
	booktitle = {Activity analysis of production and allocation},
	editor = {Tjalling C. Koopmans},
	number = {13},
	organization = {Cowles Commission for Research in Economics},
	series = {Cowles Commission Monographs},
	title = {Activity analysis of production and allocation},
	year = {1951}
}

@article{Consonni-Veronese-GutierrezPena-2004,
	abstract = {Reference analysis is one of the most successful general
methods to derive noninformative prior distributions. In practice, however,
reference priors are often difficult to obtain. Recently developed theory for
conditionally reducible natural exponential families identifies an attractive
reparameterization which allows one, among other things, to construct an
enriched conjugate prior. In this paper, under the assumption that the variance
function is simple quadratic, the order-invariant group reference prior for the
above parameter is found. Furthermore, group reference priors for the mean- and
natural parameter of the families are obtained. A brief discussion of the
frequentist coverage properties is also presented. The theory is illustrated for
the multinomial and negative-multinomial family. Posterior computations are
especially straightforward due to the fact that the resulting reference
distributions belong to the corresponding enriched conjugate family. A
substantive application of the theory relates to the construction of reference
priors for the Bayesian analysis of two-way contingency tables with respect to
two alternative parameterizations.},
	author = {Guido Consonni and Piero Veronese and Eduardo Gutiérrez-Peña},
	doi = {10.1016/S0047-259X(03)00095-2},
	journal = {Journal of Multivariate Analysis},
	keywords = {Bayesian inference; Contingency table; Enriched conjugate prior; Multinomial family; Negative-multinomial family; Noninformative prior; conditional reducibility},
	localfile = {article/Consonni-Veronese-GutierrezPena-2004.pdf},
	number = {2},
	pages = {335–364},
	publisher = {Elsevier},
	title = {Reference priors for exponential families with simple quadratic variance function},
	volume = {88},
	year = {2004}
}

@article{Quaeghebeur-DeCooman-2009-idmgames,
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	doi = {10.1016/j.ijar.2008.03.012},
	journal = {International Journal of Approximate Reasoning},
	pages = {243–256},
	title = {Learning in games using the imprecise Dirichlet-model},
	volume = {50},
	year = {2009}
}

@techreport{Ferson-et-al-2002,
	annote = {unabridged version, with corrections, printed in 2003},
	author = {Scott Ferson and Vladik Kreinovich and Lev Ginzburg and Davis S. Myers and Kari Sentz},
	institution = {Sandia National Laboratories},
	number = {SAND2002-4015},
	title = {Constructing probability boxes and Dempster-Shafer structures},
	year = {2002}
}

@article{Wagner-1999-two-envelope,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Erkenntnis},
	pages = {233–241},
	title = {Misadventures in conditional expectation: the two-envelope problem},
	volume = {51},
	year = {1999}
}

@article{Augustin-2005,
	abstract = {Dempster-Shafer theory allows to construct belief functions
from (precise) basic probability assignments. The present paper extends this
idea substantially. By considering sets of basic probability assignments, an
appealing constructive approach to general interval probability is achieved,
which allows for a very flexible modelling of uncertain knowledge.},
	author = {Thomas Augustin},
	doi = {10.1080/03081070500190839},
	journal = {International Journal of General Systems},
	keywords = {Basic probability assignment; Belief function; Dempster–Shafer theory; Imprecise probabilities; Interval probability; Linear partial information},
	localfile = {article/Augustin-2005.pdf},
	number = {4},
	pages = {451–463},
	title = {Generalized basic probability assignments},
	volume = {34},
	year = {2005}
}

@article{Pearl-1988-intervals,
	abstract = {The apparent failure of individual probabilistic expressions
to distinguish between uncertainty and ignorance, and between certainty and
confidence, have swayed researchers to seek alternative formalisms, where
confidence measures are provided explicit notation. This paper summarizes how a
causal networks formulation of probabilities facilitates the representation of
confidence measures as an integral part of a knowledge system that does not
require the use of higher order probabilities. We also examine whether
Dempster-Shafer intervals represent confidence about probabilities.},
	author = {Judea Pearl},
	doi = {10.1016/0888-613X(88)90117-X},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Dempster-Shafer theory; ca; interval probability},
	localfile = {article/Pearl-1988-intervals.pdf},
	number = {3},
	pages = {211–216},
	publisher = {Elsevier},
	title = {On probability intervals},
	volume = {2},
	year = {1988}
}

@article{DeFinetti-1933b,
	author = {Bruno de Finetti},
	journal = {Atti della reale accadeamia nazionale dei Lincei, Rendiconti, Classe di Scienze fisiche, matematiche e naturali},
	localfile = {article/DeFinetti-1933b.pdf},
	pages = {203–207},
	title = {La legge dei grandi numeri nel caso dei numeri aleatori equivalenti},
	volume = {18},
	year = {1933}
}

@article{Walley-1997-bounded,
	abstract = {A new method is proposed for drawing coherent statistical
inferences about a real-valued parameter in problems where there is little or no
prior information. Prior ignorance about the parameter is modelled by the set of
all continuous probability density functions for which the derivative of the
log-density is bounded by a positive constant. This set is
translation-invariant, it contains density functions with a wide variety of
shapes and tail behaviour, and it generates prior probabilities that are highly
imprecise. Statistical inferences can be calculated by solving a simple type of
optimal control problem whose general solution is characterized. Detailed
results are given for the problems of calculating posterior upper and lower
means, variances, distribution functions and probabilities of intervals. In
general, posterior upper and lower expectations are achieved by prior density
functions that are piecewise exponential. The results are illustrated by normal
and binomial examples},
	author = {Peter Walley},
	doi = {10.1111/1467-9469.00075},
	journal = {Scandinavian Journal of Statistics},
	localfile = {article/Walley-1997-bounded.pdf},
	number = {4},
	pages = {463–483},
	title = {A Bounded Derivative model for Prior Ignorance about a Real-valued Parameter},
	volume = {24},
	year = {1997}
}

@incollection{Piatti-et-al-2010-CNtutorial,
	address = {New York},
	annote = {ook op papier},
	author = {Alberto Piatti and Alessandro Antonucci and Marco Zaffalon},
	booktitle = {Advances in Mathematics Research},
	editor = {Albert R. Baswell},
	publisher = {Nova Publishers},
	title = {Building Knowledge-Based Systems by Credal Networks: A Tutorial},
	volume = {11},
	year = {2010}
}

@techreport{Walley-Bernard-1999,
	annote = {geannoteerde kopie},
	author = {Peter Walley and Jean-Marc Bernard},
	institution = {Université de Paris 8},
	number = {CAF-9901},
	title = {Imprecise Probabilistic Prediction for Categorical Data},
	year = {1999}
}

@book{DeFinetti-1972,
	annote = {geringde kopie},
	author = {Bruno de Finetti},
	publisher = {John Wiley \& Sons},
	title = {Probability, Induction and Statistics (The art of guessing)},
	year = {1972}
}

@article{Suppes-1974,
	abstract = {This paper criticizes some of the claims of the standard
theories of subjective probability. The criticisms are especially oriented
toward the structural axioms that cannot be regarded as axioms of pure
rationality and the general results that yield exact measurement of subjective
probabilities. Qualitative axioms for upper and lower probability are introduced
to provide a theory of inexact measurement of subjective probability. Only minor
modifications of de Finetti's qualitative axioms yield the desired theory. The
paper concludes with a comparison of the measurement of belief to the
measurement results characteristic of Euclidean geometry, and also examines
briefly some possibilities for using learning models as simplified abstract
processes for constructing belief.},
	author = {Patrick Suppes},
	issn = {0035-9246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	keywords = {belief; lower probability; measurement; upper probability},
	localfile = {article/Suppes-1974.pdf},
	number = {2},
	pages = {160–191},
	title = {The measurement of belief},
	url = {http://www.jstor.org/stable/2984811},
	volume = {36},
	year = {1974}
}

@article{Revuz-1956,
	author = {André Revuz},
	journal = {Annales de l'Institut Fourier},
	localfile = {article/Revuz-1956.pdf},
	pages = {187–269},
	title = {Fonctions croissantes et mesures sur les espaces topologiques ordonnés},
	url = {http://www.numdam.org/item?id=AIF_1956__6__187_0},
	volume = {6},
	year = {1956}
}

@phdthesis{Davison-2002,
	abstract = {User-perceived retrieval latencies in the World Wide Web can
be improved by preloading a local cache with resources likely to be accessed. A
user requesting content that can be served by the cache is able to avoid the
delays inherent in the Web, such as congested networks and slow servers. The
difficulty, then, is to determine what content to prefetch into the cache. This
work explores machine learning algorithms for user sequence prediction, both in
general and speciffically for sequences of Web requests. We also consider
information retrieval techniques to allow the use of the content of Web pages to
help predict future requests. Although history-based mechanisms can provide
strong performance in predicting future requests, performance can be improved by
including predictions from additional sources. While past researchers have used
a variety of techniques for evaluating caching algorithms and systems, most of
those methods were not applicable to the evaluation of prefetching algorithms or
systems. Therefore, two new mechanisms for evaluation are introduced. The first
is a detailed trace-based simulator, built from scratch, that estimates
client-side response times in a simulated network of clients, caches, and Web
servers with various connectivity. This simulator is then used to evaluate
various prefetching approaches. The second evaluation method presented is a
novel architecture to simultaneously evaluate multiple proxy caches in a live
network, which we introduce, implement, and demonstrate through experiments. The
simulator is appropriate for evaluation of algorithms and research ideas, while
simultaneous proxy evaluation is ideally suited to implemented systems. We also
consider the present and the future ofWeb prefetching, nding that changes to the
HTTP standard will be required in order for Web prefetching to become
commonplace.},
	author = {Brian Douglas Davison},
	school = {Rutgers, The State University of New Jersey},
	title = {The design and evaluation of web prefetching and caching techniques},
	year = {2002}
}

@book{Fiedler-etal-2006-inexactLP,
	author = {M. Fiedler and Jiri Nedoma and Jaroslav Ramík and Jiri Rohn and Karel Zimmermann},
	publisher = {Springer},
	title = {Linear Optimization Problems with Inexact Data},
	url = {http://www.nsc.ru/interval/Library/InteBooks/InexactLP.pdf},
	year = {2006}
}

@inbook{Avis-2000-lrs,
	author = {David Avis},
	booktitle = {Polytopes - Combinatorics and Computation},
	editor = {Gil Kalai and Günther Ziegler},
	pages = {177–198},
	publisher = {Birkhäuser},
	series = {DMV Seminar},
	title = {lrs: A Revised Implementation of the Reverse Search Vertex Enumeration Algorithm},
	url = {http://cgm.cs.mcgill.ca/~avis/C/lrs.html},
	volume = {29},
	year = {2000}
}

@article{Zimmermann-1975,
	abstract = {The concept of fuzzy sets is presented as a new tool for the
formulation and solution of systems and decision problems which contain fuzzy
components or fuzzy relationships. After a brief description of the basic theory
of fuzzy sets, implications to systems theory and decision making are indicated.
Fuzzy set theory is then applied to fuzzy linear programming problems and it is
shown how fuzzy linear programming problems can be solved without increasing the
computational effort. Some critical remarks concerning the presently existing
axioms and necessary future research efforts conclude this introductionary
paper.},
	author = {Hans-Jürgen Zimmermann},
	doi = {10.1080/03081077508960870},
	journal = {International Journal of General Systems},
	number = {1},
	title = {Description and optimization of fuzzy systems},
	volume = {2},
	year = {1975}
}

@article{Klee-1956,
	author = {Victor L. Jr. Klee},
	journal = {Mathematica Scandinavica},
	localfile = {article/Klee-1956.pdf},
	pages = {54–64},
	title = {The structure of semispaces},
	url = {http://www.mscand.dk/article.php?id=1449},
	volume = {4},
	year = {1956}
}

@article{DeCooman-2005-order,
	annote = {reprint},
	author = {Gert {De Cooman}},
	doi = {10.1007/s10472-005-9006-x},
	journal = {Annals of Mathematics and Artificial Intelligence},
	keywords = {belief model; belief revision; classical propositional logic; imprecise probability; order theory; possibility measure; system of spheres},
	localfile = {article/DeCooman-2005-order.pdf},
	number = {1},
	pages = {5–34},
	publisher = {Springer},
	title = {Belief models: An order-theoretic investigation},
	volume = {45},
	year = {2005}
}

@techreport{Williams-1975,
	author = {Peter M. Williams},
	booktitle = {International Journal of Approximate Reasoning},
	institution = {University of Sussex},
	localfile = {techreport/Williams-1975.pdf},
	pages = {29},
	title = {Notes on conditional previsions},
	volume = {44},
	year = {1975}
}

@inproceedings{DeCooman-Quaeghebeur-2009-ISIPTA,
	address = {Durham, United Kingdom},
	author = {Gert {De Cooman} and Erik Quaeghebeur},
	booktitle = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
	editor = {Thomas Augustin and Frank P. A. Coolen and Serafin Moral and Matthias C. M. Troffaes},
	organization = {SIPTA},
	pages = {159–168},
	title = {Exchangeability for sets of desirable gambles},
	year = {2009}
}

@techreport{Nilim-ElGhaoui-2004-robust-Markov,
	address = {Berkeley, California},
	author = {Arnab Nilim and Laurent {El Ghaoui}},
	institution = {Department of Electrical Engineering and Computer Sciences},
	number = {M04/26},
	school = {University of California Berkeley},
	title = {Robust Markov Decision Processes with Uncertain Transition Matrices},
	type = {UCB ERL MEMO},
	year = {2004}
}

@misc{Doumont-2001-writing,
	author = {Jean-luc Doumont},
	title = {Writing documents},
	year = {2001}
}

@article{Shimony-1955,
	author = {Abner Shimony},
	issn = {0022-4812},
	journal = {The Journal of Symbolic Logic},
	localfile = {article/Shimony-1955.pdf},
	number = {1},
	pages = {1–28},
	publisher = {Association for Symbolic Logic},
	title = {Coherence and the Axioms of Confirmation},
	url = {http://www.jstor.org/stable/2268039},
	volume = {20},
	year = {1955}
}

@book{Acta-Numerica-2004,
	editor = {A. Iserles},
	publisher = {Cambridge University Press},
	title = {Acta Numerica 2004},
	year = {2004}
}

@proceedings{IPMU-1996,
	booktitle = {Proceedings of the Sixth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 96},
	title = {Proceedings of the Sixth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 96},
	year = {1996}
}

@article{Troffaes-2007-decision,
	abstract = {Various ways for decision making with imprecise
probabilities—admissibility, maximal expected utility, maximality,
E-admissibility, $\Gamma$-maximax, $\Gamma$-maximin, all of which are well known
from the literature—are discussed and compared. We generalise a well-known
sufficient condition for existence of optimal decisions. A simple numerical
example shows how these criteria can work in practice, and demonstrates their
differences. Finally, we suggest an efficient approach to calculate optimal
decisions under these decision criteria.},
	author = {Matthias C. M. Troffaes},
	doi = {10.1016/j.ijar.2006.06.001},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Decision; E-admissibility; Lower prevision; Maximality; Maximin; Optimality; Probability; Uncertainty},
	localfile = {article/Troffaes-2007-decision.pdf},
	number = {1},
	pages = {17–29},
	title = {Decision making under uncertainty using imprecise probabilities},
	volume = {45},
	year = {2007}
}

@book{Floudas-Pardalos-2009,
	edition = {2},
	editor = {Christodoulos A. Floudas and Panos M. Pardalos},
	publisher = {Springer},
	series = {Springer Reference},
	title = {Encyclopedia of Optimization},
	year = {2009}
}

@book{Nuutila-1995-transcl,
	author = {Esko Nuutila},
	number = {74},
	publisher = {Finnish Academy of Technology},
	series = {Acta Polytechnica Scandinavica, Mathematics and Computing in Engineering Series},
	title = {Efficient Transitive Closure Computation in Large Digraphs},
	url = {http://www.cs.hut.fi/~enu/thesis.html},
	year = {1995}
}

@article{Connor-Mosimann-1969,
	annote = {ook op papier},
	author = {Robert J. Connor and James E. Mosimann},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Conner-Mosimann-1969.pdf},
	number = {325},
	pages = {194–206},
	title = {Concepts of independence for proportions with a generalization of the Dirichlet distribution},
	url = {http://www.jstor.org/stable/2283728},
	volume = {64},
	year = {1969}
}

@inproceedings{Zaffalon-2001,
	address = {Ithaca, New York},
	author = {Marco Zaffalon},
	booktitle = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Gert {De Cooman} and Terrence L. Fine and Teddy Seidenfeld},
	pages = {384–393},
	publisher = {Shaker Publishing, Maastricht, The Netherlands},
	title = {Statistical inference of the naive credal classifier},
	year = {2001}
}

@article{Miranda-DeCooman-Quaeghebeur-2007-Hausdorff,
	abstract = {We investigate to what extent finitely additive probability
measures on the unit interval are determined by their moment sequence. We do
this by studying the lower envelope of all finitely additive probability
measures with a given moment sequence. Our investigation leads to several
elegant expressions for this lower envelope, and it allows us to conclude that
the information provided by the moments is equivalent to the one given by the
associated lower and upper distribution functions.},
	author = {Enrique Miranda and Gert {De Cooman} and Erik Quaeghebeur},
	doi = {10.1007/s10959-007-0055-4},
	journal = {Journal of Theoretical Probability},
	keywords = {Hausdorff moment problem; coherent lower prevision; complete monotonicity; lower distribution function},
	localfile = {article/Miranda-DeCooman-Quaeghebeur-2007-Hausdorff.pdf},
	number = {3},
	pages = {663–693},
	title = {The Hausdorff Moment Problem under Finite Additivity},
	volume = {20},
	year = {2007}
}

@book{Johnson-1924,
	address = {Cambridge, United Kingdom},
	author = {W. E. Johnson},
	publisher = {Cambridge University Press},
	title = {Logic, Part III},
	year = {1924}
}

@article{Burge-karlin-1997,
	author = {Chris Burge and Samuel Karlin},
	journal = {Journal of Molecular Biology},
	pages = {78–94},
	title = {Prediction of Complete gene Structures in Human genomic DNA},
	volume = {268},
	year = {1997}
}

@book{Kemeny-Snell-Thompson-1974-finitemathintro,
	author = {John G. Kemeny and J. Laurie Snell and Gerald L. Thompson},
	edition = {3},
	publisher = {Prentice-Hall},
	title = {Introduction to Finite Mathematics},
	year = {1974}
}

@article{Ovaere-Deschrijver-Kerre-2009,
	abstract = {In this paper we present a new approach to handle
uncertainty in the Finite Element Method. As this technique is widely used to
tackle real-life design problems, it is also very prone to
parameter-uncertainty. It is hard to make a good decision regarding design
optimization if no claim can be made with respect to the outcome of the
simulation. We propose an approach that combines several techniques in order to
offer a total order on the possible design choices, taking the inherent
fuzziness into account. Additionally we propose a more efficient ordering
procedure to build a total order on fuzzy numbers.},
	annote = {ook op papier},
	author = {Koen Ovaere and Glad Deschrijver and Etienne E. Kerre},
	doi = {10.1007/s12543-009-0002-4},
	journal = {Fuzzy Information and Engineering},
	keywords = {fuzzy decision making; fuzzy finite element method; fuzzy ordering},
	localfile = {article/Ovaere-deschrijver-Kerre-2009.pdf},
	number = {1},
	pages = {27–36},
	publisher = {Springer},
	title = {Application of fuzzy decision making to the fuzzy finite element method},
	volume = {1},
	year = {2009}
}

@book{Brokken-2006,
	author = {Frank B. Brokken},
	month = sep,
	publisher = {University of Groningen},
	title = {C++ Annotations},
	year = {2006}
}

@book{BarndorffNielsen-1978,
	annote = {Geselecteerde delen kopies},
	author = {Ole Barndorff-Nielsen},
	publisher = {Wiley},
	title = {Information and Exponential Families In Statistical Theory},
	year = {1978}
}

@article{Harsanyi-1982-comment,
	annote = {ook op papier},
	author = {John C. Harsanyi},
	journal = {Management Science},
	localfile = {article/Harsanyi-1982-comment.pdf},
	number = {2},
	pages = {120–124},
	title = {Subjective probability and the theory of games: Comments on Kadane and Larkey's paper},
	url = {http://www.jstor.org/stable/2631295},
	volume = {28},
	year = {1982}
}

@proceedings{IPMU-2004,
	booktitle = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
	title = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
	year = {2004}
}

@article{Wagner-2007-Smith-Walley,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	doi = {10.1007/s11225-007-9064-7},
	journal = {Studia Logica},
	localfile = {article/Wagner-2007-Smith-Walley.pdf},
	pages = {343–350},
	title = {The Smith-Walley Interpretation of Subjective Probability: An Appreciation},
	volume = {86},
	year = {2007}
}

@book{Davey-Priestley-1990,
	author = {B. A. Davey and H. A. Priestley},
	publisher = {Cambridge University Press},
	series = {Cambridge Mathematical Textbooks},
	title = {Introduction to Lattices and Order},
	year = {1990}
}

@inproceedings{John-Langley-1995,
	author = {George H. John and Pat Langley},
	booktitle = {Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95)},
	editor = {Philippe Besnard and Steve Hanks},
	pages = {338–345},
	publisher = {Morgan Kaufmann},
	title = {Estimating Continuous Distributions in Bayesian Classifiers},
	year = {1995}
}

@article{Tanaka-Okuda-Asai-1973,
	abstract = {In problems of system analysis, it is customary to treat
imprecision by the use of probability theory. It is becoming increasingly clear,
however, that in the case of many real world problems involving large scale
systems such as economic systems, social systems, mass service systems, etc.,
the major source of imprecision should more properly be labeled 'fuzziness'
rather than 'randomness.' By fuzziness, we mean the type of imprecision which is
associated with the lack of sharp transition from membership to nonmembership,
as in tall men, small numbers, likely events, etc. In this paper our main
concern is with the application of the theory of fuzzy sets to decision problems
involving fuzzy goals and strategies, etc., as defined by R. E. Bellman and L.
A. Zadeh [1]. However, in our approach, the emphasis is on mathematical
programming and the use of the concept of a level set to extend some of the
classical results to problems involving fuzzy constraints and objective
functions.},
	author = {Hideo Tanaka and Tetsuji Okuda and Kiyoji Asai},
	doi = {10.1080/01969727308545912},
	journal = {Cybernetics and Systems: An International Journal},
	number = {4},
	pages = {37–46},
	title = {On Fuzzy-Mathematical Programming},
	volume = {3},
	year = {1973}
}

@article{Diaconis-Ylvisaker-1979,
	abstract = {Let X be a random vector distributed according to an
exponential family with natural parameter $\theta$ ∈ $\Theta$. We characterize
conjugate prior measures on $\Theta$ through the property of linear posterior
expectation of the mean parameter of X : E{E(X|$\theta$)|X = x} = ax + b. We
also delineate which hyperparameters permit such conjugate priors to be
proper.},
	annote = {geannoteerde kopie},
	author = {Persi Diaconis and Donald Ylvisaker},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	keywords = {Bayes; conjugate priors; linearity of regression},
	localfile = {article/Diaconis-Ylvisaker-1979.pdf},
	number = {2},
	pages = {269–281},
	publisher = {Institute of Mathematical Statistics},
	title = {Conjugate priors for exponential families},
	volume = {7},
	year = {1979}
}

@proceedings{ISIPTA-2001,
	address = {Ithaca, New York},
	booktitle = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Gert {De Cooman} and Terrence L. Fine and Teddy Seidenfeld},
	publisher = {Shaker Publishing, Maastricht, The Netherlands},
	title = {ISIPTA '01: Proceedings of the Second International Symposium on Imprecise Probabilities and Their Applications},
	year = {2001}
}

@article{Miranda-Combarro-Gil-2006-extreme,
	abstract = {Non-additive measures are a valuable tool to model many
different problems arising in real situations. However, two important
difficulties appear in their practical use: the complexity of the measures and
their identification from sample data. For the first problem, additional
conditions are imposed, leading to different subfamilies of non-additive
measures. Related to the second point, in this paper we study the set of
vertices of some families of non-additive measures, namely k-additive measures
and p-symmetric measures. These extreme points are necessary in order to
properly apply a new method of identification of non-additive measures based on
genetic algorithms, whose cross-over operator is the convex combination. We
solve the problem through techniques of Linear Programming.},
	author = {Pedro Miranda and Elías F. Combarro and Pedro Gil},
	doi = {10.1016/j.ejor.2005.03.005},
	journal = {European Journal of Operational Research},
	keywords = {Decision analysis; Genetic algorithms; Linear programming; Multiple criteria analysis; Non-additive measures; Vertices; k-additivity; p-symmetry},
	localfile = {article/Miranda-Combarro-Gil-2006-extreme.pdf},
	number = {3},
	pages = {1865–1884},
	title = {Extreme points of some families of non-additive measures},
	volume = {174},
	year = {2006}
}

@article{Miranda-DeCooman-Couso-2005-multivalued,
	abstract = {We discuss how lower previsions induced by multi-valued
mappings fit into the framework of the behavioural theory of imprecise
probabilities, and show how the notions of coherence and natural extension from
that theory can be used to prove and generalise existing results in an elegant
and straightforward manner. This provides a clear example for their explanatory
and unifying power.},
	annote = {uitgebreide versie, ook op papier},
	author = {Enrique Miranda and Gert {De Cooman} and Inés Couso},
	doi = {10.1016/j.jspi.2004.03.005},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Coherence; Conditioning; Evidence theory; Imprecise probabilities; Lower inverse; Lower prevision; Multi-valued mapping; Natural extension; Random set},
	localfile = {article/Miranda-DeCooman-Couso-2005-multivalued.pdf},
	number = {1},
	pages = {173–197},
	title = {Lower previsions induced by multi-valued mappings},
	volume = {133},
	year = {2005}
}

@article{Wagner-2003-uniformity-rule,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Erkenntnis},
	pages = {349–364},
	title = {Commuting probability revisions: the uniformity rule},
	volume = {59},
	year = {2003}
}

@article{Bruening-Dennenberg-2008-belELP,
	abstract = {It is known that the $\sigma$-additive Möbius transform of a
belief function (we prefer to call it belief measure) can be derived from
Choquet's Theorem. One has to show that the extreme points of the compact convex
set of belief measures are the {0,1}-valued belief measures, which are called
filter games as well. A proof is implicit in the famous 1953/54 paper of Choquet
but it is hard to read it. We present a direct proof and – for the sake of
completeness – derive the Möbius transform.},
	annote = {explicitation of an implicit result of Choquet-1954},
	author = {Martin Brüning and Dieter Denneberg},
	doi = {10.1016/j.ijar.2006.11.003},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Bruening-Dennenberg-2007-belELP.pdf},
	number = {3},
	pages = {670–675},
	publisher = {Elsevier},
	title = {The extreme points of the set of belief measures},
	volume = {48},
	year = {2008}
}

@article{Epstein-Seo-2010,
	abstract = {The de Finetti Theorem is a cornerstone of the Bayesian
approach. Bernardo (1996, p. 5) writes that its “message is very clear: if a
sequence of observations is judged to be exchangeable, then any subset of them
must be regarded as a random sample from some model, and there exists a prior
distribution on the parameter of such model, hence requiring a Bayesian
approach.” We argue that although exchangeability, interpreted as symmetry of
evidence, is a weak assumption, when combined with subjective expected utility
theory, it also implies complete confidence that experiments are identical. When
evidence is sparse and there is little evidence of symmetry, this implication of
de Finetti's hypotheses is not intuitive. This motivates our adoption of
multiple-priors utility as the benchmark model of preference. We provide two
alternative generalizations of the de Finetti Theorem for this framework. A
model of updating is also provided.},
	author = {Larry G. Epstein and Kyoungwon Seo},
	doi = {10.3982/TE596},
	issn = {1555-7561},
	journal = {Theoretical Economics},
	keywords = {ambiguity; exchangeability; learning; multiple priors; symmetry; updating},
	localfile = {article/Epstein-Seo-2010.pdf},
	number = {3},
	pages = {313–368},
	publisher = {Blackwell Publishing Ltd},
	title = {Symmetry of evidence without evidence of symmetry},
	volume = {5},
	year = {2010}
}

@article{Rho-Tang-Ye-2010,
	abstract = {The advances of next-generation sequencing technology have
facilitated metagenomics research that attempts to determine directly the whole
collection of genetic material within an environmental sample (i.e. the
metagenome). Identification of genes directly from short reads has become an
important yet challenging problem in annotating metagenomes, since the assembly
of metagenomes is often not available. Gene predictors developed for whole
genomes (e.g. Glimmer) and recently developed for metagenomic sequences (e.g.
MetaGene) show a significant decrease in performance as the sequencing error
rates increase, or as reads get shorter. We have developed a novel gene
prediction method FragGeneScan, which combines sequencing error models and codon
usages in a hidden Markov model to improve the prediction of protein-coding
region in short reads. The performance of FragGeneScan was comparable to Glimmer
and MetaGene for complete genomes. But for short reads, FragGeneScan
consistently outperformed MetaGene (accuracy improved ~62\% for reads of 400
bases with 1\% sequencing errors, and ~18\% for short reads of 100 bases that
are error free). When applied to metagenomes, FragGeneScan recovered
substantially more genes than MetaGene predicted (>90\% of the genes identified
by homology search), and many novel genes with no homologs in current protein
sequence database.},
	author = {Mina Rho and Haixu Tang and Yuzhen Ye},
	doi = {10.1093/nar},
	journal = {Nucleic Acids Research},
	localfile = {article/Rho-Tang-Ye-2010.pdf},
	number = {20},
	pages = {e191},
	title = {FragGeneScan: predicting genes in short and error-prone reads},
	volume = {38},
	year = {2010}
}

@article{Utkin-Augustin-2007,
	abstract = {The paper presents an efficient solution to decision
problems where direct partial information on the distribution of the states of
nature is available, either by observations of previous repetitions of the
decision problem or by direct expert judgements. To process this information we
use a recent generalization of Walley's imprecise Dirichlet model, allowing us
also to handle incomplete observations or imprecise judgements, including
missing data. We derive efficient algorithms and discuss properties of the
optimal solutions with respect to several criteria, including Gamma-maximinity
and E-admissibility. In the case of precise data and pure actions the former
surprisingly leads us to a frequency-based variant of the Hodges–Lehmann
criterion, which was developed in classical decision theory as a compromise
between Bayesian and minimax procedures.},
	author = {Lev V. Utkin and Thomas Augustin},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Belief functions; Coarse data; Decision making; E-admissibility; Imprecise Dirichlet model (IDM); Imprecise probabilities; Incomplete data; Interval probability; Interval statistical models; Missing or set-valued statistical data},
	localfile = {article/Utkin-Augustin-2007.pdf},
	number = {3},
	pages = {322–338},
	title = {Decision making under incomplete data using the imprecise Dirichlet model},
	volume = {44},
	year = {2007}
}

@inproceedings{DeCooman-Troffaes-Miranda-2005,
	address = {Pittsburgh, Pennsylvania},
	author = {Gert {De Cooman} and Matthias C. M. Troffaes and Enrique Miranda},
	booktitle = {ISIPTA '05: Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Fabio Gagliardi Cozman and Robert Nau and Teddy Seidenfeld},
	organization = {SIPTA},
	pages = {145–154},
	title = {n-Monotone lower previsions and lower integrals},
	year = {2005}
}

@inproceedings{Fortet-1951,
	address = {Paris},
	author = {Robert M. Fortet},
	booktitle = {Congrès international de philosophie des sciences. 4: Calcul des probabilités},
	editor = {Raymond Bayer},
	number = {1146},
	pages = {35–47},
	publisher = {Hermann},
	series = {Actualités scientifiques et industrielles},
	title = {Faut-il élargir les axiomes du calcul des probabilités?},
	year = {1951}
}

@inproceedings{Wilson-Moral-1994,
	author = {Nic Wilson and Serafín Moral},
	booktitle = {ECAI 94: Proceedings of the 11th European Conference on Artificial Intelligence},
	editor = {A. Cohn},
	pages = {386–390},
	publisher = {John Wiley \& Sons},
	title = {A logical view of probability},
	year = {1994}
}

@article{Munch-etal-2006,
	abstract = {BACKGROUND: Genomic tiling micro arrays have great potential
for identifying previously undiscovered coding as well as non-coding
transcription. To-date, however, analyses of these data have been performed in
an ad hoc fashion. RESULTS: We present a probabilistic procedure, ExpressHMM,
that adaptively models tiling data prior to predicting expression on genomic
sequence. A hidden Markov model (HMM) is used to model the distributions of
tiling array probe scores in expressed and non-expressed regions. The HMM is
trained on sets of probes mapped to regions of annotated expression and
non-expression. Subsequently, prediction of transcribed fragments is made on
tiled genomic sequence. The prediction is accompanied by an expression
probability curve for visual inspection of the supporting evidence. We test
ExpressHMM on data from the Cheng et al. (2005) tiling array experiments on ten
Human chromosomes [1]. Results can be downloaded and viewed from our web site
[2]. CONCLUSION: The value of adaptive modelling of fluorescence scores prior to
categorisation into expressed and non-expressed probes is demonstrated. Our
results indicate that our adaptive approach is superior to the previous analysis
in terms of nucleotide sensitivity and transfrag specificity.},
	author = {Kasper Munch and Paul Gardner and Peter Arctander and Anders Krogh},
	doi = {10.1186/1471-2105-7-239},
	issn = {1471-2105},
	journal = {BMC Bioinformatics},
	localfile = {article/Munch-etal-2006.pdf},
	number = {1},
	pages = {239},
	publisher = {BioMed Central Ltd},
	title = {A hidden Markov model approach for determining expression from genomic tiling micro arrays},
	volume = {7},
	year = {2006}
}

@phdthesis{Mevel-1997,
	author = {Laurent Mevel},
	school = {Université de Rennes},
	title = {Statistique asymptotique pour les modèles de Markov cachés},
	year = {1997}
}

@article{DeCooman-2001,
	abstract = {The paper discusses integration and some aspects of
conditioning in numerical possibility theory, where possibility measures have
the behavioural interpretation of upper probabilities, that is, systems of upper
betting rates. In such a context, integration can be used to extend upper
probabilities to upper previsions. It is argued that the role of the fuzzy
integral in this context is limited, as it can only be used to define a coherent
upper prevision if the associated upper probability is 0–1-valued, in which
case it moreover coincides with the Choquet integral. These results are valid
for arbitrary coherent upper probabilities, and therefore also relevant for
possibility theory. It follows from the discussion that in a numerical context,
the Choquet integral is better suited than the fuzzy integral for producing
coherent upper previsions starting from possibility measures. At the same time,
alternative expressions for the Choquet integral associated with a possibility
measure are derived. Finally, it is shown that a possibility measure is fully
conglomerable and satisfies Walley's regularity axiom for conditioning, ensuring
that it can be coherently extended to a conditional possibility measure using
both the methods of natural and regular extension.},
	author = {Gert {De Cooman}},
	doi = {10.1023/A:1016705331195},
	journal = {Annals of Mathematics and Artificial Intelligence},
	localfile = {article/DeCooman-2001.pdf},
	month = aug,
	number = {1},
	pages = {87–123},
	publisher = {Springer},
	title = {Integration and conditioning in numerical possibility theory},
	volume = {32},
	year = {2001}
}

@book{Johnson-Kotz-Balakrishnan-1997,
	author = {Norman L. Johnson and Samuel Kotz and N. Balakrishnan},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Statistics},
	title = {Discrete Multivariate Distributions},
	year = {1997}
}

@article{Casalis-1996,
	abstract = {The present paper describes all the natural exponential
families on \mathbb{R}^d whose variance function is of the form V(m) = am øtimes
m + B(m) + C, with m øtimes m($\theta$) = \langle $\theta$, m \rangle m and B
linear in m. There are 2d + 4 types of such families, which are built from
particular mixtures of families of Normal, Poisson, gamma, hyperbolic on
\mathbb{R}^d and negative-multinomial distributions. The proof of this result
relies mainly on techniques used in the elementary theory of Lie algebras.},
	author = {M. Casalis},
	doi = {10.1214/aos},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {Morris class; Variance functions},
	localfile = {article/Casalis-1996.pdf},
	number = {4},
	pages = {1828–1854},
	publisher = {Institute of Mathematical Statistics},
	title = {The 2d+4 simple quadratic natural exponential families on R^d},
	url = {http://www.jstor.org/stable/2242752},
	volume = {24},
	year = {1996}
}

@article{DeCooman-etal-2010,
	abstract = {We focus on credal nets, which are graphical models that
generalise Bayesian nets to imprecise probability. We replace the notion of
strong independence commonly used in credal nets with the weaker notion of
epistemic irrelevance, which is arguably more suited for a behavioural theory of
probability. Focusing on directed trees, we show how to combine the given local
uncertainty models in the nodes of the graph into a global model, and we use
this to construct and justify an exact message-passing algorithm that computes
updated beliefs for a variable in the tree. The algorithm, which is linear in
the number of nodes, is formulated entirely in terms of coherent lower
previsions, and is shown to satisfy a number of rationality requirements. We
supply examples of the algorithm's operation, and report an application to
on-line character recognition that illustrates the advantages of our approach
for prediction. We comment on the perspectives, opened by the availability, for
the first time, of a truly efficient algorithm based on epistemic irrelevance.},
	author = {Gert {De Cooman} and Filip Hermans and Alessandro Antonucci and Marco Zaffalon},
	doi = {10.1016/j.ijar.2010.08.011},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Coherence; Credal net; Epistemic irrelevance; Hidden Markov model; Separation; Strong independence},
	localfile = {article/DeCooman-etal-2010.pdf},
	number = {9},
	pages = {1029–1052},
	publisher = {Elsevier},
	title = {Epistemic irrelevance in credal nets: the case of imprecise Markov trees},
	volume = {51},
	year = {2010}
}

@inproceedings{Quaeghebeur-DeCooman-2003-PhDsymp,
	address = {Ghent, Belgium},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	booktitle = {Proceedings of the Fourth UGent-FTW PhD Symposium},
	title = {Command line completion: an illustration of learning and decision making using the imprecise Dirichlet model},
	year = {2003}
}

@article{Kadane-Larkey-1982,
	annote = {ook op papier},
	author = {Joseph B. Kadane and Patrick D. Larkey},
	journal = {Management Science},
	localfile = {article/Kadane-Larkey-1982.pdf},
	number = {2},
	pages = {113–120},
	title = {Subjective probability and the theory of games},
	url = {http://www.jstor.org/stable/2631294},
	volume = {28},
	year = {1982}
}

@book{Fudenberg-Levine-1998,
	annote = {geannoteerde uittreksels},
	author = {Drew Fudenberg and David K. Levine},
	editor = {Ken Binmore},
	number = {2},
	publisher = {The MIT Press},
	series = {MIT Press Series on Economic Learning and Social Evolution},
	title = {The Theory of Learning in Games},
	year = {1998}
}

@article{Bellman-Zadeh-1970,
	abstract = {By decision-making in a fuzzy environment is meant a
decision process in which the goals and/or the constraints, but not necessarily
the system under control, are fuzzy in nature. This means that the goals and/or
the constraints constitute classes of alternatives whose boundaries are not
sharply defined. An example of a fuzzy constraint is: "The cost of A should not
be substantially higher than $\alpha$," where $\alpha$ is a specified constant.
Similarly, an example of a fuzzy goal is: "$\chi$ should be in the vicinity of
$\chi$0," where $\chi$0 is a constant. The italicized words are the sources of
fuzziness in these examples. Fuzzy goals and fuzzy constraints can be defined
precisely as fuzzy sets in the space of alternatives. A fuzzy decision, then,
may be viewed as an intersection of the given goals and constraints. A
maximizing decision is defined as a point in the space of alternatives at which
the membership function of a fuzzy decision attains its maximum value. The use
of these conc},
	author = {R. E. Bellman and L. A. Zadeh},
	doi = {10.1287/mnsc.17.4.B141},
	issn = {0025-1909},
	journal = {Management Science},
	localfile = {article/Bellman-Zadeh-1970.pdf},
	number = {4},
	pages = {141–164},
	publisher = {INFORMS},
	title = {Decision-making in a fuzzy environment},
	volume = {17},
	year = {1970}
}

@article{Zaffalon-2002-missing,
	abstract = {This paper proposes an exact, no-assumptions approach to
dealing with incomplete sets of multivariate categorical data. An incomplete
data set is regarded as a 1nite collection of complete data sets, and a joint
distribution is obtained from each of them, at a descriptive level. The tools to
simultaneously treat all the possible joint distributions compatible with an
incomplete set of data are given. In particular, a linear description of the set
of distributions is formulated, and it is shown that the computation of bounds
on the expectation of real-valued functions under such distributions is both
possible and efficient, by means of linear programming. Specific algorithms are
also developed whose complexity grows linearly in the number of observations. An
analysis is then carried out to estimate population probabilities from
incomplete multinomial samples. The descriptive tool extends in a
straightforward way to the inferential problem by exploiting Walley s imprecise
Dirichlet model.},
	author = {Marco Zaffalon},
	doi = {10.1016/S0378-3758(01)00206-3},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Belief functions; Credal sets; Flow network; Imprecise Dirichlet model; Imprecise probabilities; Incomplete data; Linear optimization},
	localfile = {article/Zaffalon-2002-missing.pdf},
	pages = {105–122},
	title = {Exact credal treatment of missing data},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375801002063},
	volume = {105},
	year = {2002}
}

@inproceedings{Strens-2000,
	author = {Malcolm Strens},
	booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
	title = {A Bayesian Framework for Reinforcement Learning},
	year = {2000}
}

@phdthesis{Coolen-1994,
	author = {Frank P. A. Coolen},
	school = {Technische Universiteit Eindhoven},
	title = {Statistical Modelling of Expert Opinions Using Imprecise Probabilities},
	year = {1994}
}

@article{DeCooman-Zaffalon-2004-incomplete,
	abstract = {Currently, there is renewed interest in the problem, raised
by Shafer in 1985, of updating probabilities when observations are incomplete
(or set-valued). This is a fundamental problem in general, and of particular
interest for Bayesian networks. Recently, Grünwald and Halpern have shown that
commonly used updating strategies fail in this case, except under very special
assumptions. In this paper we propose a new method for updating probabilities
with incomplete observations. Our approach is deliberately conservative: we make
no assumptions about the so-called incompleteness mechanism that associates
complete with incomplete observations. We model our ignorance about this
mechanism by a vacuous lower prevision, a tool from the theory of imprecise
probabilities, and we use only coherence arguments to turn prior into posterior
(updated) probabilities. In general, this new approach to updating produces
lower and upper posterior probabilities and previsions (expectations), as well
as partially determinate decisions. This is a logical consequence of the
existing ignorance about the incompleteness mechanism. As an example, we use the
new updating method to properly address the apparent paradox in the [`]Monty
Hall' puzzle. More importantly, we apply it to the problem of classification of
new evidence in probabilistic expert systems, where it leads to a new, so-called
conservative updating rule. In the special case of Bayesian networks constructed
using expert knowledge, we provide an exact algorithm to compare classes based
on our updating rule, which has linear-time complexity for a class of networks
wider than polytrees. This result is then extended to the more general framework
of credal networks, where computations are often much harder than with Bayesian
nets. Using an example, we show that our rule appears to provide a solid basis
for reliable updating with incomplete observations, when no strong assumptions
about the incompleteness mechanism are justified.},
	author = {Gert {De Cooman} and Marco Zaffalon},
	doi = {10.1016/j.artint.2004.05.006},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Incomplete observations; Updating probabilities},
	localfile = {article/DeCooman-Zaffalon-2004-incomplete.pdf},
	number = {1-2},
	pages = {75–125},
	publisher = {Elsevier},
	title = {Updating beliefs with incomplete observations},
	volume = {159},
	year = {2004}
}

@article{Liu-Mueller-2003,
	author = {Xueli Liu and Hans-Georg Müller},
	doi = {10.1093/bioinformatics},
	journal = {Bioinformatics},
	localfile = {article/Liu-Mueller-2003.pdf},
	number = {15},
	pages = {1937–1944},
	title = {Modes and clustering for time-warped gene expression profile data},
	volume = {19},
	year = {2003}
}

@book{DeGroot-2004,
	annote = {Wiley Classics Library Edition},
	author = {Morris H. DeGroot},
	publisher = {Wiley},
	title = {Optimal Statistical Decisions},
	year = {2004}
}

@article{Benaim-Hirsch-1999,
	abstract = {Fictitious play in infinitely repeated, randomly perturbed
games is investigated. Dynamical systems theory is used to study the Markov
process {x\_k}, whose state vector x\_k lists the empirical frequencies of
player's actions in the first k games. For 2 × 2 games with countably many Nash
distribution equilibria, we prove that sample paths converge almost surely. But
for Jordan's 3 × 2 matching game, there are robust parameter values giving
probability 0 of convergence. Applications are made to coordination and
anticoordination games and to general theory. Proofs rely on results in
stochastic approximation and dynamical systems.},
	annote = {op papier},
	author = {Michael Benaïm and Morris W. Hirsch},
	doi = {10.1006/game.1999.0717},
	issn = {0899-8256},
	journal = {Games and Economic Behavior},
	month = oct,
	number = {1-2},
	pages = {36–72},
	title = {Mixed Equilibria and Dynamical Systems Arising from Fictitious Play in Perturbed Games},
	volume = {29},
	year = {1999}
}

@incollection{Brams-Fishburn-1991-altvote,
	author = {Steven J. Brams and Peter C. Fishburn},
	booktitle = {Political Pareties and Elections in the United States: An Encyclopedia},
	editor = {Sandy L Maisel},
	pages = {23–31},
	publisher = {garland},
	title = {Alternative voting systems},
	url = {http://bcn.boulder.co.us/government/approvalvote/altvote.html},
	volume = {1},
	year = {1991}
}

@incollection{Miranda-DeCooman-Quaeghebeur-2008-IPMU2006,
	author = {Enrique Miranda and Gert {De Cooman} and Erik Quaeghebeur},
	booktitle = {Uncertainty and Intelligent Information Systems},
	chapter = {3},
	editor = {Bernadette Bouchon-Meunier and R. R. Yager and C. Marsala and M. Rifqi},
	pages = {33–45},
	publisher = {World Scientific},
	title = {The moment problem for finitely additive probabilities},
	url = {http://www.worldscibooks.com/compsci/6747.html},
	year = {2008}
}

@article{Dubois-Prade-Sabbadin-2001,
	author = {Didier Dubois and Henri Prade and Régis Sabbadin},
	doi = {10.1016/S0377-2217(99)00473-7},
	journal = {European Journal of Operational Research},
	keywords = {decision theory; possibility theory; uncertainty},
	localfile = {article/Dubois-Prade-Sabbadin-2001.pdf},
	pages = {459–478},
	title = {Decision-theoretic foundations of qualitative possibility theory},
	volume = {128},
	year = {2001}
}

@techreport{Hofbauer-1995,
	author = {Josef Hofbauer},
	institution = {Collegium Budapest},
	title = {Stability for the Best Response Dynamics},
	year = {1995}
}

@article{Derriennic-1985,
	author = {Marie-Madeleine Derriennic},
	journal = {Journal of Approximation Theory},
	pages = {155–166},
	title = {On Multivariate Approximation by Bernstein-Type Polynomials},
	volume = {45},
	year = {1985}
}

@article{Amara-1995-wavelets,
	annote = {op papier},
	author = {Amara Graps},
	journal = {IEEE Computational Science and Engineering},
	number = {2},
	publisher = {IEEE Computer Society},
	title = {An Introduction to Wavelets},
	volume = {2},
	year = {1995}
}

@inproceedings{Bouckaert-2004,
	author = {Remco R. Bouckaert},
	booktitle = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
	editor = {Geoffrey I. Webb and Xinghuo Yu},
	pages = {1089–1094},
	publisher = {Springer},
	series = {Lecture Notes in AI},
	title = {Naive Bayes Classifiers that Perform Well with Continuous Variables},
	year = {2004}
}

@article{Gillett-et-al-2007,
	abstract = {This article presents a probabilistic logic whose sentences
can be interpreted as asserting the acceptability of gambles described in terms
of an underlying logic. This probabilistic logic has a concrete syntax and a
complete inference procedure, and it handles conditional as well as
unconditional probabilities. It synthesizes Nilsson's probabilistic logic and
Frisch and Haddawy's anytime inference procedure with Wilson and Moral's logic
of gambles. Two distinct semantics can be used for our probabilistic logic: (1)
the measure-theoretic semantics used by the prior logics already mentioned and
also by the more expressive logic of Fagin, Halpern, and Meggido and (2) a
behavioral semantics. Under the measure-theoretic semantics, sentences of our
probabilistic logic are interpreted as assertions about a probability
distribution over interpretations of the underlying logic. Under the behavioral
semantics, these sentences are interpreted only as asserting the acceptability
of gambles, and this suggests different directions for generalization.},
	annote = {Reasoning with Imprecise Probabilities},
	author = {Peter R. Gillett and Richard B. Scherl and Glenn Shafer},
	doi = {10.1016/j.ijar.2006.07.014},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Anytime deduction; Behavioral semantics; Gambles; Measure–theoretic; Probabilistic logic},
	localfile = {article/Gillett-et-al-2007.pdf},
	number = {3},
	pages = {281–300},
	title = {A probabilistic logic based on the acceptability of gambles},
	volume = {44},
	year = {2007}
}

@article{Zhu-Lomsadze-Borodovsky-2010,
	abstract = {We describe an algorithm for gene identification in DNA
sequences derived from shotgun sequencing of microbial communities. Accurate ab
initio gene prediction in a short nucleotide sequence of anonymous origin is
hampered by uncertainty in model parameters. While several machine learning
approaches could be proposed to bypass this difficulty, one effective method is
to estimate parameters from dependencies, formed in evolution, between
frequencies of oligonucleotides in protein-coding regions and genome nucleotide
composition. Original version of the method was proposed in 1999 and has been
used since for (i) reconstructing codon frequency vector needed for gene finding
in viral genomes and (ii) initializing parameters of self-training gene finding
algorithms. With advent of new prokaryotic genomes en masse it became possible
to enhance the original approach by using direct polynomial and logistic
approximations of oligonucleotide frequencies, as well as by separating models
for bacteria and archaea. These advances have increased the accuracy of model
reconstruction and, subsequently, gene prediction. We describe the refined
method and assess its accuracy on known prokaryotic genomes split into short
sequences. Also, we show that as a result of application of the new method,
several thousands of new genes could be added to existing annotations of several
human and mouse gut metagenomes.},
	author = {Wenhan Zhu and Alexandre Lomsadze and Mark Borodovsky},
	doi = {10.1093/nar},
	journal = {Nucleic Acids Research},
	number = {12},
	pages = {e132},
	title = {Ab initio gene identification in metagenomic sequences},
	volume = {38},
	year = {2010}
}

@techreport{Fink-1995-conjugate-compendium,
	address = {Bozeman, Montana},
	author = {Daniel Fink},
	institution = {Environmental Statistics Group, Department of Biology, Montana State Univeristy},
	title = {A Compendium of Conjugate Priors},
	url = {http://www.people.cornell.edu/pages/df36/CONJINTRnewTEX.pdf},
	year = {1995}
}

@article{Zabell-1992,
	abstract = {A major difficulty for currently existing theories of
inductive inference involves the question of what to do when novel, unknown, or
previously unsuspected phenomena occur. In this paper one particular instance of
this difficulty is considered, the so-called sampling of species problem. The
classical probabilistic theories of inductive inference due to Laplace, Johnson,
de Finetti, and Carnap adopt a model of simple enumerative induction in which
there are a prespecified number of types or species which may be observed. But,
realistically, this is often not the case. In 1838 the English mathematician
Augustus De Morgan proposed a modification of the Laplacian model to accommodate
situations where the possible types or species to be observed are not assumed to
be known in advance; but he did not advance a justification for his solution. In
this paper a general philosophical approach to such problems is suggested,
drawing on work of the English mathematician J. F. C. Kingman. It then emerges
that the solution advanced by De Morgan has a very deep, if not totally
unexpected, justification. The key idea is that although lsquoexchangeablersquo
random sequences are the right objects to consider when all possible
outcome-types are known in advance, exchangeable random partitions are the right
objects to consider when they are not. The result turns out to be very
satisfying. The classical theory has several basic elements: a representation
theorem for the general exchangeable sequence (the de Finetti representation
theorem), a distinguished class of sequences (those employing Dirichlet priors),
and a corresponding rule of succession (the continuum of inductive methods). The
new theory has parallel basic elements: a representation theorem for the general
exchangeable random partition (the Kingman representation theorem), a
distinguished class of random partitions (the Poisson-Dirichlet process), and a
rule of succession which corresponds to De Morgan's rule.},
	annote = {ook op papier},
	author = {Sandy L. Zabell},
	doi = {10.1007/BF00485351},
	journal = {Synthese},
	localfile = {article/Zabell-1992.pdf},
	number = {2},
	pages = {205–232},
	publisher = {Harvard Business School Publication Corp.},
	title = {Predicting the unpredictable},
	volume = {90},
	year = {1992}
}

@article{Inuiguchi-Ramik-2000,
	abstract = {In this paper, we review some fuzzy linear programming
methods and techniques from a practical point of view. In the first part, the
general history and the approach of fuzzy mathematical programming are
introduced. Using a numerical example, some models of fuzzy linear programming
are described. In the second part of the paper, fuzzy mathematical programming
approaches are compared to stochastic programming ones. The advantages and
disadvantages of fuzzy mathematical programming approaches are exemplified in
the setting of an optimal portfolio selection problem. Finally, some newly
developed ideas and techniques in fuzzy mathematical programming are briefly
reviewed.},
	author = {Masahiro Inuiguchi and Jaroslav Ramík},
	doi = {10.1016/S0165-0114(98)00449-7},
	issn = {0165-0114},
	journal = {Fuzzy Sets and Systems},
	keywords = {Fuzzy constraint; Fuzzy goal; Fuzzy mathematical programming; Necessity measure; Portfolio selection; Possibility measure; Simplex method; Stochastic programming},
	localfile = {article/Inuiguchi-Ramik-2000.pdf},
	number = {1},
	pages = {3–28},
	title = {Possibilistic linear programming: a brief review of fuzzy mathematical programming and a comparison with stochastic programming in portfolio selection problem},
	volume = {111},
	year = {2000}
}

@book{Protein-Atlas-1978,
	editor = {M. O. Dayhoff},
	title = {Atlas of Protein Sequence and Structure},
	year = {1978}
}

@article{Hart-MasColell-2001,
	author = {Sergiu Hart and Andreu Mas-Colell},
	doi = {10.1006/jeth.2000.2746},
	journal = {Journal of Economic Theory},
	localfile = {article/Hart-MasColell-2001.pdf},
	pages = {26–54},
	title = {A General Class of Adaptive Strategies},
	volume = {98},
	year = {2001}
}

@article{Couso-Moral-Walley-2000-independence,
	abstract = {Our aim in this paper is to clarify the notion of
independence for imprecise probabilities. Suppose that two marginal experiments
are each described by an imprecise probability model, i.e., by a convex set of
probability distributions or an equivalent model such as upper and lower
probabilities or previsions. Then there are several ways to define independence
of the two experiments and to construct an imprecise probability model for the
joint experiment. We survey and compare six definitions of independence. To
clarify the meaning of the definitions and the relationships between them, we
give simple examples which involve drawing balls from urns. For each concept of
independence, we give a mathematical definition, an intuitive or behavioural
interpretation, assumptions under which the definition is justified, and an
example of an urn model to which the definition is applicable. Each of the
independence concepts we study appears to be useful in some kinds of
application. The concepts of strong independence and epistemic independence
appear to be the most frequently applicable.},
	author = {Inés Couso and Serafín Moral and Peter Walley},
	doi = {10.1017/S1357530900000156},
	journal = {Risk, Decision and Policy},
	localfile = {article/Couso-Moral-Walley-2000-independence.pdf},
	number = {2},
	pages = {165–181},
	title = {A survey of concepts of independence for imprecise probabilities},
	volume = {5},
	year = {2000}
}

@article{DeCooman-2002,
	abstract = {Hierarchical models are rather common in uncertainty theory.
They arise when there is a ‘correct’ or ‘ideal’ (the so-called first-order)
uncertainty model about a phenomenon of interest, but the modeler is uncertain
about what it is. The modeler's uncertainty is then called second-order
uncertainty. For most of the hierarchical models in the literature, both the
first- and the second-order models are precise, i.e., they are based on
classical probabilities. In the present paper, I propose a specific hierarchical
model that is imprecise at the second level, which means that at this level,
lower probabilities are used. No restrictions are imposed on the underlying
first-order model: that is allowed to be either precise or imprecise. I argue
that this type of hierarchical model generalizes and includes a number of
existing uncertainty models, such as imprecise probabilities, Bayesian models,
and fuzzy probabilities. The main result of the paper is what I call
precision–imprecision equivalence: the implications of the model for decision
making and statistical reasoning are the same, whether the underlying
first-order model is assumed to be precise or imprecise.},
	annote = {reprint},
	author = {Gert {De Cooman}},
	doi = {10.1016/S0378-3758(01)00209-9},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {coherence; hierarchical uncertainty model; imprecision; natural extension},
	localfile = {article/DeCooman-2002.pdf},
	number = {1},
	pages = {175–198},
	publisher = {Elsevier},
	title = {Precision-imprecision equivalence in a broad class of imprecise hierarchical uncertainty models},
	volume = {105},
	year = {2002}
}

@article{Buehler-1976,
	abstract = {De Finetti has defined coherent previsions and coherent
probabilities, and others have described concepts of coherent actions or
coherent decisions. Here we consider a related concept of coherent preferences.
Willingness to accept one side of a bet is an example of a preference. A set of
preferences is called incoherent if reversal of some subset yields a uniform
increase in utility, as with a sure win for a collection of bets. In both
probability and statistical models (where preferences are conditional on data)
separating hyperplane theorems show that coherence implies existence of a
probability measure from which the preferences could have been inferred.
Relationships to confidence intervals and to decision theory are indicated. No
single definition of coherence is given which covers all cases of interest. The
various cases distinguish between probability and statistical models and between
finite and infinite spaces. No satisfactory theory is given for continuous
statistical models.},
	annote = {ook op papier},
	author = {Robert J. Buehler},
	doi = {10.1214/aos},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	localfile = {article/Buehler-1976.pdf},
	number = {6},
	pages = {1051–1064},
	publisher = {Institute of Mathematical Statistics},
	title = {Coherent preferences},
	url = {http://www.jstor.org/stable/2958578},
	volume = {4},
	year = {1976}
}

@book{Michie-Spiegelhalter-Taylor-1994,
	editor = {D. Michie and D. J. Spiegelhalter and C. C. Taylor},
	title = {Machine Learning, Neural and Statistical Classification},
	url = {http://www.amsta.leeds.ac.uk/~charles/statlog},
	year = {1994}
}

@article{Kadane-Larkey-1982-reply,
	annote = {ook op papier},
	author = {Joseph B. Kadane and Patrick D. Larkey},
	doi = {10.1287/mnsc.28.2.124},
	journal = {Management Science},
	localfile = {article/Kadane-Larkey-1982-reply.pdf},
	number = {2},
	pages = {124},
	publisher = {INFORMS},
	title = {Reply to Professor Harsanyi},
	url = {http://repository.cmu.edu/statistics/38},
	volume = {28},
	year = {1982}
}

@article{Zaffalon-Fagiuoli-2003,
	abstract = {Bayesian networks are models for uncertain reasoning which
are achieving a growing importance also for the data mining task of
classification. Credal networks extend Bayesian nets to sets of distributions,
or credal sets. This paper extends a state-of-the-art Bayesian net for
classification, called tree-augmented naive Bayes classifier, to credal sets
originated from probability intervals. This extension is a basis to address the
fundamental problem of prior ignorance about the distribution that generates the
data, which is a commonplace in data mining applications. This issue is often
neglected, but addressing it properly is a key to ultimately draw reliable
conclusions from the inferred models. In this paper we formalize the new model,
develop an exact linear-time classification algorithm, and evaluate the credal
net-based classifier on a number of real data sets. The empirical analysis shows
that the new classifier is good and reliable, and raises a problem of excessive
caution that is discussed in the paper. Overall, given the favorable trade-off
between expressiveness and efficient computation, the newly proposed classifier
appears to be a good candidate for the wide-scale application of reliable
classifiers based on credal networks, to real and complex tasks.},
	author = {Marco Zaffalon and Enrico Fagiuoli},
	doi = {10.1023/A:1025822321743},
	journal = {Reliable Computing},
	number = {6},
	pages = {487–509},
	title = {Tree-Based Credal Networks for Classification},
	volume = {9},
	year = {2003}
}

@article{BenTal-Nemirovski-2002,
	abstract = {Robust Optimization (RO) is a modeling methodology, combined
with computational tools, to process optimization problems in which the data are
uncertain and is only known to belong to some uncertainty set. The paper surveys
the main results of RO as applied to uncertain linear, conic quadratic and
semidefinite programming. For these cases, computationally tractable robust
counterparts of uncertain problems are explicitly obtained, or good
approximations of these counterparts are proposed, making RO a useful tool for
real-world applications. We discuss some of these applications, specifically:
antenna design, truss topology design and stability analysis/synthesis in
uncertain dynamic systems. We also describe a case study of 90 LPs from the
NETLIB collection. The study reveals that the feasibility properties of the
usual solutions of real world LPs can be severely affected by small
perturbations of the data and that the RO methodology can be successfully used
to overcome this phenomenon.},
	author = {Aharon Ben-Tal and Arkadi Nemirovski},
	doi = {10.1007/s101070100286},
	issn = {0025-5610},
	journal = {Mathematical Programming},
	localfile = {article/BenTal-Nemirovski-2002.pdf},
	number = {3},
	pages = {453–480},
	publisher = {Springer},
	title = {Robust optimization – methodology and applications},
	volume = {92},
	year = {2002}
}

@article{Morris-1982,
	author = {Carl N. Morris},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Morris-1982.pdf},
	number = {1},
	pages = {65–80},
	publisher = {Institute of Mathematical Statistics},
	title = {Natural exponential families with quadratic variance functions},
	volume = {10},
	year = {1982}
}

@article{Krogh-etal-1994,
	abstract = {Hidden Markov Models (HMMs) are applied to the problems of
statistical modeling, database searching and multiple sequence alignment of
protein families and protein domains. These methods are demonstrated on the
globin family, the protein kinase catalytic domain, and the EF-hand calcium
binding motif. In each case the parameters of an HMM are estimated from a
training set of unaligned sequences. After the HMM is built, it is used to
obtain a multiple alignment of all the training sequences. It is also used to
search the SWISS-PROT 22 database for other sequences that are members of the
given protein family, or contain the given domain. The HMM produces multiple
alignments of good quality that agree closely with the alignments produced by
programs that incorporate three-dimensional structural information. When
employed in discrimination tests (by examining how closely the sequences in a
database fit the globin, kinase and EF-hand HMMs), the HMM is able to
distinguish members of these families from non-members with a high degree of
accuracy. Both the HMM and PROFILESEARCH (a technique used to search for
relationships between a protein sequence and multiply aligned sequences) perform
better in these tests than PROSITE (a dictionary of sites and patterns in
proteins). The HMM appears to have a slight advantage over PROFILESEARCH in
terms of lower rates of false negatives and false positives, even though the HMM
is trained using only unaligned sequences, whereas PROFILESEARCH requires
aligned training sequences. Our results suggest the presence of an EF-hand
calcium binding motif in a highly conserved and evolutionary preserved putative
intracellular region of 155 residues in the [alpha]-1 subunit of L-type calcium
channels which play an important role in excitation-contraction coupling. This
region has been suggested to contain the functional domains that are typical or
essential for all L-type calcium channels regardless of whether they couple to
ryanodine receptors, conduct ions or both.},
	author = {Anders Krogh and Michael Brown and I. Saira Mian and Kimmen Sjölander and David Haussler},
	doi = {10.1006/jmbi.1994.1104},
	issn = {0022-2836},
	journal = {Journal of Molecular Biology},
	keywords = {EF-hand; globin; hidden Markov models; kinase},
	localfile = {article/Krogh-etal-1994.pdf},
	number = {5},
	pages = {1501–1531},
	title = {Hidden Markov models in computational biology: Applications to protein modeling},
	volume = {235},
	year = {1994}
}

@article{Diaconis-Freedman-1980-partial-xch,
	author = {Persi Diaconis and D. Freedman},
	doi = {10.1214/aop},
	journal = {The Annals of Probability},
	localfile = {article/Diaconis-Freedman-1980-partial-xch.pdf},
	number = {1},
	pages = {115–130},
	title = {De Finetti's theorem for Markov chains},
	url = {http://www.jstor.org/stable/2243063},
	volume = {8},
	year = {1980}
}

@techreport{Murphy-2001-graphical,
	author = {Kevin P. Murphy},
	institution = {University of British Columbia},
	title = {An introduction to graphical models},
	url = {http://people.cs.ubc.ca/~murphyk/Papers/intro_gm.pdf},
	year = {2001}
}

@book{Huzurbazar-1976,
	address = {New York},
	author = {Vasant S. Huzurbazar},
	editor = {Anant M. Kshirsagar},
	publisher = {Marcel Dekker},
	series = {Statistics: Textbooks and Monographs},
	title = {Sufficient Statistics: Selected Contributions},
	volume = {19},
	year = {1976}
}

@article{Kadane-Schervish-Seidenfeld-1996,
	abstract = {When can a Bayesian select an hypothesis H and design an
experiment (or a sequence of experiments) to make certain that, given the
experimental outcome(s), the posterior probability of H will be greater than its
prior probability? We discuss an elementary result that establishes sufficient
conditions under which this reasoning to a foregone conclusion cannot occur. We
illustrate how when the sufficient conditions fail, because probability is
finitely but not countably additive, it may be that a Bayesian can design an
experiment to lead his/her posterior probability into a foregone conclusion. The
problem has a decision theoretic version in which a Bayesian might rationally
pay not to see the outcome of certain cost-free experiments, which we discuss
from several perspectives. Also, we relate this issue in Bayesian hypothesis
testing to various concerns about "optional stopping."},
	author = {Joseph B. Kadane and Mark J. Schervish and Teddy Seidenfeld},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {coherence; finite additivity; sequential tests; stopping rules; value of information},
	localfile = {article/Kadane-Schervish-Seidenfeld-1996-foregone.pdf},
	month = sep,
	number = {435},
	pages = {1228–1235},
	publisher = {American Statistical Association},
	title = {Reasoning to a foregone conclusion},
	url = {http://www.jstor.org/stable/2291741},
	volume = {91},
	year = {1996}
}

@inproceedings{Quaeghebeur-DeCooman-2003-games,
	address = {Waterloo, Ontario, Canada},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	booktitle = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Jean-Marc Bernard and Teddy Seidenfeld and Marco Zaffalon},
	location = {Lugano, Switzerland},
	pages = {452–466},
	publisher = {Carleton Scientific},
	series = {Proceedings in Informatics},
	title = {Game-Theoretic Learning Using the Imprecise Dirichlet Model},
	volume = {18},
	year = {2003}
}

@article{Moral-2005-desir,
	abstract = {This paper studies graphoid properties for epistemic
irrelevance in sets of desirable gambles. For that aim, the basic operations of
conditioning and marginalization are expressed in terms of variables. Then, it
is shown that epistemic irrelevance is an asymmetric graphoid. The intersection
property is verified in probability theory when the global probability
distribution is positive in all the values. Here it is always verified due to
the handling of zero probabilities in sets of gambles. An asymmetrical
D-separation principle is also presented, by which this type of independence
relationships can be represented in directed acyclic graphs.},
	annote = {ook op papier},
	author = {Serafín Moral},
	doi = {10.1007/s10472-005-9011-0},
	journal = {Annals of Mathematics and Artificial Intelligence},
	keywords = {Desirable gambles; conditioning; epistemic independence; epistemic irrelevance; imprecise probabilities},
	localfile = {article/Moral-2005-desir.pdf},
	pages = {197–214},
	title = {Epistemic irrelevance on sets of desirable gambles},
	volume = {45},
	year = {2005}
}

@techreport{Schervish-Seidenfeld-Kadane-1998,
	author = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
	institution = {Carnegie Mellon University},
	number = {660},
	title = {Two Measures of Incoherence: How Not to Gamble If You Must},
	url = {http://www.stat.cmu.edu/tr/tr660/tr660.html},
	year = {1998}
}

@article{Buckley-Qu-1991,
	author = {J. J. Buckley and Y. Qu},
	doi = {10.1016/0165-0114(91)90019-M},
	journal = {Fuzzy Sets and Systems},
	keywords = {algebra; fuzzy number},
	localfile = {article/Buckley-Qu-1991.pdf},
	number = {1},
	pages = {33–43},
	publisher = {Elsevier},
	title = {Solving systems of linear fuzzy equations},
	volume = {43},
	year = {1991}
}

@article{Dyer-1983,
	abstract = {The computational complexity of problems relating to the
enumeration of all the vertices of a convex polyhedron defined by linear
inequalities is examined. Several published approaches are evaluated in this
light. A new algorithm is described, and shown to have a better complexity
estimate than existing methods. Empirical evidence supporting the theoretical
superiority is presented. Finally vertex enumeration is discussed when the space
containing the polyhedra is of fixed dimension and only the size of the
inequality system is permitted to vary.},
	author = {M. E. Dyer},
	issn = {0364-765X},
	journal = {Mathematics of Operations Research},
	localfile = {article/Dyer-1983.pdf},
	number = {3},
	pages = {381–402},
	publisher = {INFORMS},
	title = {The complexity of vertex enumeration methods},
	url = {http://www.jstor.org/stable/3689308},
	volume = {8},
	year = {1983}
}

@phdthesis{Quaeghebeur-2010-phd,
	author = {Liesbet Quaeghebeur},
	school = {Universiteit Antwerpen},
	title = {A Philosophy of Everyday, Face-to-face Conversation},
	year = {2010}
}

@book{Dubois-Prade-1988,
	address = {Paris},
	author = {Didier Dubois and Henri Prade},
	edition = {2},
	publisher = {Masson},
	title = {Théorie des possibilités: applications à la représentation des connaissances en informatique},
	year = {1988}
}

@article{Soyster-1973,
	abstract = {This note formulates a convex mathematical programming
problem in which the usual definition of the feasible region is replaced by a
significantly different strategy. Instead of specifying the feasible region by a
set of convex inequalities, fi(x)≤ bi, i=1,2,⋯,m, the feasible region is defined
via set containment. Here n convex activity sets {Kj, j=1,2,⋯,n} and a convex
resource set K are specified and the feasible region is given by X={x ∈
R^{n}|x\_{1}K\_{1}+x\_{2}K\_{2}+⋯ +x\_{n}K\_{n}\subseteq K,\ x\_{j}\geq 0},
where the binary operation + refers to addition of sets. The problem is then to
find x∈ X that maximizes the linear function c· x. When the resource set has a
special form, this problem is solved via an auxiliary linear-programming problem
and application to inexact linear programming is possible.},
	author = {A.L. L. Soyster},
	issn = {0030-364X},
	journal = {Operations Research},
	localfile = {article/Soyster-1973.pdf},
	number = {5},
	pages = {1154–1157},
	publisher = {INFORMS},
	title = {Convex programming with set-inclusive constraints and applications to inexact linear programming},
	url = {http://www.jstor.org/stable/168933},
	volume = {21},
	year = {1973}
}

@article{GutierrezPena-Smith-1997-review,
	author = {Eduardo Gutiérrez-Peña and Adrian F. M. Smith},
	doi = {10.1007/BF02564426},
	journal = {Test},
	localfile = {article/GutierrezPena-Smith-1997-review.pdf},
	number = {1},
	pages = {1–90},
	publisher = {Springer},
	title = {Exponential and Bayesian conjugate families: review and extensions},
	volume = {6},
	year = {1997}
}

@article{vonNeumann-1928,
	author = {John von Neumann},
	doi = {10.1007/BF01448847},
	journal = {Mathematische Annalen},
	localfile = {article/vonNeumann-1928.pdf},
	number = {1},
	pages = {295–320},
	title = {Zur Theorie der Gesellschaftsspiele},
	volume = {100},
	year = {1928}
}

@inproceedings{Miranda-Troffaes-destercke-2008-SMPS,
	annote = {extended version on paper; available from authors},
	author = {Enrique Miranda and Matthias C. M. Troffaes and Sébastien Destercke},
	booktitle = {SMPS},
	doi = {10.1007/978-3-540-85027-4_29},
	editor = {Didier Dubois and María Asunción Lubiano and Henri Prade and María Angeles Gil and Przemysław Grzegorzewski and Olgierd Hryniewicz},
	isbn = {978-3-540-85026-7},
	pages = {235–242},
	publisher = {Springer},
	series = {Advances in Soft Computing},
	title = {Generalised p-Boxes on Totally Ordered Spaces},
	volume = {48},
	year = {2008}
}

@techreport{Fioretti-2001-Shackle,
	abstract = {Evidence Theory is a branch of mathematics that concerns the
combination of empirical evidence in an individual's mind in order to construct
a coherent picture of reality. Designed to deal with unexpected empirical
evidence suggesting new possibilities, evidence theory has a lot in common with
Shackle's idea of decision-making as a creative act. This essay investigates
this connection in detail, pointing to the usefulness of evidence theory to
formalise and extend Shackle's decision theory. In order to ease a proper
framing of the issues involved, evidence theory is not only compared with
Shackle's ideas but also with additive and sub-additive probability theories.
Furthermore, the presentation of evidence theory does not refer to the original
version only, but takes account of its most recent developments, too.},
	author = {Guido Fioretti},
	institution = {Università di Firenze and ICER, Torino},
	title = {A mathematical theory of evidence for G. L. S. Shackle},
	year = {2001}
}

@phdthesis{Kriegler-2005,
	author = {Elmar Kriegler},
	school = {Universität Potsdam},
	title = {Imprecise Probability Analysis for Integrated Assessment of Climate Change},
	year = {2005}
}

@inproceedings{Williams-1974,
	author = {Peter M. Williams},
	booktitle = {Formal methods in the methodology of empirical sciences: Proceedings of the conference for formal methods in the methodology of empirical sciences},
	editor = {Marian Przełęcki and Klemens Szaniawski and Ryszard Wójcicki},
	pages = {229–246},
	publisher = {D. Reidel Publishing Company and Ossolineum Publishing company},
	title = {Indeterminate probabilities},
	year = {1974}
}

@article{Antonucci-Zaffalon-2008,
	abstract = {Credal networks are models that extend Bayesian nets to deal
with imprecision in probability, and can actually be regarded as sets of
Bayesian nets. Credal nets appear to be powerful means to represent and deal
with many important and challenging problems in uncertain reasoning. We give
examples to show that some of these problems can only be modeled by credal nets
called non-separately specified. These, however, are still missing a graphical
representation language and updating algorithms. The situation is quite the
opposite with separately specified credal nets, which have been the subject of
much study and algorithmic development. This paper gives two major
contributions. First, it delivers a new graphical language to formulate any type
of credal network, both separately and non-separately specified. Second, it
shows that any non-separately specified net represented with the new language
can be easily transformed into an equivalent separately specified net, defined
over a larger domain. This result opens up a number of new outlooks and concrete
outcomes: first of all, it immediately enables the existing algorithms for
separately specified credal nets to be applied to non-separately specified ones.
We explore this possibility for the 2U algorithm: an algorithm for exact
updating of singly connected credal nets, which is extended by our results to a
class of non-separately specified models. We also consider the problem of
inference on Bayesian networks, when the reason that prevents some of the
variables from being observed is unknown. The problem is first reformulated in
the new graphical language, and then mapped into an equivalent problem on a
separately specified net. This provides a first algorithmic approach to this
kind of inference, which is also proved to be NP-hard by similar transformations
based on our formalism.},
	author = {Alessandro Antonucci and Marco Zaffalon},
	doi = {10.1016/j.ijar.2008.02.005},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Bayesian networks; Conservative inference rule; Conservative updating; Credal networks; Credal sets; Imprecise probabilities; Probabilistic graphical models},
	localfile = {article/Antonucci-Zaffalon-2008.pdf},
	number = {2},
	pages = {345–361},
	title = {Decision-theoretic specification of credal networks: A unified language for uncertain modeling with sets of Bayesian networks},
	volume = {49},
	year = {2008}
}

@article{Kschischang-Frey-Loeliger-2001,
	abstract = {Algorithms that must deal with complicated global functions
of many variables often exploit the manner in which the given functions factor
as a product of ldquo;local rdquo; functions, each of which depends on a subset
of the variables. Such a factorization can be visualized with a bipartite graph
that we call a factor graph, In this tutorial paper, we present a generic
message-passing algorithm, the sum-product algorithm, that operates in a factor
graph. Following a single, simple computational rule, the sum-product algorithm
computes-either exactly or approximately-various marginal functions derived from
the global function. A wide variety of algorithms developed in artificial
intelligence, signal processing, and digital communications can be derived as
specific instances of the sum-product algorithm, including the forward/backward
algorithm, the Viterbi algorithm, the iterative ldquo;turbo rdquo; decoding
algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks,
the Kalman filter, and certain fast Fourier transform (FFT) algorithms},
	author = {F. R. Kschischang and B. J. Frey and H.-A. Loeliger},
	doi = {10.1109/18.910572},
	issn = {0018-9448},
	journal = {IEEE Transactions on Information Theory},
	keywords = {Bayesian networks; FFT algorithms; HMM; Kalman filter; Kalman filters; Viterbi algorithm; Viterbi decoding; artificial intelligence; belief networks; belief propagation algorithm; bipartite graph; computational rule; digital communication; digital communications; factor graphs; factorization; fast Fourier transform; fast Fourier transforms; forward/backward algorithm; functional analysis; generic message-passing algorithm; global function; global functions; graph theory; hidden Markov models; iterative decoding; iterative turbo decoding algorithm; local functions; marginal functions; message passing; signal processing; sum-product algorithm; turbo codes},
	localfile = {article/Kschischang-Frey-Loeliger-2001.pdf},
	number = {2},
	pages = {498–519},
	title = {Factor graphs and the sum-product algorithm},
	volume = {47},
	year = {2001}
}

@article{Seidenfeld-1985,
	abstract = {Can there be good reasons for judging one set of
probabilistic assertions more reliable than a second? There are many candidates
for measuring "goodness" of probabilistic forecasts. Here, I focus on one such
aspirant: calibration. Calibration requires an alignment of announced
probabilities and observed relative frequency, e.g., 50 percent of forecasts
made with the announced probability of .5 occur, 70 percent of forecasts made
with probability .7 occur, etc. To summarize the conclusions: (i) Surveys
designed to display calibration curves, from which a recalibration is to be
calculated, are useless without due consideration for the interconnections
between questions (forecasts) in the survey. (ii) Subject to feedback,
calibration in the long run is otiose. It gives no ground for validating one
coherent opinion over another as each coherent forecaster is (almost) sure of
his own long-run calibration. (iii) Calibration in the short run is an
inducement to hedge forecasts. A calibration score, in the short run, is
improper. It gives the forecaster reason to feign violation of total evidence by
enticing him to use the more predictable frequencies in a larger finite
reference class than that directly relevant.},
	author = {Teddy Seidenfeld},
	journal = {Philosophy of Science},
	localfile = {article/Seidenfeld-1985.pdf},
	number = {2},
	pages = {274–294},
	title = {Calibration, coherence, and scoring rules},
	url = {http://www.jstor.org/stable/187511},
	volume = {52},
	year = {1985}
}

@book{DeFinetti-1970-book,
	author = {Bruno de Finetti},
	publisher = {Giulio Einaudi},
	title = {Teoria Delle Probabilità},
	year = {1970}
}

@article{Avis-Fukuda-1992,
	author = {David Avis and Komei Fukuda},
	doi = {10.1007/BF02293050},
	journal = {Discrete \& Computational Geometry},
	localfile = {article/Avis-Fukuda-1992.pdf},
	number = {1},
	pages = {295–313},
	publisher = {Springer},
	title = {A pivoting algorithm for convex hulls and vertex enumeration of arrangements and polyhedra},
	url = {http://www.digizeitschriften.de/dms/img/?PPN=GDZPPN000365548},
	volume = {8},
	year = {1992}
}

@article{Georgakopoulos-Kavvadias-Papadimitriou-1988,
	annote = {ook op papier},
	author = {George Georgakopoulos and Dimitris Kavvadias and Christos H. Papadimitriou},
	doi = {10.1016/0885-064X(88)90006-4},
	journal = {Journal of Complexity},
	localfile = {article/Georgakopoulos-Kavvadias-Papadimitriou-1988.pdf},
	number = {1},
	pages = {1–11},
	publisher = {Elsevier},
	title = {Probabilistic satisfiability},
	volume = {4},
	year = {1988}
}

@incollection{Jimenez-etal-2005,
	author = {F. Jiménez and G. Sánchez and J. Cadenas and A. Gómez-Skarmeta and J. Verdegay},
	booktitle = {Computational Intelligence, Theory and Applications},
	doi = {10.1007/3-540-31182-3_66},
	editor = {Bernd Reusch},
	isbn = {978-3-540-22807-3},
	localfile = {incollection/Jimenez-etal-2005.pdf},
	pages = {713–722},
	publisher = {Springer},
	series = {Advances in Soft Computing},
	title = {Nonlinear Optimization with Fuzzy Constraints by Multi-Objective Evolutionary Algorithms},
	volume = {33},
	year = {2005}
}

@article{Ide-Cozman-2008,
	abstract = {This paper presents a family of algorithms for approximate
inference in credal networks (that is, models based on directed acyclic graphs
and set-valued probabilities) that contain only binary variables. Such networks
can represent incomplete or vague beliefs, lack of data, and disagreements among
experts; they can also encode models based on belief functions and possibilistic
measures. All algorithms for approximate inference in this paper rely on exact
inferences in credal networks based on polytrees with binary variables, as these
inferences have polynomial complexity. We are inspired by approximate algorithms
for Bayesian networks; thus the Loopy 2U algorithm resembles Loopy Belief
Propagation, while the Iterated Partial Evaluation and Structured Variational 2U
algorithms are, respectively, based on Localized Partial Evaluation and
variational techniques.},
	author = {Jaime Shinsuke Ide and Fabio Gagliardi Cozman},
	doi = {10.1016/j.ijar.2007.09.003},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {2U algorithm; Credal networks; Loopy Belief Propagation; Variational methods},
	localfile = {article/Ide-Cozman-2008.pdf},
	number = {1},
	pages = {275–296},
	title = {Approximate algorithms for credal networks with binary variables},
	volume = {48},
	year = {2008}
}

@article{Milne-1993,
	abstract = {Taking as starting point two familiar interpretations of
probability, we develop these in a perhaps unfamiliar way to arrive ultimately
at an improbable claim concerning the proper axiomatization of probability
theory: the domain of definition of a point-valued probability distribution is
an orthomodular partially ordered set. Similar claims have been made in the
light of quantum mechanics but here the motivation is intrinsically
probabilistic. This being so the main task is to investigate what light, if any,
this sheds on quantum mechanics. In particular it is important to know under
what conditions these point-valued distributions can be thought of as derived
from distribution-pairs of upper and lower probabilities on boolean algebras.
Generalising known results this investigation unsurprisingly proves unrewarding.
In the light of this failure the next topic investigated is how these
generalized probability distributions are to be interpreted.},
	author = {Peter Milne},
	doi = {10.1007/BF01049259},
	journal = {Journal of Philosophical Logic},
	pages = {129–168},
	title = {The foundations of probability and quantum mechanics},
	volume = {22},
	year = {1993}
}

@article{Dietzenbacher-1994-Perron-Frobenius,
	abstract = {The dominant eigenvalue and the corresponding eigenvector
(or Perron vector) of a non-linear eigensystem are considered. We discuss the
effects upon these, of perturbations and of aggregation of the underlying
mapping. The results are applied to study the sensitivity of the outputs in a
non-linear input-output model. For that purpose, it is shown that the
input-output model can be rewritten as a non-linear eigensystem. It turns out
that the Perron vector of this eigensystem includes the solution vector of the
input-output model.},
	author = {Erik Dietzenbacher},
	doi = {10.1016/0304-4068(94)90033-7},
	journal = {Journal of Mathematical Economics},
	localfile = {article/Dietzenbacher-1994-Perron-Frobenius.pdf},
	number = {1},
	pages = {21–31},
	publisher = {Elsevier},
	title = {The non-linear Perron-Frobenius theorem: Perturbations and aggregation},
	volume = {23},
	year = {1994}
}

@inproceedings{Quaeghebeur-DeCooman-2004,
	address = {Helvoirt, The Netherlands},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	booktitle = {Book of Abstracts 23rd Benelux Meeting on Systems and Control},
	editor = {Bram de Jager and Vincent Verdult},
	pages = {113},
	title = {Command line completion: an illustration of learning and decision making using the imprecise Dirichlet model},
	year = {2004}
}

@article{DeCampos-Huete-Moral-1994-intervals,
	abstract = {We study probability intervals as an interesting tool to
represent uncertain information. A number of basic operations necessary to
develop a calculus with probability intervals, such as combination,
marginalization, conditioning and integration are studied in detail. Moreover,
probability intervals are compared with other uncertainty theories, such as
lower and upper probabilities, Choquet capacities of order two and belief and
plausibility functions. The advantages of probability intervals with respect to
these formalisms in computational efficiency are also highlighted.},
	annote = {op papier},
	author = {Luis M. de Campos and Juan F. Huete and Serafín Moral},
	doi = {10.1142/S0218488594000146},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	keywords = {Combination; Conditioning; Lower and Upper Probability; Marginalization; Probability Intervals; Uncertainty Management},
	localfile = {article/DeCampos-Huete-Moral-1994-intervals.pdf},
	number = {2},
	pages = {167–196},
	title = {Probability intervals: a tool for uncertain reasoning},
	url = {http://decsai.ugr.es/~lci/journal-papers-pdf/ijuf94.pdf},
	volume = {2},
	year = {1994}
}

@article{DeCooman-Hermans-2008-bridging,
	abstract = {We give an overview of two approaches to probability theory
where lower and upper probabilities, rather than probabilities, are used:
Walley's behavioural theory of imprecise probabilities, and Shafer and Vovk's
game-theoretic account of probability. We show that the two theories are more
closely related than would be suspected at first sight, and we establish a
correspondence between them that (i) has an interesting interpretation, and (ii)
allows us to freely import results from one theory into the other. Our approach
leads to an account of probability trees and random processes in the framework
of Walley's theory. We indicate how our results can be used to reduce the
computational complexity of dealing with imprecision in probability trees, and
we prove an interesting and quite general version of the weak law of large
numbers.},
	author = {Gert {De Cooman} and Filip Hermans},
	doi = {10.1016/j.artint.2008.03.001},
	journal = {Artificial Intelligence},
	keywords = {Coherence; Conglomerability; Event tree; Game-theoretic probability; Hoeffding’s inequality; Immediate prediction; Imprecise probabilities; Imprecise probability tree; Law of large numbers; Lower prevision; Markov chain; Prequential Principle; Probability tree; Random process},
	localfile = {article/DeCooman-Hermans-2008-bridging.pdf},
	number = {11},
	pages = {1400–1427},
	publisher = {Elsevier},
	title = {Imprecise probability trees: Bridging two theories of imprecise probability},
	volume = {172},
	year = {2008}
}

@article{DeCooman-Quaeghebeur-Miranda-2007-exchangeable,
	author = {Gert {De Cooman} and Erik Quaeghebeur and Enrique Miranda},
	doi = {10.3150/09-BEJ182},
	journal = {Bernoulli},
	number = {3},
	pages = {721–735},
	title = {Exchangeable lower previsions},
	volume = {15},
	year = {2009}
}

@article{Billingsley-1961,
	abstract = {This paper is an expository survey of the mathematical
aspects of statistical inference as it applies to finite Markov chains, the
problem being to draw inferences about the transition probabilities from one
long, unbroken observation {x\_1, x\_2, ⋯, x\_n} on the chain. The topics
covered include Whittle's formula, chi-square and maximum-likelihood methods,
estimation of parameters, and multiple Markov chains. At the end of the paper it
is briefly indicated how these methods can be applied to a process with an
arbitrary state space or a continuous time parameter. Section 2 contains a
simple proof of Whittle's formula; Section 3 provides an elementary and
self-contained development of the limit theory required for the application of
chi-square methods to finite chains. In the remainder of the paper, the results
are accompanied by references to the literature, rather than by complete proofs.
As is usual in a review paper, the emphasis reflects the author's interests.
Other general accounts of statistical inference on Markov processes will be
found in Grenander [53], Bartlett [9] and [10], Fortet [35], and in my monograph
[18]. I would like to thank Paul Meier for a number of very helpful discussions
on the topics treated in this paper, particularly those of Section 3.},
	annote = {geannoteerde kopie},
	author = {Patrick Billingsley},
	journal = {The Annals of Mathematical Statistics},
	localfile = {article/Billingsley-1961.pdf},
	number = {1},
	pages = {12–40},
	title = {Statistical methods in Markov chains},
	url = {http://www.jstor.org/stable/2237603},
	volume = {32},
	year = {1961}
}

@article{Augustin-Coolen-2004,
	abstract = {The assumption A(n), proposed by Hill (J. Amer. Statist.
Assoc. 63 (1968) 677), provides a natural basis for low structure non-parametric
predictive inference, and has been justified in the Bayesian framework. This
paper embeds A(n)-based inference into the theory of interval probability, by
showing that the corresponding bounds are totally monotone F-probability and
coherent. Similar attractive internal consistency results are proven to hold for
conditioning and updating.},
	author = {Thomas Augustin and Frank P. A. Coolen},
	doi = {10.1016/j.jspi.2003.07.003},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {A(n); Capacities; Conditioning; Consistency; Imprecise probabilities; Interval probability; Low structure inference; Non- parametrics; Predictive inference; Updating},
	localfile = {article/Augustin-Coolen-2004.pdf},
	number = {2},
	pages = {251–272},
	title = {Nonparametric predictive inference and interval probability},
	volume = {124},
	year = {2004}
}

@techreport{Walley-1981,
	annote = {only the part on 2-monotonicity},
	author = {Peter Walley},
	institution = {Department of statistics, University of Warwick},
	title = {Coherent lower (and upper) probabilities},
	year = {1981}
}

@article{Nash-1951,
	annote = {ook op papier},
	author = {John Nash},
	journal = {The Annals of Mathematics},
	localfile = {article/Nash-1951.pdf},
	number = {2},
	pages = {286–295},
	title = {Non-cooperative games},
	url = {http://www.jstor.org/stable/1969529},
	volume = {54},
	year = {1951}
}

@mastersthesis{Walter-2006,
	author = {Gero Walter},
	localfile = {mastersthesis/Walter-2006.pdf},
	school = {Ludwig-Maximilians-Universität München},
	title = {Robuste Bayes-Regression mit Mengen von Prioris – Ein Beitrag zur Statistik unter komplexer Unsicherheit},
	url = {http://www.stat.uni-muenchen.de/~thomas/team/diplomathesis_GeroWalter.pdf},
	year = {2006}
}

@article{Trump-Prautzsch-1996,
	abstract = {In this paper we present fast algorithms to raise the degree
n of a simplicial Bézier representation of degree n to arbitrarily high degree.
Each Bézier point of some (n + r)th degree representation can be computed in a
simplicial recursive scheme of depth n. In the case of curves the recurrence
relation reveals that the (n + r)th degree Bézier polygon can also be obtained
by inserting r knots into some nth degree spline which provides a very fast
algorithm. Furthermore, a short new proof is given for the fact that the Bézier
nets of a multivariate polynomial converge to the polynomial under repeated
degree elevation.},
	author = {Wilfried Trump and Hartmut Prautzsch},
	doi = {10.1016/0167-8396(95)00031-3},
	journal = {Computer Aided Geometric Design},
	keywords = {Bézier curves; Bézier simplices; Bézier triangles; b-splines; convergence; elevation; knot insertion; pyramidal schemes; repeated degree; simplicial recursions},
	localfile = {article/Trump-Prautzsch-1996.pdf},
	number = {5},
	pages = {387–398},
	publisher = {Elsevier},
	title = {Arbitrarily high degree elevation of Bézier representations},
	volume = {13},
	year = {1996}
}

@article{Miranda-Zaffalon-2011,
	abstract = {We detail the relationship between sets of desirable gambles
and conditional lower previsions. The former is one the most general models of
uncertainty. The latter corresponds to Walley’s celebrated theory of imprecise
probability. We consider two avenues: when a collection of conditional lower
previsions is derived from a set of desirable gambles, and its converse. In
either case, we relate the properties of the derived model with those of the
originating one. Our results constitute basic tools to move from one formalism
to the other, and thus to take advantage of work done in the two fronts.},
	author = {Enrique Miranda and Marco Zaffalon},
	doi = {10.1007/s10472-011-9231-4},
	issn = {1012-2443},
	issue = {3},
	journal = {Annals of Mathematics and Artificial Intelligence},
	keyword = {Computer Science},
	keywords = {coherence; equal expressivity; lower and upper previsions; mathematics subject classifications; natural extension; sets of desirable gambles},
	localfile = {article/Miranda-Zaffalon-2011.pdf},
	pages = {251–309},
	publisher = {Springer Netherlands},
	title = {Notes on desirability and conditional lower previsions},
	volume = {60},
	year = {2011}
}

@proceedings{WWW-2002,
	booktitle = {Proceedings of the Eleventh International World Wide Web Conference},
	title = {Proceedings of the Eleventh International World Wide Web Conference},
	year = {2002}
}

@inproceedings{Couso-Moral-2009-ISIPTA,
	address = {Durham, United Kingdom},
	author = {Inés Couso and Serafín Moral},
	booktitle = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
	editor = {Thomas Augustin and Frank P. A. Coolen and Serafin Moral and Matthias C. M. Troffaes},
	organization = {SIPTA},
	pages = {99–108},
	title = {Sets of desirable gambles and credal sets},
	url = {http://www.sipta.org/isipta09/proceedings/063.html},
	year = {2009}
}

@article{Mantel-1976-tails,
	author = {Nathan Mantel},
	journal = {The American Statistician},
	localfile = {article/Mantel-1976-tails.pdf},
	number = {1},
	pages = {14–17},
	title = {Tails of Distributions},
	url = {http://www.jstor.org/stable/2682880},
	volume = {30},
	year = {1976}
}

@book{Jaynes-2003,
	author = {E. T. Jaynes},
	editor = {J. Larry Bretthorst},
	publisher = {Cambridge University Press},
	title = {Probability Theory: The Logic of Science},
	year = {2003}
}

@incollection{Shapley-1964-twoperson,
	author = {Lloyd S. Shapley},
	booktitle = {Advances in game theory},
	editor = {D. Dresher and L. S. Shapley and A. W. Tucker},
	publisher = {Princeton University Press},
	title = {Some topics in two-person games},
	year = {1964}
}

@article{Shannon-1948,
	annote = {ook op papier},
	author = {Claude E. Shannon},
	journal = {The Bell System Technical Journal},
	localfile = {article/Shannon-1948.pdf},
	pages = {379–423,623–656},
	title = {A Mathematical Theory of Communication},
	url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
	volume = {27},
	year = {1948}
}

@inproceedings{Schervish-Seidenfeld-Kadane-1999-incoherence,
	abstract = {We introduce two indices for the degree of incoherence in a
set of lower and upper previsions: maximizing the rate of loss the incoherent
Bookie experiences in a Dutch Book, or maximizing the rate of profit the Gambler
achieves who makes Dutch Book against the incoherent Bookie. We report how
efficient bookmaking is achieved against these two indices in the case of
incoherent previsions for events on a finite partition, and for incoherent
previsions that include also a simple random variable. We relate the
epsilon-contamination model to efficient bookmaking in the case of the rate of
profit.},
	address = {Ghent, Belgium},
	author = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
	booktitle = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
	editor = {Gert {De Cooman} and Fabio Gagliardi Cozman and Serafin Moral and Peter Walley},
	keywords = {Dutch Book; coherence; epsilon-contamination mod},
	pages = {319–323},
	title = {How Sets of Coherent Probabilities may Serve as Models for Degrees of Incoherence},
	url = {http://decsai.ugr.es/~smc/isipta99/proc/072.html},
	year = {1999}
}

@phdthesis{Jaeger-1995,
	address = {Saarbrücken},
	author = {Manfred Jaeger},
	school = {Universität des Saarlandes},
	title = {Default Reasoning about Probabilities},
	year = {1995}
}

@book{Cowell-etal-2003-expert,
	author = {R. G. Cowell and A. Philip Dawid and Steffen L. Lauritzen and D. J. Spiegelhalter},
	isbn = {978-0-387-98767-5},
	publisher = {Springer},
	series = {Information Science and Statistics},
	title = {Probabilistic Networks and Expert Systems},
	year = {2003}
}

@article{Kunreuther-et-al-2002,
	abstract = {This paper reviews the state of the art of research on
individual decision-making in high-stakes, low-probability settings. A central
theme is that resolving high-stakes decisions optimally poses a formidable
challenge not only to naïve decision makers, but also to users of more
sophisticated tools, such as decision analysis. Such decisions are difficult to
make because precise information about probabilities is not available, and the
dynamics of the decision are complex. When faced with such problems, naïve
decision-makers fall prey to a wide range of potentially harmful biases, such as
failing to recognize a high-stakes problem, ignoring the information about
probabilities that does exist, and responding to complexity by accepting the
status quo. A proposed agenda for future research focuses on how the process and
outcomes of high-stakes decision making might be improved.},
	author = {Howard C. Kunreuther and Robert Meyer and Richard Zeckhauser and Paul Slovic and Barry Schwartz and Christian Schade and Mary Frances Luce and Steven Lippman and David H. Krantz and Barbara Kahn and Robin Hogarth},
	doi = {10.1023/A:1020287225409},
	journal = {Marketing Letters},
	keywords = {decision biases; decision heuristics; decision making under certainty; high-stakes decisions; risky decision making},
	localfile = {article/Kunreuther-et-al-2002.pdf},
	number = {3},
	pages = {259–268},
	publisher = {Springer},
	title = {High stakes decision making: Normative, descriptive and prescriptive considerations},
	volume = {13},
	year = {2002}
}

@article{McKinney-1962,
	author = {Richard L. McKinney},
	doi = {10.1090/S0002-9947-1962-0147879-X},
	journal = {Transactions of the American Mathematical Society},
	localfile = {article/McKinney-1962.pdf},
	pages = {131–148},
	title = {Positive bases for linear spaces},
	url = {http://www.ams.org/journals/tran/1962-103-01/S0002-9947-1962-0147879-X/S0002-99
47-1962-0147879-X.pdf},
	volume = {103},
	year = {1962}
}

@article{Hartfiel-1991,
	abstract = {Let T be a non-empty subset of n x n stochastic matrices.
Define T2={A1A2 | A1,A2∈T}, T3= {A1A2A3 | A1,A2,A3∈T}, ⋯. The sequence T1,T2,⋯
is called a Markov set-chain. An important problem in this area is to determine
when such a set-chain converges. This paper gives a notion of a sequential
limiting set and shows how it can be used to obtain a result on set-chain
convergence.},
	author = {Darald J. Hartfiel},
	issn = {0021-9002},
	journal = {Journal of Applied Probability},
	localfile = {article/Hartfiel-1991.pdf},
	number = {4},
	pages = {910–913},
	publisher = {Applied Probability Trust},
	title = {Sequential limits in Markov set-chains},
	url = {http://www.jstor.org/stable/3214695},
	volume = {28},
	year = {1991}
}

@article{Coolen-Augustin-2009-IDM-alternative,
	abstract = {Nonparametric predictive inference (NPI) is a general
methodology to learn from data in the absence of prior knowledge and without
adding unjustified assumptions. This paper develops NPI for multinomial data
when the total number of possible categories for the data is known. We present
the upper and lower probabilities for events involving the next observation and
several of their properties. We also comment on differences between this NPI
approach and corresponding inferences based on Walley's Imprecise Dirichlet
Model.},
	annote = {Special Section on The Imprecise Dirichlet Model and Special Section on Bayesian Robustness (Issues in Imprecise Probability)},
	author = {Frank P. A. Coolen and Thomas Augustin},
	doi = {10.1016/j.ijar.2008.03.011},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Circular A(n); Imprecise Dirichlet Model; Imprecise probabilities; Interval probability; Lower and upper probabilities; Multinomial data; Nonparametric predictive inference; Rule of succession},
	localfile = {article/Coolen-Augustin-2008-IDM-alternative.pdf},
	number = {2},
	pages = {217–230},
	publisher = {Elsevier},
	title = {A nonparametric predictive alternative to the Imprecise Dirichlet Model: the case of a known number of categories},
	volume = {50},
	year = {2009}
}

@proceedings{ISIPTA-1999,
	address = {Ghent, Belgium},
	booktitle = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
	editor = {Gert {De Cooman} and Fabio Gagliardi Cozman and Serafin Moral and Peter Walley},
	title = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise Probabilities and Their Applications},
	year = {1999}
}

@article{Koopman-1940-axioms,
	author = {B. O. Koopman},
	issn = {0003-486X},
	journal = {The Annals of Mathematics},
	localfile = {article/Koopman-1940-axioms.pdf},
	number = {2},
	pages = {269–292},
	title = {The axioms and algebra of intuitive probability},
	url = {http://www.jstor.org/stable/1969003},
	volume = {41},
	year = {1940}
}

@misc{Manski-2003,
	author = {Charles F. Manski},
	title = {Partial identification of probability distributions},
	year = {2003}
}

@article{Inuiguchi-2006,
	author = {Masahiro Inuiguchi},
	journal = {Kybernetika},
	keywords = {bender; fuzzy linear programming; necessity measure; oblique fuzzy vector; s},
	localfile = {article/Inuiguchi-2006.pdf},
	number = {4},
	pages = {441–452},
	publisher = {THE ACADEMY OF SCIENCES OF THE CZECH REPUBLIC},
	title = {A necessity measure optimization approach to linear programming problems with oblique fuzzy vectors},
	url = {http://hdl.handle.net/10338.dmlcz/135726},
	volume = {42},
	year = {2006}
}

@inproceedings{Fuller-Zimmermann-1992,
	author = {Robert Fullér and Hans-Jürgen Zimmermann},
	booktitle = {Proceedings of 2nd International Workshop on Current Issues in Fuzzy Technologies},
	title = {Approximate Reasoning for Solving Fuzzy Linear Programming Problems},
	year = {1992}
}

@article{Northrop-1936-prob-in-QM,
	author = {F. S. C. Northrop},
	journal = {Philosophy of Science},
	localfile = {article/Northrop-1936-prob-in-QM.pdf},
	number = {2},
	pages = {215–232},
	title = {The Philosophical Significance of the Concept of Probability in Quantum Mechanics},
	url = {http://www.jstor.org/stable/184348},
	volume = {3},
	year = {1936}
}

@misc{Cozman-2003,
	author = {Fabio Gagliardi Cozman},
	note = {slide printout},
	title = {Graph-Theoretical Models for Multivariate Modeling with Imprecise Probabilities},
	year = {2003}
}

@article{Avis-Bremner-Seidel-1997,
	abstract = {A convex polytope P can be specified in two ways: as the
convex hull of the vertex set V of P, or as the intersection of the set H of its
facet-inducing halfspaces. The vertex enumeration problem is to compute V from
H. The facet enumeration problem is to compute H from V. These two problems are
essentially equivalent under point/hyperplane duality. They are among the
central computational problems in the theory of polytopes. It is open whether
they can be solved in time polynomial in |H| + |V| and the dimension. In this
paper we consider the main known classes of algorithms for solving these
problems. We argue that they all have at least one of two weaknesses: inability
to deal well with "degeneracies", or, inability to control the sizes of
intermediate results. We then introduce families of polytopes that exercise
those weaknesses. Roughly speaking, fat-lattice or intricate polytopes cause
algorithms with bad degeneracy handling to perform badly; dwarfed polytopes
cause algorithms with bad intermediate size control to perform badly. We also
present computational experience with trying to solve these problem on these
hard polytopes, using various implementations of the main algorithms.},
	annote = {ook op papier},
	author = {David Avis and David Bremner and Raimund Seidel},
	doi = {10.1016/S0925-7721(96)00023-5},
	journal = {Computational Geometry},
	keywords = {Convex hulls; Convex polytopes; Lattice complexity; Triangulation complexity; Vertex enumeration},
	localfile = {article/Avis-Bremner-Seidel-1997.pdf},
	pages = {265–301},
	title = {How good are convex hull algorithms?},
	volume = {7},
	year = {1997}
}

@article{OHara-OHara-1999,
	author = {K. O'Hara and J. O'Hara},
	doi = {10.1046/j.1464-410x.1999.0830s1079.x},
	issn = {1464-4096},
	journal = {BJU international},
	keywords = {Age Distribution; Circumcision; Coitus; Female; Humans; Interpersonal Relations; Male; Male: adverse effects; Male: psychology; Orgasm; Sexual Behavior; Sexual Partners; Sexual Partners: psychology},
	localfile = {article/OHara-OHara-1999.pdf},
	month = jan,
	number = {Supplement 1},
	pages = {79–84},
	pmid = {10349418},
	title = {The effect of male circumcision on the sexual enjoyment of the female partner.},
	volume = {83},
	year = {1999}
}

@article{Genest-MacKay-1986,
	author = {Christian Genest and Jock MacKay},
	journal = {The American Statistician},
	keywords = {Archimedean copulas; Fréchet bounds; Kendall's tau; fixed marginals; singular distributions},
	localfile = {article/Genest-MacKay-1986.pdf},
	number = {4},
	pages = {280–283},
	title = {The joy of copulas: bivariate distributions with uniform marginals},
	url = {http://www.jstor.org/stable/2684602},
	volume = {40},
	year = {1986}
}

@article{Rudolph-1996-duality,
	abstract = {Using elementary module theory, an intrinsic definition of
the dual (or adjoint) of a generalized time-varying linear system is given. With
this, the duality of controllability and observability is recovered from their
intrinsic module theoretical definitions. The duality of state feedback and
output injection is discussed both in the static case and for its quasistatic
generalization. Related Brunovsky type canonical forms are derived in the
quasistatic case. The corresponding controllability indices and their duals the
observability indices are defined intrinsically.},
	author = {J. Rudolph},
	doi = {10.1016/0024-3795(94)00222-3},
	issn = {0024-3795},
	journal = {Linear Algebra and its Applications},
	localfile = {article/Rudolph-1996-duality.pdf},
	pages = {83–106},
	title = {Duality in time-varying linear systems: a module theoretic approach},
	volume = {245},
	year = {1996}
}

@article{Benavoli-et-al-2010,
	abstract = {We extend hidden Markov models for continuous variables
taking into account imprecision in our knowledge about the probabilistic
relationships involved. To achieve that, we consider sets of probabilities, also
called coherent lower previsions. In addition to the general formulation, we
study in detail a particular case of interest: linear-vacuous mixtures. We also
show, in a practical case, that our extension outperforms the Kalman filter when
modelling errors are present in the system.},
	address = {Seattle, Washington},
	annote = {ook op papier},
	author = {Alessio Benavoli and Marco Zaffalon and Enrique Miranda},
	journal = {IEEE Transactions on Automatic Control},
	localfile = {article/Benavoli-et-al-2010.pdf},
	organization = {IEEE},
	pages = {1743–1750},
	title = {A new robust approach to filtering based on coherent lower previsions},
	year = {2009}
}

@proceedings{IPMU-2002,
	booktitle = {Proceedings of the Ninth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2002},
	title = {Proceedings of the Ninth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2002},
	year = {2002}
}

@article{Savage-1971,
	author = {Leonard J. Savage},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Savage-1971.pdf},
	number = {336},
	pages = {783–801},
	title = {Elicitation of personal probabilities and expectations},
	url = {http://www.jstor.org/stable/2284229},
	volume = {66},
	year = {1971}
}

@techreport{Minka-2001,
	author = {Thomas P. Minka},
	title = {Bayesian linear regression},
	year = {2001}
}

@article{Lange-1995,
	author = {Kenneth Lange},
	doi = {10.1007/BF01441156},
	journal = {Genetica},
	localfile = {article/Lange-1995.pdf},
	number = {1},
	pages = {107–117},
	publisher = {Springer},
	title = {Applications of the Dirichlet distribution to forensic match probabilities},
	volume = {96},
	year = {1995}
}

@techreport{Miranda-2008-updating,
	annote = {ook op papier},
	author = {Enrique Miranda},
	institution = {Department of Statistics and Operations Research, Rey Juan Carlos University},
	title = {Updating coherent previsions on finite spaces},
	year = {2008}
}

@article{Goodhardt-Ehrenberg-Chatfield-1984,
	abstract = {The Dirichlet is a stochastic model of purchase incidence
and brand choice which parsimoniously integrates a wide range of already
well-established empirical regularities.},
	annote = {ook op papier},
	author = {G. J. Goodhardt and A. S. C. Ehrenberg and C. Chatfield},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	localfile = {article/Goodhardt-Ehrenberg-Chatfield-1984.pdf},
	number = {5},
	pages = {621–655},
	title = {The Dirichlet: a comprehensive model of buying behaviour},
	url = {http://www.jstor.org/stable/2981696},
	volume = {147},
	year = {1984}
}

@article{DeCooman-Troffaes-Miranda-2005,
	abstract = {We study n-monotone lower previsions, which constitute a
generalisation of n-monotone lower probabilities. We investigate their relation
with the concepts of coherence and natural extension in the behavioural theory
of imprecise probabilities, and improve along the way upon a number of results
from the literature.},
	author = {Gert {De Cooman} and Matthias C. M. Troffaes and Enrique Miranda},
	journal = {Journal of Intelligent and Fuzzy Systems},
	keywords = {Choquet integral; coherence; comonotone additivity; n-monotonicity; natural extension},
	localfile = {article/DeCooman-Troffaes-Miranda-2005-Kerre.pdf},
	number = {4},
	pages = {253–263},
	publisher = {IOS Press},
	title = {n-Monotone lower previsions and lower integrals},
	url = {http://iospress.metapress.com/content/22bh7djyjk86a55h},
	volume = {16},
	year = {2005}
}

@book{Geisser-1993,
	author = {Seymour Geisser},
	number = {55},
	publisher = {Chapman \& Hall},
	series = {Monographs on Statistics and Applied Probability},
	title = {Predictive Inference: An Introduction},
	year = {1993}
}

@inproceedings{DeCooman-Quaeghebeur-2010-IPMU,
	address = {Berlin},
	author = {Gert {De Cooman} and Erik Quaeghebeur},
	booktitle = {Communications in Computer and Information Science},
	doi = {10.1007/978-3-642-14055-6_7},
	editor = {Eyke Hüllermeier and Rudolf Kruse and Frank Hoffmann},
	isbn = {978-3-64214054-9},
	issn = {1865-0929},
	pages = {60–69},
	publisher = {Springer},
	title = {Infinite exchangeability for sets of desirable gambles},
	volume = {80},
	year = {2010}
}

@article{Zaffalon-DeCooman-2005-editorial,
	annote = {reprint},
	author = {Marco Zaffalon and Gert {De Cooman}},
	doi = {10.1007/s10472-005-9009-7},
	journal = {Annals of Mathematics and Artificial Intelligence},
	pages = {1–4},
	title = {Editorial: Imprecise probability perspectives on artificial intelligence},
	volume = {45},
	year = {2005}
}

@book{Schneier-1996-crypto,
	annote = {stukken uit H5,6 op papier},
	author = {Bruce Schneier},
	publisher = {John Wiley \& Sons},
	title = {Applied Cryptography},
	url = {http://www.schneier.com/book-applied.html},
	year = {1996}
}

@inbook{DeFinetti-1972-notation,
	annote = {Translation of \cite{DeFinetti-1967} by Leonard J. Savage geringde kopie},
	author = {Bruno de Finetti},
	booktitle = {Probability, Induction and Statistics (The art of guessing)},
	pages = {xviii–xxiv},
	publisher = {John Wiley \& Sons},
	title = {A Useful Notation},
	year = {1972}
}

@article{Harsanyi-1982-rejoinder,
	annote = {ook op papier},
	author = {John C. Harsanyi},
	doi = {10.1287/mnsc.28.2.124a},
	journal = {Management Science},
	localfile = {article/Harsanyi-1982-rejoinder.pdf},
	number = {2},
	pages = {124–125},
	publisher = {INFORMS},
	title = {Rejoinder to professors Kadane and Larkey},
	volume = {28},
	year = {1982}
}

@article{Mazaheri-Nasri-2007,
	author = {H. Mazaheri and M. Nasri},
	journal = {International Mathematical Forum},
	localfile = {article/Mazaheri-Nasri-2007.pdf},
	number = {16},
	pages = {747–751},
	title = {Complemented Subspaces in the Normed Spaces},
	url = {http://www.m-hikari.com/imf-password2007/13-16-2007/mazaheriIMF13-16-2007-3.pdf
},
	volume = {2},
	year = {2007}
}

@article{Hofbauer-Sigmund-2003,
	author = {Josef Hofbauer and Karl Sigmund},
	journal = {Bulletin of the American Mathematical Society},
	number = {4},
	pages = {479–519},
	title = {Evolutionary game dynamics},
	volume = {40},
	year = {2003}
}

@article{Antonucci-etal-2009-milident,
	abstract = {Credal networks are imprecise probabilistic graphical models
generalizing Bayesian networks to convex sets of probability mass functions.
This makes credal networks particularly suited to model expert knowledge under
very general conditions, including states of qualitative and incomplete
knowledge. In this paper, we present a credal network for risk evaluation in
case of intrusion of civil aircrafts into a restricted flight area. The
different factors relevant for this evaluation, together with an independence
structure over them, are initially identified. These factors are observed by
sensors, whose reliabilities can be affected by variable external factors, and
even by the behaviour of the intruder. A model of these observation processes,
and the necessary fusion scheme for the information returned by the sensors
measuring the same factor, are both completely embedded into the structure of
the credal network. A pool of experts, facilitated in their task by specific
techniques to convert qualitative judgements into imprecise probabilistic
assessments, has made possible the quantification of the network. We show the
capabilities of the proposed model by means of some preliminary tests referred
to simulated scenarios. Overall, we can regard this application as a useful tool
to support military experts in their decision, but also as a quite general
imprecise-probability paradigm for information fusion.},
	author = {Alessandro Antonucci and Ralph Brühlmann and Alberto Piatti and Marco Zaffalon},
	doi = {10.1016/j.ijar.2009.01.005},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Credal networks; Information fusion; Sensor management; Tracking systems},
	localfile = {article/Antonucci-etal-2009-milident.pdf},
	number = {4},
	pages = {666–679},
	title = {Credal networks for military identification problems},
	volume = {50},
	year = {2009}
}

@book{Boole-1854,
	author = {George Boole},
	publisher = {Macmillan},
	title = {The laws of thought},
	url = {http://www.gutenberg.org/ebooks/15114},
	year = {1854}
}

@inproceedings{Miranda-DeCooman-Couso-2002,
	abstract = {We discuss how lower previsions induced by multi-valued
mappings fit into the framework of the behavioural theory of imprecise
probabilities, and show how the notions of coherence and natural extension from
that theory can be used to prove and generalise existing results in an elegant
and straightforward manner. This provides a clear example for their explanatory
and unifying power.},
	annote = {uitgebreide versie, ook op papier},
	author = {Enrique Miranda and Gert {De Cooman} and Inés Couso},
	booktitle = {Proceedings of the Ninth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2002},
	keywords = {coherence; conditioning; evidence theory},
	title = {Lower previsions induced by multi-valued mappings},
	year = {2002}
}

@book{Ziegler-1995,
	author = {Günter M. Ziegler},
	publisher = {Springer},
	title = {Lectures on Polytopes},
	year = {1995}
}

@article{Bildikar-Patil-1968,
	abstract = {Let \mathbf{x} and \mathbf{$\theta$} denote s-dimensional
column vectors. The components x\_1, x\_2,⋯ x\_s of \mathbf{x} are random
variables jointly following an s-variate distribution and components
$\theta$\_1, $\theta$\_2,⋯, $\theta$\_s of \mathbf{$\theta$} are real numbers.
The random vector \mathbf{x} is said to follow an s-variate Exponential-type
distribution with the parameter vector (pv) \mathbf{$\theta$}, if its
probability function (pf) is given by \begin{equation*}\tag{1.1} f(\mathbf{x},
\mathbf{$\theta$}) = h(\mathbf{x}) \exp {\mathbf{x'$\theta$} -
q(\mathbf{$\theta$})},\end{equation*} \mathbf{x} \varepsilon R\_s and
\mathbf{$\theta$} \varepsilon (\mathbf{a}, \mathbf{b}) \subset R\_s. R\_s
denotes the s-dimensional Euclidean space. The s-dimensional open interval
(\mathbf{a}, \mathbf{b}) may or may not be finite. h(\mathbf{x}) is a function
of \mathbf{x}, independent of \mathbf{$\theta$}, and q(\mathbf{$\theta$}) is a
bounded analytic function of $\theta$\_1, $\theta$\_2,⋯ $\theta$\_s, independent
of \mathbf{x}. We note that f(\mathbf{x}, \mathbf{$\theta$}), given by (1.1),
defines the class of multivariate exponential-type distributions which includes
distributions like multivariate normal, multinomial, multivariate negative
binomial, multivariate logarithmic series, etc. This paper presents a
theoretical study of the structural properties of the class of multivariate
exponential-type distributions. For example, different distributions connected
with a multivariate exponential-type distribution are derived. Statistical
independence of the components x\_1, x\_2,⋯, x\_s is discussed. The problem of
characterization of different distributions in the class is studied under
suitable restrictions on the cumulants. A canonical representation of the
characteristic function of an infinitely divisible (id), purely discrete random
vector, whose moments of second order are all finite, is also obtained.
$\phi$(\mathbf{t}), m(\mathbf{t}), k(\mathbf{t}) denote, throughout this paper,
the characteristic function (ch. f.), the moment generating function (mgf), and
the cumulant generating function (cgf), respectively, of a random vector
\mathbf{x}. The components t\_i of the s-dimensional column vector \mathbf{t}
are all real.},
	annote = {ook op papier},
	author = {Sheela Bildikar and G. P. Patil},
	doi = {10.1214/aoms},
	journal = {The Annals of Mathematical Statistics},
	localfile = {article/Bildikar-Patil-1968.pdf},
	number = {4},
	pages = {1316–1326},
	publisher = {Institute of Mathematical Statistics},
	title = {Multivariate exponential-type distributions},
	volume = {39},
	year = {1968}
}

@book{Laplace-1825-essai,
	address = {Paris},
	author = {Pierre-Simon Laplace},
	edition = {5},
	publisher = {Bachelier},
	title = {Essai philosophique sur les probabilités},
	url = {http://books.google.com/books?id=Ovo3AAAAMAAJ},
	year = {1825}
}

@article{Koopman-1936,
	author = {B. O. Koopman},
	journal = {Transactions of the American Mathematical Society},
	localfile = {article/Koopman-1936.pdf},
	number = {3},
	pages = {399–409},
	title = {On distributions admitting a sufficient statistic},
	url = {http://www.ams.org/journals/tran/1936-039-03/S0002-9947-1936-1501854-3/S0002-99
47-1936-1501854-3.pdf},
	volume = {39},
	year = {1936}
}

@techreport{Nesterov-Palma-2000,
	annote = {geannoteerde reprint},
	author = {Yu. Nesterov and André {De Palma}},
	institution = {Center for Operations Research \& Econometrics, Université catholique de Louvain},
	number = {2000/27},
	title = {Stable dynamics in transportation systems},
	type = {CORE discussion paper},
	year = {2000}
}

@proceedings{IPMU-2006,
	address = {Paris},
	booktitle = {Proceedings of the Eleventh International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
	title = {Information Processing and Management of Uncertainty in Knowledge-based Systems},
	year = {2006}
}

@inproceedings{Dash-Cooper-2002,
	author = {Denver Dash and Gregory F. Cooper},
	booktitle = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
	pages = {91–98},
	title = {Exact model averaging with naive Bayesian classifiers},
	year = {2002}
}

@article{Fisher-1934,
	author = {R. A. Fisher},
	doi = {10.1098/rspa.1934.0050},
	journal = {Proceedings of the Royal Society of London, A},
	localfile = {article/Fisher-1934.pdf},
	pages = {285–307},
	title = {Two new properties of mathematical likelihood},
	volume = {144},
	year = {1934}
}

@article{Neumaier-2003-surprise,
	abstract = {This paper presents a new approach to fuzzy modeling based
on the concept of surprise. The new concept is related to the traditional
membership function by an antitone transformation. Advantages of the surprise
approach include: 1. It has a consistent semantic interpretation. 2. It allows
the joint use of quantitative and qualitative knowledge, using simple rules of
logic. 3. It is a direct extension of (and allows combination with) the
least-squares approach to reconciling conflicting approximate numerical data. 4.
It is ideally suited to optimization under imprecise or conflicting goals,
specified by a combination of soft and hard interval constraints. 5. It gives a
straightforward approach to constructing families of functions consistent with
fuzzy associative memories as used in fuzzy control, with tuning parameters
(reflecting linguistic ambiguity) that can be adapted to available performance
data.},
	author = {Arnold Neumaier},
	doi = {10.1016/S0165-0114(02)00248-8},
	issn = {0165-0114},
	journal = {Fuzzy Sets and Systems},
	localfile = {article/Neumaier-2003-surprise.pdf},
	number = {1},
	pages = {21–38},
	title = {Fuzzy modeling in terms of surprise},
	volume = {135},
	year = {2003}
}

@article{Pelessoni-Vicig-2003-risk,
	abstract = {In this paper the theory of coherent imprecise previsions is
applied to risk measurement. We introduce the notion of coherent risk measure
defined on an arbitrary set of risks, showing that it can be considered a
special case of coherent upper prevision. We also prove that our definition
generalizes the notion of coherence for risk measures defined on a linear space
of random numbers, given in literature. Consistency properties of Value-at-Risk
(VaR), currently one of the most used risk measures, are investigated too,
showing that it does not necessarily satisfy a weaker notion of consistency
called 'avoiding sure loss'. We introduce sufficient conditions for VaR to avoid
sure loss and to be coherent. Finally we discuss ways of modifying incoherent
risk measures into coherent ones.},
	author = {Renato Pelessoni and Paolo Vicig},
	doi = {10.1142/S0218488503002156},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	keywords = {Imprecise prevision; Value-at-Risk; avoiding sure loss condition; coherent risk measure},
	localfile = {article/Pelessoni-Vicig-2003-risk.pdf},
	number = {4},
	pages = {393–412},
	title = {Imprecise previsions for risk measurement},
	volume = {11},
	year = {2003}
}

@article{Dose-2007-gravity,
	author = {Volker Dose},
	doi = {10.1088/0957-0233},
	journal = {Measurement Science and Technology},
	keywords = {gravitational constant; robust estimation},
	localfile = {article/Dose-2007-gravity.pdf},
	pages = {176–182},
	title = {Bayesian estimate of the Newtonian constant of gravitation},
	volume = {18},
	year = {2007}
}

@techreport{Coolen-Augustin-2006-cNPI4,
	author = {Frank P. A. Coolen and Thomas Augustin},
	title = {Nonparametric predictive inference for multinomial data - Notes 4 (known number of categories, interval previsions for gambles, classification, and further comments)},
	year = {2006}
}

@article{Boumont-2002,
	author = {Jean-luc Doumont},
	doi = {10.1109/TPC.2002.805164},
	journal = {IEEE Transactions on Professional Communication},
	localfile = {article/Doumont-2002.pdf},
	month = dec,
	number = {4},
	pages = {291–296},
	title = {The three laws of professional communication},
	volume = {45},
	year = {2002}
}

@book{Carnap-1952,
	address = {Chicago},
	author = {Rudoplh Carnap},
	publisher = {The University of Chicago Press},
	title = {The Continuum of Inductive Methods},
	year = {1952}
}

@inproceedings{Brown-1951,
	author = {George W. Brown},
	booktitle = {Activity analysis of production and allocation},
	editor = {Tjalling C. Koopmans},
	number = {13},
	organization = {Cowles Commission for Research in Economics},
	pages = {374–376},
	series = {Cowles Commission Monographs},
	title = {Iterative solution of games by fictitious play},
	year = {1951}
}

@article{Halpern-2010,
	abstract = {The relationship between Popper spaces (conditional
probability spaces that satisfy some regularity conditions), lexicographic
probability systems (LPS's), and nonstandard probability spaces (NPS's) is
considered. If countable additivity is assumed, Popper spaces and a subclass of
LPS's are equivalent; without the assumption of countable additivity, the
equivalence no longer holds. If the state space is finite, LPS's are equivalent
to NPS's. However, if the state space is infinite, NPS's are shown to be more
general than LPS's.},
	author = {Joseph Y. Halpern},
	doi = {10.1016/j.geb.2009.03.013},
	issn = {0899-8256},
	journal = {Games and Economic Behavior},
	localfile = {article/Halpern-2010.pdf},
	number = {1},
	pages = {155–179},
	title = {Lexicographic probability, conditional probability, and nonstandard probability},
	volume = {68},
	year = {2010}
}

@techreport{DeCooman-1991-margextconv,
	author = {Gert {De Cooman}},
	title = {Marginal extension and convexity},
	year = {2008}
}

@misc{Lauritzen-2004b,
	annote = {Transparanten},
	author = {Steffen L. Lauritzen},
	title = {Exponential Families of Distributions},
	year = {2004}
}

@article{Fishburn-LaValle-1998,
	abstract = {The theory of subjective expected lexicographic utility
brings together two classical developments in expected utility theory. The first
is Hausner's theory of expected lexicographic utility in decision under risk.
The second is a lottery-based theory of subjective expected utility in decision
under uncertainty that was first axiomatized by Anscombe and Aumann. Our
synthesis of the two produces representations of preference in decision
underuncertainty in which utilities are finite-dimensional real vectors ordered
lexicographicallyand subjective probabilities are real matrices. Axiomatizations
of subjective expected lexicographic utility are described for finite and
infinite sets of states. Procedures for assessing vector utilities and matrix
probabilities are outlined.},
	author = {Peter C. Fishburn and Irving H. LaValle},
	doi = {10.1023/A:1018911830478},
	issn = {0254-5330},
	journal = {Annals of Operations Research},
	localfile = {article/Fishburn-LaValle-1998.pdf},
	pages = {183–206},
	publisher = {Springer Netherlands},
	title = {Subjective expected lexicographic utility: Axioms and assessment},
	volume = {80},
	year = {1998}
}

@book{Schechter-1997-HAF,
	author = {Eric Schechter},
	publisher = {Academic Press},
	title = {Handbook of Analysis and Its Foundations},
	year = {1997}
}

@incollection{Seidenfeld-2000-reasonsforFA,
	author = {Teddy Seidenfeld},
	booktitle = {Probability theory: philosophy, recent history and relations to science},
	editor = {Vincent F. Hendricks and Stig Andur Pedersen and Klaus Frovin Jørgensen},
	keywords = {improper regular conditional distributions},
	publisher = {Kluwer Academic Publishers},
	series = {Synthese Library},
	title = {Remarks on the theory of conditional probability: some issues of finite versus countable additivity},
	volume = {297},
	year = {2001}
}

@book{Ito-Kunisch-2008,
	author = {Kazufumi Ito and Karl Kunisch},
	publisher = {SIAM},
	title = {Lagrange Multiplier Approach to Variational Problems and Applications},
	year = {2008}
}

@article{Haldane-1945,
	author = {J. B. S. Haldane},
	journal = {Biometrika},
	localfile = {article/Haldane-1945.pdf},
	number = {3},
	pages = {222–225},
	title = {On a method of estimating frequencies},
	url = {http://www.jstor.org/stable/2332299},
	volume = {33},
	year = {1945}
}

@book{Cramer-1946,
	author = {Harald Cramér},
	publisher = {Princeton University Press},
	title = {Mathematical methods of statistics},
	year = {1946}
}

@article{Halpern-Koller-2004,
	abstract = {Non-deductive reasoning systems are often representation
dependent: representing the same situation in two di erent ways may cause such a
system to return two different answers. Some have viewed this as a significant
problem. For example, the principle of maximum entropy has been subjected to
much criticism due to its representation dependence. There has, however, been
almost no work investigating representation dependence. In this paper, we
formalize this notion and show that it is not a problem specific to maximum
entropy. In fact, we show that any representation-independent probabilistic
inference procedure that ignores irrelevant information is essentially
entailment, in a precise sense. Moreover, we show that representation
independence is incompatible with even a weak default assumption of
independence. We then show that invariance under a restricted class of
representation changes can form a reasonable compromise between representation
independence and other desiderata, and provide a construction of a family of
inference procedures that provides such restricted representation independence,
using relative entropy.},
	annote = {ook op papier},
	author = {Joseph Y. Halpern and Daphne Koller},
	doi = {10.1613/jair.1292},
	journal = {Journal of Artificial Intelligence Research},
	localfile = {article/Halpern-Koller-2004.pdf},
	pages = {319–356},
	title = {Representation Dependence in Probabilistic Inference},
	volume = {21},
	year = {2004}
}

@book{Bernardo-Smith-1994,
	author = {José M. Bernardo and Adrian F. M. Smith},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Mathematical Statistics},
	title = {Bayesian theory},
	year = {1994}
}

@manual{DeJongh-html,
	author = {Hans de Jong},
	title = {Handleiding HTML},
	url = {http://www.handleidinghtml.nl}
}

@proceedings{WWW-2000,
	booktitle = {Proceedings of the Ninth International World Wide Web Conference},
	title = {Proceedings of the Ninth International World Wide Web Conference},
	year = {2000}
}

@article{Oshime-1983,
	author = {Yorimasa Oshime},
	journal = {Journal of mathematics of Kyoto university},
	localfile = {article/Oshime-1983-Perron.pdf},
	number = {4},
	pages = {803–830},
	title = {An extension of Morishima's nonlinear Perron-Frobenius theorem},
	url = {http://projecteuclid.org/euclid.kjm/1250521436},
	volume = {23},
	year = {1983}
}

@incollection{VanderGaag-Renooij-Coupe-2007,
	author = {Linda C. van der Gaag and Silja Renooij and Veerle M. H. Coupé},
	booktitle = {Advances in Probabilistic Graphical Models},
	doi = {10.1007/978-3-540-68996-6_5},
	localfile = {inbook/VanderGaag-Renooij-Coupe-2007.pdf},
	pages = {103–124},
	publisher = {Springer},
	title = {Sensitivity analysis of probabilistic networks},
	volume = {124},
	year = {2007}
}

@article{Johnson-1967,
	abstract = {It is shown that the uniqueness of relationship between a
regression function and a prior distribution, in Bates-Neyman type models,
previously established for linear regressions and gamma prior distributions, is
of much more general application.},
	author = {Norman L. Johnson},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Johnson-1967.pdf},
	number = {317},
	pages = {288–289},
	publisher = {American Statistical Association},
	title = {Note on a uniqueness relation in certain accident proneness models},
	url = {http://www.jstor.org/stable/2282931},
	volume = {62},
	year = {1967}
}

@incollection{Shenoy-Shafer-2008-axioms,
	abstract = {In this paper, we describe an abstract framework and axioms
under which exact local computation of marginals is possible. The primitive
objects of the framework are variables and valuations. The primitive operators
of the framework are combination and marginalization. These operate on
valuations. We state three axioms for these operators and we derive the
possibility of local computation from the axioms. Next, we describe a
propagation scheme for computing marginals of a valuation when we have a
factorization of the valuation on a hypertree. Finally we show how the problem
of computing marginals of joint probability distributions and joint belief
functions fits the general framework.},
	author = {Prakash Shenoy and Glenn Shafer},
	booktitle = {Classic Works of the Dempster-Shafer Theory of Belief Functions},
	doi = {10.1007/978-3-540-44792-4_20},
	editor = {Roland Yager and Liping Liu},
	pages = {499–528},
	publisher = {Springer},
	series = {Studies in Fuzziness and Soft Computing},
	title = {Axioms for Probability and Belief-Function Propagation},
	volume = {219},
	year = {2008}
}

@article{Walley-DeCooman-2001,
	author = {Peter Walley and Gert {De Cooman}},
	doi = {10.1016/S0020-0255(01)00090-1},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {Imprecise probabilities; Linguistic information; Monotonic predicate; Plausibility ordering; Possibility distribution; Prototype theory; Vagueness},
	localfile = {article/Walley-DeCooman-2001.pdf},
	number = {1-4},
	pages = {1–37},
	title = {A behavioral model for linguistic uncertainty},
	volume = {134},
	year = {2001}
}

@phdthesis{Quaeghebeur-2009-phd,
	author = {Erik Quaeghebeur},
	school = {Ghent University},
	title = {Learning from samples using coherent lower previsions},
	year = {2009}
}

@inproceedings{Quaeghebeur-DeCooman-Hermans-2011,
	author = {Erik Quaeghebeur and Gert {De Cooman} and Filip Hermans},
	booktitle = {ISIPTA '11: Program and Abstracts},
	keywords = {acceptability; desirability; favorability; indifference; preference},
	pages = {29},
	title = {Generalizing nonstrict and strict preference desirability},
	url = {http://hdl.handle.net/1854/LU-1863955},
	year = {2011}
}

@article{Schervish-1989-forecaster,
	author = {Mark J. Schervish},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Schervish-1989-forecaster.pdf},
	number = {4},
	pages = {1856–1879},
	publisher = {Institute of Mathematical Statistics},
	title = {A general method for comparing probability assessors},
	url = {http://www.jstor.org/stable/2241668},
	volume = {17},
	year = {1989}
}

@inproceedings{Nilim-ElGhaoui-2003-robust-Markov-NIPS,
	author = {Arnab Nilim and Laurent {El Ghaoui}},
	booktitle = {NIPS},
	editor = {Sebastian Thrun and Lawrence K Saul and Bernhard Schölkopf},
	isbn = {0-262-20152-6},
	publisher = {MIT Press},
	title = {Robustness in Markov Decision Problems with Uncertain Transition Matrices},
	year = {2003}
}

@article{Hill-1968-An,
	abstract = {A Bayesian approach to inference about the percentiles and
other characteristics of a finite population is proposed. The approach does not
depend upon, though it need not exclude, the use of parametric models. Some
related questions concerning the existence of exchangeable distributions are
considered. It is shown that there are no countably additive exchangeable
distributions on the space of observations which give ties probability 0 and for
which a next observation is conditionally equally likely to fall in any of the
open intervals between successive order statistics of a given sample.},
	author = {Bruce M. Hill},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Hill-1968-An.pdf},
	number = {322},
	pages = {677–691},
	title = {Posterior distribution of percentiles: Bayes' theorem for sampling from a population},
	url = {http://www.jstor.org/stable/2284038},
	volume = {63},
	year = {1968}
}

@incollection{Wagner-2003-two-dogmas,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	booktitle = {The Epistemology of Keith Lehrer},
	chapter = {9},
	editor = {E. J. Olsson},
	pages = {143–152},
	publisher = {Kluwer Academic Publishers},
	title = {Two dogmas of probabilism},
	year = {2003}
}

@book{Eckel-2000-C++V1,
	author = {Bruce Eckel},
	edition = {2},
	publisher = {Prentice Hall},
	title = {Thinking in C++},
	volume = {1},
	year = {2000}
}

@article{Walley-DeCooman-1999,
	abstract = {Possibility measures and conditional possibility measures
are given a behavioural interpretation as marginal betting rates against events.
Under this interpretation, possibility measures should satisfy two consistency
criteria, known as ‘avoiding sure loss’ and ‘coherence’. We survey the rules
that have been proposed for defining conditional possibilities and investigate
which of them satisfy our consistency criteria in two situations of practical
interest. Only two of these rules satisfy the criteria in both cases studied,
and the conditional possibilities produced by these rules are highly
uninformative. We introduce a new rule that is more informative and is also
coherent in both cases.},
	annote = {reprint},
	author = {Peter Walley and Gert {De Cooman}},
	doi = {10.1016/S0888-613X(99)00007-9},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Dempster's rule; coherence; conditional possibility; imprecise probabilities; natural extension; possibility measure; possibility theory; upper probability},
	localfile = {article/Walley-DeCooman-1999.pdf},
	pages = {63–107},
	title = {Coherence of rules for defining conditional possibility},
	volume = {21},
	year = {1999}
}

@book{Bertsimas-Tsitsiklis-1997,
	author = {Dimitris Bertsimas and John N. Tsitsiklis},
	isbn = {1-886529-19-1},
	keywords = {Linear programming; integer programming; mathematical optimization},
	publisher = {Athena Scientific},
	title = {Introduction to linear optimization},
	year = {1997}
}

@book{Huygens-1920,
	author = {Christiaan Huygens},
	publisher = {Martinus Nijhoff},
	title = {Oevres complètes de Christiaan Huygens},
	volume = {14},
	year = {1920}
}

@article{Zaffalon-Wesnes-Petrini-2003-dementia,
	abstract = {Dementia is a serious personal, medical and social problem.
Recent research indicates early and accurate diagnoses as the key to effectively
cope with it. No definitive cure is available but in some cases when the
impairment is still mild the disease can be contained. This paper describes a
diagnostic tool that jointly uses the naive credal classifier and the most
widely used computerized system of cognitive tests in dementia research, the
Cognitive Drug Research system. The naive credal classifier extends the discrete
naive Bayes classifier to imprecise probabilities. The naive credal classifier
models both prior ignorance and ignorance about the likelihood by sets of
probability distributions. This is a new way to deal with small and incomplete
datasets that departs significantly from most established classification
methods. In the empirical study presented here, the naive credal classifier
provides reliability and unmatched predictive performance. It delivers up to
95\% correct predictions while being very robust with respect to the partial
ignorance due to the largely incomplete data. The diagnostic tool also proves to
be very effective in discriminating between Alzheimer's disease and dementia
with Lewy bodies.},
	author = {Marco Zaffalon and Keith Wesnes and Orlando Petrini},
	doi = {10.1016/S0933-3657(03)00046-0},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Cognitive tests; Credal classification; Dementia},
	number = {1-2},
	pages = {61–79},
	title = {Reliable diagnoses of dementia by the naive credal classifier inferred from incomplete cognitive data},
	volume = {29},
	year = {2003}
}

@article{Fishburn-1986,
	author = {Peter C. Fishburn},
	doi = {10.1214/ss},
	journal = {Statistical Science},
	localfile = {article/Fishburn-1986.pdf},
	number = {3},
	pages = {335–345},
	publisher = {Institute of Mathematical Statistics},
	title = {The axioms of subjective probability},
	volume = {1},
	year = {1986}
}

@book{Kolmogorov-1956,
	address = {New York},
	annote = {English translation of Grundbegriffe der Wahrscheinlichkeit[s]rechnung, 1933},
	author = {A. N. Kolmogorov},
	edition = {Second},
	editor = {Nathan Morisson},
	publisher = {Chelsea publishing company},
	title = {Foundations of the theory of probability},
	year = {1956}
}

@mastersthesis{Quaeghebeur-2002,
	abstract = {This text treats of the problem of predicting the flow of a
river using past flow measurements, rainfall measurements and rainfall
predictions. The objective is to generalize a forecasting method already used to
predict daily consumption of electricity. The developed method will be compared
to Hydromax, an existing riverflow forecasting model. The developed method
simultaneously generates a series of consecutive flow predictions called a flow
curve. This is done by combining separate forecasts for the mean, standard
deviation and the normalized profile of the flow curve. Therefore, three
separate forecasting models will be used, one for each of the aforementioned
flow curve components. To understand the difficulties involved in riverflow
forecasting we first take a look at the relevant hydrological concepts and the
data used in constructing and testing the forecasting method. We will
principally be interested in the prediction of floods, as these phenomena can
have dire socioeconomic consequences if they take place without a timely
warning. Some of the forecasting models will be based on mathematical techniques
derived from the field of artificial neural networks. As an introduction we will
shortly elaborate on this field before presenting a more profound study of the
two derived techniques we will use. These are the so-called `self-organizing
maps' and `radial basis function networks'. An essential part of the forecasting
method are linear and nonlinear regression models. We shall take a look at these
parameterized prediction models, the associated prediction errors and the
parameter estimation involved in constructing them. Thus being well prepared, we
will have at this stage a close look at the Hydromax model and explain the
forecasting method we have developed. They will be compared in terms of the data
needed and the hydrological knowledge involved. Finally, we will show how the
developed method is used in practice. The obtained results will be compared with
those obtained by Hydromax, and remarks will be made about possible
improvements. We will conclude with showing that the developed method holds
promise, but is not yet suitable for practical applications. Suggestions for
further research are also included.},
	author = {Erik Quaeghebeur},
	localfile = {mastersthesis/Quaeghebeur-2002.pdf},
	school = {Université catholique de Louvain},
	title = {Analyse et prédiction de débit de rivières par des méthodes non linéaires},
	url = {http://users.ugent.be/~equaeghe/content/EQ-2002-UCL-memoire-hyperlinked.pdf},
	year = {2002}
}

@article{Ghahramani-2001-HMM+BN-intro,
	abstract = {We provide a tutorial on learning and inference in hidden
Markov models in the context of the recent literature on Bayesian networks. This
perspective makes it possible to consider novel generalizations of hidden Markov
models with multiple hidden state variables, multiscale representations, and
mixed discrete and continuous variables. Although exact inference in these
generalizations is usually intractable, one can use approximate inference
algorithms such as Markov chain sampling and variational methods. We describe
how such methods are applied to these generalized hidden Markov models. We
conclude this review with a discussion of Bayesian methods for model selection
in generalized HMMs.},
	author = {Zoubin Ghahramani},
	doi = {10.1142/S0218001401000836},
	journal = {International Journal of Pattern Recognition and Artificial Intelligence},
	keywords = {Dynamic Bayesian networks; hidden Markov models},
	localfile = {article/Ghahramani-2001-HMM+BN-intro.pdf},
	number = {1},
	pages = {9–42},
	title = {An introduction to hidden Markov models and Bayesian networks},
	volume = {15},
	year = {2001}
}

@book{Abramowitz-Stegun-1972,
	editor = {Milton Abramowitz and Irene A Stegun},
	publisher = {Dover},
	title = {Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical tables},
	url = {http://frameindex.htm},
	year = {1972}
}

@book{Fine-1973,
	address = {New York and London},
	author = {Terrence L. Fine},
	publisher = {Academic Press},
	title = {Theories of Probability (An Examination of Foundations)},
	year = {1973}
}

@article{Basu-Pereira-1983a,
	abstract = {A Skibinsky (1970) characterization of the family of
hypergeometric distributions is re-examined from the point of view of sufficient
experiments and a number of other distributions similarly characterized.},
	annote = {ook op papier},
	author = {D. Basu and Carlos A. B. Pereira},
	journal = {Sankhya Series A},
	localfile = {article/Basu-Pereira-1983a.pdf},
	number = {1},
	pages = {99–104},
	publisher = {Springer},
	title = {A Note on Blackwell Sufficiency and a Skibinsky Characterization of Distributions},
	url = {http://www.jstor.org/stable/25050417},
	volume = {45},
	year = {1983}
}

@article{Fudenberg-Kreps-1993,
	abstract = {We study learning processes for finite strategic-form games,
in which players use the history of past play to forecast play in the current
period. In a generalization of fictitious play, we assume only that players
asymptotically choose best responses to the historical frequencies of opponents′
past play. This implies that if the stage-game strategies converge, the limit is
a Nash equilibrium. In the basic model, plays seems unlikely to converge to a
mixed-strategy equilibrium, but such convergence is natural when the stage game
is perturbed in the manner of Harsanyi′s purification theorem.},
	annote = {geannoteerde kopie},
	author = {Drew Fudenberg},
	doi = {10.1006/game.1993.1021},
	issn = {0899-8256},
	journal = {Games and Economic Behavior},
	localfile = {article/Fudenberg-Kreps-1993.pdf},
	month = jul,
	number = {3},
	pages = {320–367},
	publisher = {MIT Press},
	title = {Learning Mixed Equilibria},
	volume = {5},
	year = {1993}
}

@article{Chateauneuf-Jaffray-1989,
	abstract = {Monotone capacities (on finite sets) of finite or infinite
order (lower probabilities) are characterized by properties of their Möbius
inverses. A necessary property of probabilities dominating a given capacity is
demonstrated through the use of Gale's theorem for the transshipment problem.
This property is shown to be also sufficient if and only if the capacity is
monotone of infinite order. A characterization of dominating probabilities
specific to capacities of order 2 is also proved.},
	annote = {ook op papier},
	author = {Alain Chateauneuf and Jean-Yves Jaffray},
	doi = {10.1016/0165-4896(89)90056-5},
	journal = {Mathematical Social Sciences},
	keywords = {Decision theory; belief funct; lower probability},
	localfile = {article/Chateauneuf-Jaffray-1989.pdf},
	number = {3},
	pages = {263–283},
	publisher = {Elsevier},
	title = {Some characterizations of lower probabilities and other monotone capacities through the use of Möbius inversion},
	volume = {17},
	year = {1989}
}

@article{Wallner-2007-extremepoints,
	abstract = {Every coherent probability (= F-probability) \mathcal{F} on
a finite sample space $Ømega$\_k with k elements defines a set of classical
probabilities in accordance with the interval limits. This set, called
"structure" of , is a convex polytope having dimension \leq k - 1. We prove that
the maximal number of extreme points of structures is exactly k!.},
	author = {Anton Wallner},
	doi = {10.1016/j.ijar.2006.07.017},
	journal = {International Journal of Approximate Reasoning},
	keywords = {0/1-Matrix; Coherent probability; Core; Credal set; Extreme point; F-probability; Interval probability; Polyhedron; Polytope; Structure; Vertex},
	localfile = {article/Wallner-2007-extremepoints.pdf},
	number = {3},
	pages = {339–357},
	title = {Extreme points of coherent probabilities in finite spaces},
	volume = {44},
	year = {2007}
}

@article{Quaeghebeur-DeCooman-2008-ELP-FSS,
	abstract = {We consider lower probabilities on finite possibility spaces
as models for the uncertainty about the state. These generalizations of
classical probabilities can have some interesting properties; for example:
k-monotonicity, avoiding sure loss, coherence, permutation invariance. The sets
formed by all the lower probabilities satisfying zero or more of these
properties are convex. We show how the extreme points and rays of these sets –
the extreme lower probabilities – can be calculated and we give an illustration
of our results.},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	doi = {10.1016/j.fss.2007.11.020},
	journal = {Fuzzy Sets and Systems},
	keywords = {combinatorial problems; extreme points; imprecise probabilities; lower probabilities; non-additive measures},
	localfile = {article/Quaeghebeur-DeCooman-2008-ELP-FSS.pdf},
	month = sep,
	number = {16},
	pages = {2163–2175},
	publisher = {Elsevier},
	title = {Extreme lower probabilities},
	volume = {159},
	year = {2008}
}

@article{Heath-Sudderth-1976-exchangeability,
	abstract = {A simple proof is given for de Finetti's theorem that every
sequence of exchangeable 0-1 random variables is a probability mixture of
sequences of independent, identically distributed variables. The proof can
easily be presented to seniors or first year graduate students of mathematical
statistics and should aid them in understanding the relationship between the
classical and the Bayesian point of view.},
	author = {David Heath and William Sudderth},
	journal = {The American Statistician},
	localfile = {article/Heath-Sudderth-1976-exchangeability.pdf},
	number = {4},
	pages = {188–189},
	title = {De Finetti's theorem on exchangeable variables},
	url = {http://www.jstor.org/stable/2683760},
	volume = {30},
	year = {1976}
}

@book{BangJensen-Gutin-2008-digraphs,
	author = {Jørgen Bang-Jensen and Gregory Gutin},
	edition = {2},
	publisher = {Springer},
	title = {Digraphs: Theory, Algorithms and Applications},
	url = {http://books.google.com/books?id=4UY-ucucWucC},
	year = {2008}
}

@incollection{Garloff-Graf-1999,
	author = {J. Garloff and B. Graf},
	editor = {N. Munro},
	pages = {339–352},
	publisher = {The Institution of Electrical Engineers (IEE)},
	title = {Solving strict polynomial inequalities by Bernstein expansion},
	year = {1999}
}

@unpublished{DeBoor-1995-BBForm,
	author = {C. de Boor},
	keywords = {Bernstein polynomials},
	title = {B-form basics},
	year = {1995}
}

@article{Zimmermann-1985,
	abstract = {Mathematical programming is one of the areas to which fuzzy
set theory has been applied extensively. Primarily based on Bellman and Zadeh's
model of decision in fuzzy environments, models have been suggested which allow
flexibility in constraints and fuzziness in the objective function in
traditional linear and nonlinear programming, in integer and fractional
programming, and in dynamic programming. These models in turn have been used to
offer computationally efficient approaches for solving vector maximum problems.
This paper surveys major models and theories in this area and offers some
indication on future developments which can be expected.},
	author = {Hans-Jürgen Zimmermann},
	doi = {10.1016/0020-0255(85)90025-8},
	issn = {0020-0255},
	journal = {Information Sciences},
	number = {1-2},
	pages = {29–58},
	title = {Applications of fuzzy set theory to mathematical programming},
	volume = {36},
	year = {1985}
}

@article{Wagner-1997-old+new,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Philosophy of Science},
	pages = {677–691},
	title = {Old Evidence and New Explanation},
	volume = {64},
	year = {1997}
}

@article{Friedman-1997,
	abstract = {The classification problem is considered in which an
outputvariable y assumes discrete values with respectiveprobabilities that
depend upon the simultaneous values of a set of input variablesx =
{x\_1,....,x\_n}. At issue is how error in the estimates of theseprobabilities
affects classification error when the estimates are used ina classification
rule. These effects are seen to be somewhat counterintuitive in both their
strength and nature. In particular the bias andvariance components of the
estimation error combine to influenceclassification in a very different way than
with squared error on theprobabilities themselves. Certain types of (very high)
bias can becanceled by low variance to produce accurate classification. This
candramatically mitigate the effect of the bias associated with some
simpleestimators like “naive” Bayes, and the bias induced by
thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy
such simple methods are often competitive with and sometimes superiorto more
sophisticated ones for classification, and why “bagging/aggregating” classifiers
can often improveaccuracy. These results also suggest simple modifications to
theseprocedures that can (sometimes dramatically) further improve
theirclassification performance.},
	author = {Jerome H. Friedman},
	doi = {10.1023/A:1009778005914},
	journal = {Data Mining and Knowledge Discovery},
	keywords = {bagging; bias; classification; curse-of-dimensionality; naive Bayes; nearest-neighbors; variance},
	localfile = {article/Friedman-1997.pdf},
	number = {1},
	pages = {55–77},
	publisher = {Springer},
	title = {On Bias, Variance, 0/1—Loss, and the Curse-of-Dimensionality},
	volume = {1},
	year = {1997}
}

@phdthesis{Troffaes-2005,
	author = {Matthias C. M. Troffaes},
	school = {Universiteit Gent – Ghent University},
	title = {Optimaliteit, onzekerheid, en dynamisch programmeren met onderprevisies – Optimality, Uncertainty, and Dynamic Programming with Lower Previsions},
	year = {2005}
}

@proceedings{ISIPTA-2005,
	address = {Pittsburgh, Pennsylvania},
	booktitle = {ISIPTA '05: Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Fabio Gagliardi Cozman and Robert Nau and Teddy Seidenfeld},
	organization = {SIPTA},
	title = {ISIPTA '05: Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications},
	year = {2005}
}

@book{Rohn-2005,
	author = {Jiri Rohn},
	title = {A Handbook of Results on Interval Linear Problems},
	url = {http://uivtx.cs.cas.cz/~rohn/handbook},
	year = {2005}
}

@article{Wong-1998,
	abstract = {Generalized Dirichlet distribution has a more general
covariance structure than Dirichlet distribution. This makes the generalized
Dirichlet distribution to be more practical and useful. The concept of complete
neutrality will be used to derive the general moment function for the
generalized Dirichlet distribution, and then some properties of the generalized
Dirichlet distribution will be established. Similar to the Dirichlet
distribution, the generalized Dirichlet distribution will be shown to conjugate
to multinominal sampling. Two experiments are designed for studying the
differences between the Dirichlet and the generalized Dirichlet distributions in
Bayesian analysis. A method for generating samples from a generalized Dirichlet
in presented. When a prior distribution is either a Dirichlet or a generalized
Dirichlet distribution, the way for constructing such a prior is discussed.},
	annote = {ook op papier},
	author = {Tzu-Tsung Wong},
	doi = {10.1016/S0096-3003(97)10140-0},
	journal = {Applied Mathematics and Computation},
	keywords = {Bayesian analysis; Completely neutral; Conjugate; Generalized Dirichlet distribution; Prior construction},
	localfile = {article/Wong-1998.pdf},
	number = {2-3},
	pages = {165–181},
	publisher = {Elsevier},
	title = {Generalized Dirichlet distribution in Bayesian analysis},
	volume = {97},
	year = {1998}
}

@book{DeFinetti-1974/1975,
	author = {Bruno de Finetti},
	publisher = {John Wiley \& Sons},
	title = {Theory of Probability},
	year = {1974-1975}
}

@article{Hall-2006-sensitivity-indices,
	abstract = {An uncertainty-based sensitivity index represents the
contribution that uncertainty in model input Xi makes to the uncertainty in
model output Y. This paper addresses the situation where the uncertainties in
the model inputs are expressed as closed convex sets of probability measures, a
situation that exists when inputs are expressed as intervals or sets of
intervals with no particular distribution specified over the intervals, or as
probability distributions with interval-valued parameters. Three different
approaches to measuring uncertainty, and hence uncertainty-based sensitivity,
are explored. Variance-based sensitivity analysis (VBSA) estimates the
contribution that each uncertain input, acting individually or in combination,
makes to variance in the model output. The partial expected value of perfect
information (partial EVPI), quantifies the (financial) value of learning the
true numeric value of an input. For both of these sensitivity indices the
generalization to closed convex sets of probability measures yields lower and
upper sensitivity indices. Finally, the use of relative entropy as an
uncertainty-based sensitivity index is introduced and extended to the imprecise
setting, drawing upon recent work on entropy measures for imprecise
information.},
	author = {Jim W. Hall},
	doi = {10.1016/j.ress.2005.11.042},
	issn = {0951-8320},
	journal = {Reliability Engineering \& System Safety},
	keywords = {Coherent lower and upper probabilities; Entropy-based sensitivity indices; Generalized information theory; Partial expected value of perfect information; Variance-based sensitivity indices},
	localfile = {article/Hall-2006-sensitivity-indices.pdf},
	month = oct,
	number = {10-11},
	pages = {1443–1451},
	publisher = {Elsevier},
	title = {Uncertainty-based sensitivity indices for imprecise probability distributions},
	volume = {91},
	year = {2006}
}

@proceedings{CIPS-1951,
	address = {Paris},
	booktitle = {Congrès international de philosophie des sciences. 4: Calcul des probabilités},
	editor = {Raymond Bayer},
	number = {1146},
	publisher = {Hermann},
	series = {Actualités scientifiques et industrielles},
	title = {Congrès international de philosophie des sciences},
	year = {1951}
}

@book{Hume-1739,
	author = {David Hume},
	edition = {Annotated},
	editor = {David Fate Norton and Mary J. Norton},
	publisher = {Oxford University Press},
	series = {Oxford Philosophical Texts},
	title = {A treatise of human nature},
	year = {1739}
}

@article{Rota-1964-moebius,
	author = {Gian-Carlo Rota},
	doi = {10.1007/BF00531932},
	journal = {Probability Theory and Related Fields},
	localfile = {article/Rota-1964-moebius.pdf},
	month = jan,
	number = {4},
	pages = {340–368},
	title = {On the foundations of combinatorial theory: I. Theory of Möbius functions},
	volume = {2},
	year = {1964}
}

@article{Walley-Pelessoni-Vicig-2004,
	abstract = {We solve two fundamental problems of probabilistic
reasoning: given finitely many conditional probability assessments, how to
determine whether the assessments are mutually consistent, and how to determine
what they imply about the conditional probabilities of other events? These
problems were posed in 1854 by George Boole, who gave a partial solution using
algebraic methods. The two problems are fundamental in applications of the
Bayesian theory of probability; Bruno de Finetti solved the second problem for
the special case of unconditional probability assessments in what he called ‘the
fundamental theorem of probability’. We give examples to show that previous
attempts to solve the two problems, using probabilistic logic and similar
methods, can produce incorrect answers. Using ideas from the theory of imprecise
probability, we show that the general problems have simple, direct solutions
which can be implemented using linear programming algorithms. Unlike earlier
proposals, our methods are formulated directly in terms of the assessments,
without introducing unknown probabilities. Our methods work when any of the
conditioning events may have probability zero, and they work when the
assessments include imprecise (upper and lower) probabilities or previsions. The
main methodological contribution of the paper is to provide general algorithms
for making inferences from any finite collection of (possibly imprecise)
conditional probabilities.},
	author = {Peter Walley and Renato Pelessoni and Paolo Vicig},
	doi = {10.1016/j.jspi.2003.09.005},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Avoiding uniform loss; Bayesian inference; Coherent probabilities; Fundamental theorem of probability; Imprecise probability; Lower probability; Natural extension; Probabilistic logic; Probabilistic reasoning},
	localfile = {article/Walley-Pelessoni-Vicig-2004.pdf},
	number = {1},
	pages = {119–151},
	title = {Direct algorithms for checking consistency and making inferences from conditional probability assessments},
	volume = {126},
	year = {2004}
}

@article{Matheiss-Rubin-1980-vertexenum,
	abstract = {This paper surveys the literature on methods for finding all
vertices of convex polytopes, contrasting the main features of each method and
providing computational results for representative methods.},
	annote = {ook op papier},
	author = {T. H. Matheiss and David S. Rubin},
	issn = {0364-765X},
	journal = {Mathematics of Operations Research},
	localfile = {article/Matheiss-Rubin-1980.pdf},
	number = {2},
	pages = {167–185},
	title = {A survey and comparison of methods for finding all vertices of convex polyhedral sets},
	url = {http://www.jstor.org/stable/3689148},
	volume = {5},
	year = {1980}
}

@article{Friedman-2004,
	annote = {geannoteerde kopie},
	author = {Nir Friedman},
	doi = {10.1126/science.1094068},
	journal = {Science},
	localfile = {article/Friedman-2004.pdf},
	number = {5659},
	pages = {799–805},
	publisher = {American Association for the Advancement of Science},
	title = {Inferring cellular networks using probabilistic graphical models},
	volume = {303},
	year = {2004}
}

@article{Bremner-Fukuda-Marzetta-1998-pd,
	author = {David Bremner and Komei Fukuda and Ambros Marzetta},
	doi = {10.1007/PL00009389},
	journal = {Discrete \& Computational Geometry},
	pages = {333–357},
	title = {Primal-Dual Methods for Vertex and Facet Enumeration},
	url = {http://www.cs.unb.ca/profs/bremner/pd},
	volume = {20},
	year = {1998}
}

@proceedings{CIM-1928,
	address = {Bologna},
	booktitle = {Atti del congresso internationale dei matematici},
	publisher = {N. Zanichelli},
	title = {Atti del congresso internationale dei matematici},
	year = {1928}
}

@article{Goldstein-1983,
	abstract = {We prove that the result EX = E(E(X|Y)) is true, for bounded
X, when the usual concept of conditional expectation or prevision is replaced by
an alternative definition reflecting an individual's actual beliefs concerning X
after observing Y. We discuss the importance of this result to subjectivist
theory.},
	author = {Michael Goldstein},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	keywords = {coherence},
	localfile = {article/Goldstein-1983.pdf},
	month = dec,
	number = {384},
	pages = {817–819},
	publisher = {American Statistical Association},
	title = {The prevision of a prevision},
	url = {http://www.jstor.org/stable/2288190},
	volume = {78},
	year = {1983}
}

@article{Sarukkai-2000-link-prediction,
	abstract = {The enormous growth in the number of documents in the World
Wide Web increases the need for improved link navigation and path analysis
models. Link prediction and path analysis are important problems with a wide
range of applications ranging from personalization to Web server request
prediction. The sheer size of the World Wide Web coupled with the variation in
users' navigation patterns makes this a very difficult sequence modelling
problem. In this paper, the notion of probabilistic link prediction and path
analysis using Markov chains is proposed and evaluated. Markov chains allow the
system to dynamically model the URL access patterns that are observed in
navigation logs based on the previous state. Furthermore, the Markov chain model
can also be used in a generative mode to automatically obtain tours. The Markov
transition matrix can be analysed further using eigenvector decomposition to
obtain `personalized hubs/authorities'. The utility of the Markov chain approach
is demonstrated in many domains: HTTP request prediction, system-driven adaptive
Web navigation, tour generation, and detection of `personalized
hubs/authorities' from user navigation profiles. The generality and power of
Markov chains is a first step towards the application of powerful probabilistic
models to Web path analysis and link prediction.},
	author = {Ramesh R. Sarukkai},
	doi = {10.1016/S1389-1286(00)00044-X},
	journal = {Computer Networks},
	keywords = {Adaptive navigation; HTTP request; Hubs=authorities; Link prediction; Markov chains; Tour generation},
	localfile = {article/Sarukkai-2000-link-prediction.pdf},
	number = {1-6},
	pages = {377–386},
	title = {Link prediction and path analysis using Markov chains},
	volume = {33},
	year = {2000}
}

@article{Walley-2002-reconciling,
	abstract = {This paper describes the author's research connecting the
empirical analysis of treatment response with the normative analysis of
treatment choice under ambiguity. Imagine a planner who must choose a treatment
rule assigning a treatment to each member of a heterogeneous population of
interest. The planner observes certain covariates for each person. Each member
of the population has a response function mapping treatments into a real-valued
outcome of interest. Suppose that the planner wants to choose a treatment rule
that maximizes the population mean outcome. An optimal rule assigns to each
member of the population a treatment that maximizes mean outcome conditional on
the person's observed covariates. However, identification problems in the
empirical analysis of treatment response commonly prevent planners from knowing
the conditional mean outcomes associated with alternative treatments; hence
planners commonly face problems of treatment choice under ambiguity. The
research surveyed here characterizes this ambiguity in practical settings where
the planner may be able to bound but not identify the relevant conditional mean
outcomes. The statistical problem of treatment choice using finite-sample data
is discussed as well.},
	author = {Peter Walley},
	doi = {10.1016/S0378-3758(01)00204-X},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Conditional inference; Consistency function; Contamination neighborhood; Foundations of statistics; Frequentist principle; Imprecise Beta model; Imprecise probability; Upper probability},
	localfile = {article/Walley-2002-reconciling.pdf},
	pages = {35–65},
	title = {Reconciling frequentist properties with the likelihood principle},
	volume = {105},
	year = {2002}
}

@book{Rockafellar-1970,
	address = {Princeton, New Jersey},
	author = {R. Tyrell Rockafellar},
	publisher = {Princeton University Press},
	series = {Princeton Landmarks in Mathematics},
	title = {Convex Analysis},
	year = {1970}
}

@article{Cozman-2000-cn,
	abstract = {This paper presents a complete theory of credal networks,
structures that associate convex sets of probability measures with directed
acyclic graphs. Credal networks are graphical models for precise/imprecise
beliefs. The main contribution of this work is a theory of credal networks that
displays as much flexibility and representational power as the theory of
standard Bayesian networks. Results in this paper show how to express judgements
of irrelevance and independence, and how to compute inferences in credal
networks. A credal network admits several extensions–several sets of
probability measures comply with the constraints represented by a network. Two
types of extensions are investigated. The properties of strong extensions are
clarified through a new generalization of d-separation, and exact and
approximate inference methods are described for strong extensions. Novel results
are presented for natural extensions, and linear fractional programming methods
are described for natural extensions. The paper also investigates credal
networks that are defined globally through perturbations of a single network.},
	author = {Fabio Gagliardi Cozman},
	doi = {10.1016/S0004-3702(00)00029-1},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Bayesian networks; Convex sets of probability measures; Graphical d-separation relations; Graphical models of inference; Independence relations; Lower and upper expectations; Robust Bayesian analysis},
	localfile = {article/Cozman-2000-cn.pdf},
	number = {2},
	pages = {199–233},
	title = {Credal networks},
	volume = {120},
	year = {2000}
}

@book{Oliphant-2006-numpy,
	author = {Travis E. Oliphant},
	publisher = {Trelgol Publishing},
	title = {Guide to NumPy},
	url = {http://www.tramy.us/numpybook.pdf},
	year = {2006}
}

@article{Daboni-1953,
	author = {Luciano Daboni},
	journal = {Giornale dell'Istituto italiano degli attuari},
	localfile = {article/Daboni-1953.pdf},
	pages = {58–65},
	title = {Considerazioni geometriche sulla condizione di equivalenza per una classa di eventi},
	volume = {16},
	year = {1953}
}

@misc{Suppes-2003,
	author = {Patrick Suppes},
	title = {Nonmonotonic Upper Probabilities and Quantum Entanglement},
	year = {2003}
}

@article{Ferreira-Cozman-2005-AR+,
	abstract = {A credal network is a graphical representation for a set of
joint probability distributions. In this paper we discuss algorithms for exact
and approximate inferences in credal networks. We propose a branch-and-bound
framework for inference, and focus on inferences for polytree-shaped networks.
We also propose a new algorithm, A/R+, for outer approximations in
polytree-shaped credal networks.},
	author = {José Carlos {Ferreira da Rocha} and Fabio Gagliardi Cozman},
	doi = {10.1016/j.ijar.2004.10.009},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Ferreira-Cozman-2005-AR+.pdf},
	number = {2-3},
	pages = {279–296},
	title = {Inference in credal networks: branch-and-bound methods and the A/R+ algorithm},
	volume = {39},
	year = {2005}
}

@book{Eckel-2000-C++V2,
	author = {Bruce Eckel},
	edition = {2},
	publisher = {Prentice Hall},
	title = {Thinking in C++},
	volume = {2},
	year = {2000}
}

@article{Polya-1930,
	annote = {Bevat deel over urneproblemen},
	author = {G. Pólya},
	journal = {Annales de l'Institut Henri Poincaré},
	localfile = {article/Polya-1930.pdf},
	number = {2},
	pages = {117–161},
	title = {Sur quelques points de la théorie des probabilités},
	url = {http://www.numdam.org/item?id=AIHP_1930__1_2_117_0},
	volume = {1},
	year = {1930}
}

@inproceedings{DeBock-DeCooman-2011,
	abstract = {We present an efficient exact algorithm for estimating state
sequences from outputs (or observations) in imprecise hidden Markov models
(iHMM), where both the uncertainty linking one state to the next, and that
linking a state to its output, are represented using coherent lower previsions.
The notion of independence we associate with the credal network representing the
iHMM is that of epistemic irrelevance. We consider as best estimates for state
sequences the (Walley–Sen) maximal sequences for the posterior joint state
model (conditioned on the observed output sequence), associated with a gain
function that is the indicator of the state sequence. This corresponds to (and
generalises) finding the state sequence with the highest posterior probability
in HMMs with precise transition and output probabilities (pHMMs). We argue that
the computational complexity is at worst quadratic in the length of the Markov
chain, cubic in the number of states, and essentially linear in the number of
maximal state sequences. For binary iHMMs, we investigate experimentally how the
number of maximal state sequences depends on the model parameters.},
	address = {Innsbruck, Austria},
	author = {Jasper {De Bock} and Gert {De Cooman}},
	booktitle = {ISIPTA'11: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications},
	editor = {Frank P. A. Coolen and Gert {De Cooman} and Thomas Fetz and Michael Oberguggenberger},
	pages = {159–168},
	publisher = {SIPTA},
	title = {State sequence prediction in imprecise hidden Markov models},
	year = {2011}
}

@article{Inuiguchi-Sakawa-1997,
	abstract = {In this paper, we focus on a treatment of a linear
programming problem with an interval objective function. From the viewpoint of
the achievement rate, a new solution concept, the maximin achievement rate
solution, is proposed. Nice properties of this solution are shown: a maximin
achievement rate solution is necessarily optimal when a necessarily optimal
solution exists, and if not, then it is still a possibly optimal solution. An
algorithm for a maximin achievement rate solution is proposed based on a
relaxation procedure together with a simplex method. A numerical example is
given to demonstrate the proposed solution algorithm.},
	author = {Masahiro Inuiguchi and M. Sakawa},
	issn = {0160-5682},
	journal = {Journal of the Operational Research Society},
	keywords = {fractional programming},
	localfile = {article/Inuiguchi-Sakawa-1997.pdf},
	number = {1},
	pages = {25–33},
	publisher = {Palgrave Macmillan Journals on behalf of the Operational Research Society},
	title = {An achievement rate approach to linear programming problems with an interval objective function},
	url = {http://www.jstor.org/stable/3009940},
	volume = {48},
	year = {1997}
}

@article{Williams-2007-notes,
	abstract = {The personalist conception of probability is often
explicated in terms of betting rates acceptable to an individual. A common
approach, that of de Finetti for example, assumes that the individual is willing
to take either side of the bet, so that the bet is “fair” from the individual's
point of view. This can sometimes be unrealistic, and leads to difficulties in
the case of conditional probabilities or previsions. An alternative conception
is presented in which it is only assumed that the collection of acceptable bets
forms a convex cone, rather than a linear space. This leads to the more general
conception of an upper conditional prevision. The main concerns of the paper are
with the extension of upper conditional previsions. The main result is that any
upper conditional prevision is the upper envelope of a family of additive
conditional previsions.},
	author = {Peter M. Williams},
	doi = {10.1016/j.ijar.2006.07.019},
	institution = {University of Sussex},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Coherence; Conditional prevision; Imprecise probabilities; de Finetti},
	localfile = {article/Williams-2007-notes.pdf},
	pages = {366–383},
	title = {Notes on conditional previsions},
	volume = {44},
	year = {2007}
}

@proceedings{ISIPTA-2009,
	address = {Durham, United Kingdom},
	editor = {Thomas Augustin and Frank P. A. Coolen and Serafin Moral and Matthias C. M. Troffaes},
	organization = {SIPTA},
	title = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
	year = {2009}
}

@article{Delbaen-1974,
	annote = {ook op papier},
	author = {Freddy Delbaen},
	doi = {10.1016/0022-247X(74)90133-4},
	journal = {Journal of Mathematical Analysis and Applications},
	localfile = {article/Delbaen-1974.pdf},
	number = {1},
	pages = {210–233},
	publisher = {Elsevier},
	title = {Convex games and extreme points},
	volume = {45},
	year = {1974}
}

@inproceedings{Quaeghebeur-2010-UAI,
	author = {Erik Quaeghebeur},
	booktitle = {UAI-10: Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
	editor = {Peter Spirtes and Peter Grünwald},
	isbn = {978-0-9749039-6-5},
	pages = {466–473},
	publisher = {AUAI Press},
	title = {Characterizing the set of coherent lower previsions with a finite number of constraints or vertices},
	year = {2010}
}

@article{Jaffray-1989,
	author = {Jean-Yves Jaffray},
	doi = {10.1016/0167-6377(89)90010-2},
	journal = {Operations Research Letters},
	localfile = {article/Jaffray-1989.pdf},
	number = {2},
	pages = {107–112},
	publisher = {Elsevier},
	title = {Linear utility theory for belief functions},
	volume = {8},
	year = {1989}
}

@article{Williams-1980,
	author = {Peter M. Williams},
	journal = {The British Journal for the Philosophy of Science},
	localfile = {article/Williams-1980.pdf},
	number = {2},
	pages = {131–144},
	title = {Bayesian conditionalisation and the principle of minimum information},
	url = {http://www.jstor.org/stable/687182},
	volume = {31},
	year = {1980}
}

@article{Huber-Strassen-1973,
	author = {Peter J. Huber and Volker Strassen},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Huber-Strassen-1973.pdf},
	pages = {251–263},
	title = {Minimax tests and the Neyman-Pearson lemma for capacities},
	volume = {1},
	year = {1973}
}

@article{Ioannidis-et-al-2010-afraid,
	author = {J. P. A. Ioannidis and Athina Tatsioni and F. B. Karassa},
	doi = {10.1111/j.1365-2362.2010.02272.x},
	issn = {1365-2362},
	journal = {European Journal of Clinical Investigation},
	localfile = {article/Ioannidis-et-al-2010-afraid.pdf},
	number = {4},
	pages = {285–287},
	title = {Who is afraid of reviewers' comments?: Or, why anything can be published and anything can be cited},
	volume = {40},
	year = {2010}
}

@techreport{kam981,
	author = {Milan Hladík},
	institution = {KAM-DIMATIA Series},
	number = {981},
	title = {Interval linear programming: A survey},
	year = {2010}
}

@article{Gneiting-Raftery-2007,
	abstract = {Scoring rules assess the quality of probabilistic forecasts,
by assigning a numerical score based on the predictive distribution and on the
event or value that materializes. A scoring rule is proper if the forecaster
maximizes the expected score for an observation drawn from the distributionF if
he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly
proper if the maximum is unique. In prediction problems, proper scoring rules
encourage the forecaster to make careful assessments and to be honest. In
estimation problems, strictly proper scoring rules provide attractive loss and
utility functions that can be tailored to the problem at hand. This article
reviews and develops the theory of proper scoring rules on general probability
spaces, and proposes and discusses examples thereof. Proper scoring rules derive
from convex functions and relate to information measures, entropy functions, and
Bregman divergences. In the case of categorical variables, we prove a rigorous
version of the Savage representation. Examples of scoring rules for
probabilistic forecasts in the form of predictive densities include the
logarithmic, spherical, pseudospherical, and quadratic scores. The continuous
ranked probability score applies to probabilistic forecasts that take the form
of predictive cumulative distribution functions. It generalizes the absolute
error and forms a special case of a new and very general type of score, the
energy score. Like many other scoring rules, the energy score admits a kernel
representation in terms of negative definite functions, with links to
inequalities of Hoeffding type, in both univariate and multivariate settings.
Proper scoring rules for quantile and interval forecasts are also discussed. We
relate proper scoring rules to Bayes factors and to cross-validation, and
propose a novel form of cross-validation known as random-fold cross-validation.
A case study on probabilistic weather forecasts in the North American Pacific
Northwest illustrates the importance of propriety. We note optimum score
approaches to point and quantile estimation, and propose the intuitively
appealing interval score as a utility function in interval estimation that
addresses width as well as coverage.},
	author = {Tilmann Gneiting and Adrian E. Raftery},
	doi = {10.1198/016214506000001437},
	journal = {Journal of the American Statistical Association},
	keywords = {Bayes factor; Bregman divergence; Brier score; Coherent; Continuous ranked probability score; Cross-validation; Entropy; Kernel score; Loss function; Minimum contrast estimation; Negative definite function; Prediction interval; Predictive distribution; Quantile forecast; Scoring rule; Skill score; Strictly proper; Utility function},
	localfile = {article/Gneiting-Raftery-2007.pdf},
	number = {477},
	pages = {359–378},
	publisher = {ASA},
	title = {Strictly proper scoring rules, prediction, and estimation},
	volume = {102},
	year = {2007}
}

@incollection{Seidenfeld-Schervish-Kadane-1990-decwoord,
	address = {Dordrecht},
	author = {Teddy Seidenfeld and Mark J. Schervish and Joseph B. Kadane},
	booktitle = {Acting and Reflecting: The Interdisciplinary Turn in Philosophy},
	editor = {Wilfried Sieg},
	pages = {143–170},
	publisher = {Kluwer Academic Publishers},
	series = {Synthese Library},
	title = {Decisions without ordering},
	volume = {211},
	year = {1990}
}

@inproceedings{Cozman-Seidenfeld-2009-graphoid,
	abstract = {This paper examines definitions of independence for events
and variables in the context of full conditional measures; that is, when
conditional probability is a primitive notion and conditioning is allowed on
null events. Several independence concepts are evaluated with respect to
graphoid properties; we show that properties of weak union, contraction and
intersection may fail when null events are present. We propose a concept of
“full” independence, characterize the form of a full conditional measure under
full independence, and suggest how to build a theory of Bayesian networks that
accommodates null events.},
	annote = {Conference held in Amsterdam, May 2-5, 2007},
	author = {Fabio Gagliardi Cozman and Teddy Seidenfeld},
	booktitle = {Foundations of the Formal Sciences VI: Reasoning about Probabilities and Probabilistic Reasoning},
	editor = {Benedikt Löwe and Eric Pacuit and Jan-Willem Romeijn},
	publisher = {College Publications},
	series = {Studies in Logic},
	title = {Independence for Full Conditional Measures and their Graphoid Problems},
	year = {2010}
}

@inproceedings{Zaffalon-1999,
	address = {Ghent, Belgium},
	author = {Marco Zaffalon},
	booktitle = {ISIPTA '99: Proceedings of the First International Symposium on Imprecise probabilities and Their Applications},
	editor = {Gert {De Cooman} and Fabio Gagliardi Cozman and Serafin Moral and Peter Walley},
	pages = {405–414},
	title = {A Credal Approach to Naive Classification},
	year = {1999}
}

@inproceedings{Quaeghebeur-2003,
	address = {Leuven, Belgium},
	author = {Erik Quaeghebeur},
	booktitle = {Proceedings of the 7th workshop on dynamics and computation: Iterated games and cooperation},
	title = {Fictitious play: two viewpoints and two versions},
	year = {2003}
}

@inproceedings{Delort-BouchonMeunier-2000,
	author = {Jean-Yves Delort and Bernadette Bouchon-Meunier},
	booktitle = {Proceedings of the Eleventh International World Wide Web Conference},
	title = {Facing Uncertainty in Link Recommender Systems},
	year = {2002}
}

@article{Fukuda-2004-Minkowski-addition,
	abstract = {A zonotope is the Minkowski addition of line segments in Rd.
The zonotope construction problem is to list all extreme points of a zonotope
given by its line segments. By duality, it is equivalent to the arrangement
construction problem—that is, to generate all regions of an arrangement of
hyperplanes. By replacing line segments with convex V-polytopes, we obtain a
natural generalization of the zonotope construction problem: the construction of
the Minkowski addition of k polytopes. Gritzmann and Sturmfels studied this
general problem in various aspects and presented polynomial algorithms for the
problem when one of the parameters k or d is fixed. The main objective of the
present work is to introduce an efficient algorithm for variable d and k. Here
we call an algorithm efficient or polynomial if it runs in time bounded by a
polynomial function of both the input size and the output size. The algorithm is
a natural extension of a known algorithm for the zonotope construction, based on
linear programming and reverse search. It is compact, highly parallelizable and
very easy to implement. This work has been motivated by the use of polyhedral
computation for optimal tolerance determination in mechanical engineering.},
	author = {Komei Fukuda},
	doi = {10.1016/j.jsc.2003.08.007},
	journal = {Journal of Symbolic Computation},
	keywords = {Convex polytope; Efficient algorithm; Minkowski addition; Reverse search},
	localfile = {article/Fukuda-2004-Minkowski-addition.pdf},
	pages = {1261–1272},
	title = {From the zonotope construction to the Minkowski addition of convex polytopes},
	volume = {38},
	year = {2004}
}

@misc{Doumont-2001-fundamentals,
	author = {Jean-luc Doumont},
	title = {Fundamentals},
	year = {2001}
}

@incollection{Mura-2008-backmatter,
	author = {Bruno de Finetti},
	doi = {10.1007/978-1-4020-8202-3},
	editor = {Alberto Mura},
	publisher = {Springer},
	series = {Synthese Library},
	title = {Back Matter},
	volume = {340},
	year = {2008}
}

@article{Vicig-Zaffalon-Cozman-2007-notes,
	abstract = {These notes comment on Williams' fundamental essay Notes on
Conditional Previsions, written as a research report in 1975 and published in
the present issue. Basic aspects of that work are discussed, including
historical background and relevance to the foundations of probability; examples
are supplied to help understanding.},
	author = {Paolo Vicig and Marco Zaffalon and Fabio Gagliardi Cozman},
	doi = {10.1016/j.ijar.2006.07.018},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Vicig-Zaffalon-Cozman-2007-notes.pdf},
	pages = {358–365},
	title = {Notes on “Notes on conditional previsions”},
	volume = {44},
	year = {2007}
}

@inproceedings{DeCooman-Miranda-2004,
	author = {Gert {De Cooman} and Enrique Miranda},
	booktitle = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
	pages = {451–458},
	title = {A weak law of large numbers for coherent lower previsions},
	year = {2004}
}

@incollection{Neumaier-2004,
	author = {Arnold Neumaier},
	editor = {A. Iserles},
	pages = {271–369},
	publisher = {Cambridge University Press},
	title = {Complete Search in Continuous Global Optimization and Constraint Satisfaction},
	year = {2004}
}

@article{Nau-1992,
	abstract = {This paper presents a quasi-Bayesian model of subjective
uncertainty in which beliefs which are represented by lower and upper
probabilities qualified by numerical confidence weights. The representation is
derived from a system of axioms of binary preferences which differs from
standard axiom systems insofar as completeness is not assumed and transitivity
is weakened. Confidence-weighted probabilities may be elicited through the
acceptance of bets with limited stakes, a generalization of the operational
method of de Finetti. The model is applicable to the reconciliation of
inconsistent probability judgments and to the sensitivity analysis of Bayesian
decision models.},
	author = {Robert F. Nau},
	doi = {10.1214/aos},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {coherence; confidence-weighted probabilities; fuzzy sets; incompletenes; lower and upper probabilities; second-order probabilities; subjective probability},
	localfile = {article/Nau-1992.pdf},
	month = dec,
	number = {4},
	pages = {1737–1767},
	publisher = {Institute of Mathematical Statistics},
	title = {Indeterminate probabilities on finite sets},
	url = {http://www.jstor.org/stable/2242366},
	volume = {20},
	year = {1992}
}

@misc{Walley-1998,
	annote = {(used to be?) part of the Imprecise Probabilities Project geannoteerde kopie},
	author = {Peter Walley},
	title = {Coherent upper and lower previsions},
	year = {1998}
}

@article{Ericson-1969,
	abstract = {Scheffe (1958) introduced the simplex-lattice design for
experiments with mixtures of q components. The purpose of this design is the
empirical prediction of the response to any mixture of the components when the
response depends only on the proportions of the components but not on the total
amount of the mixture. In this paper an alternative to the simplex-lattice
design is developed in which all the features of the design are maintained
except that the pure mixtures are replaced by the (q - 1)-nary mixtures.},
	annote = {with discussion},
	author = {W. A. Ericson},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	number = {2},
	pages = {195–233},
	title = {Subjective Bayesian Models in Sampling Finite Populations},
	url = {http://links.jstor.org/sici?sici=0035-9246(1969)31:2%3C195:SBMISF%3E2.0.CO},
	volume = {31},
	year = {1969}
}

@misc{Doumont-2001-graphing,
	author = {Jean-luc Doumont},
	title = {Graphing data},
	year = {2001}
}

@article{Zimmermann-1983,
	abstract = {Fuzzy linear programming (FLP) was originally suggested to
solve problems which could be formulated as LP-models, the parameters of which,
however, were fuzzy rather than crisp numbers. It has turned out in the meantime
that FLP is also well suited to solve LP-problems with several objective
functions. FLP belongs to goal programming in the sense that implicitly or
explicitly aspiration levels have to be defined at which the membership
functions of the fuzzy sets reach their maximum or minimum. Main advantages of
FLP are, that the models used are numerically very efficient and that they can
in many ways be well adopted to different decision behaviors and contexts.},
	author = {Hans-Jürgen Zimmermann},
	doi = {10.1016/0305-0548(83)90004-7},
	issn = {0305-0548},
	journal = {Computers \& Operations Research},
	number = {4},
	pages = {291–298},
	title = {Fuzzy mathematical programming},
	volume = {10},
	year = {1983}
}

@misc{Knuth-Larrabee-Roberts-1987,
	annote = {Report based on a Stanford University course},
	author = {Donald E. Knuth and Tracy Larrabee and Paul M. Roberts},
	title = {Mathematical Writing},
	year = {1987}
}

@article{Bernard-2005,
	abstract = {The imprecise Dirichlet model (IDM) was recently proposed by
Walley as a model for objective statistical inference from multinomial data with
chances $\theta$. In the IDM, prior or posterior uncertainty about $\theta$ is
described by a set of Dirichlet distributions, and inferences about events are
summarized by lower and upper probabilities. The IDM avoids shortcomings of
alternative objective models, either frequentist or Bayesian. We review the
properties of the model, for both parametric and predictive inferences, and some
of its recent applications to various statistical problems.},
	author = {Jean-Marc Bernard},
	doi = {10.1016/j.ijar.2004.10.002},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Bayesian inference; Dirichlet distribution; Frequentist inference; IDM; Lower and upper probabilities; Predictive inference; Prior ignorance},
	localfile = {article/Bernard-2005.pdf},
	pages = {123–150},
	title = {An introduction to the imprecise Dirichlet model for multinomial data},
	volume = {39},
	year = {2005}
}

@article{Domingos-Pazzani-1997,
	abstract = {The simple Bayesian classifier is known to be optimal when
attributes are independent given the class, but the question of whether other
sufficient conditions for its optimality exist has so far not been explored.
Empirical results showing that it performs surprisingly well in many domains
containing clear attribute dependences suggest that the answer to this question
may be positive. This article shows that, although the Bayesian classifier's
probability estimates are only optimal under quadratic loss if the independence
assumption holds, the classifier itself can be optimal under zero-one loss
(misclassification rate) even when this assumption is violated by a wide margin.
The region of quadratic-loss optimality of the Bayesian classifier is in fact a
second-order infinitesimal fraction of the region of zero-one optimality. This
implies that the Bayesian classifier has a much greater range of applicability
than previously thought. For example, in this article it is shown to be optimal
for learning conjunctions and disjunctions, even though they violate the
independence assumption. Further, studies in artificial domains show that it
will often outperform more powerful classifiers for common training set sizes
and numbers of attributes, even if its bias is a priori much less appropriate to
the domain. This article's results also imply that detecting attribute
dependence is not necessarily the best way to extend the Bayesian classifier,
and this is also verified empirically.},
	author = {Pedro Domingos and Michael Pazzani},
	doi = {10.1023/A:1007413511361},
	journal = {Machine Learning},
	keywords = {induction; naive bayesian classifier; optimal classification; simple bayesian classifier; with attribute dependences; zero-one loss},
	localfile = {article/Domingos-Pazzani-1997.pdf},
	number = {2},
	pages = {103–130},
	title = {On the optimality of the simple Bayesian classifier under zero-one loss},
	volume = {29},
	year = {1997}
}

@article{Tessem,
	abstract = {Belief networks are tried as a method for propagation of
singleton interval probabilities. A convex polytope representation of the
interval probabilities is shown to make the problem intractable even for small
parameters. A solution to this is to use the interval bounds directly in
computations of the propagation algorithm. The algorithm presented leads to
approximative results but has the advantage of being polynomial in time. It is
shown that the method gives fairly good results.},
	author = {Bjørnar Tessem},
	doi = {10.1016/0888-613X(92)90006-L},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	number = {3-4},
	pages = {95–120},
	title = {Interval probability propagation},
	volume = {7},
	year = {1992}
}

@article{DeCooman-1997-postheo1,
	abstract = {In this paper, I provide the basis for a measure- and
integral-theoretic formulation of possibility theory. It is shown thai, using a
general definition of possibility measures, and a generalization of Sugeno's
fuzzy integral-the semi-normed fuzzy integral, or possibility integral-. a
unified and consistent account can be given of many of the possibilistic results
extant in the literature. The striking formal analogy between this treatment of
possibility theory, using possibility integrals, and Kolmogorov's
measure-theoretic formulation of probability theory, using Lebesgue integrals,
is explored and exploited. I introduce and study possibilistic and fuzzy
variables as possibilistic counterparts of stochastic and real stochastic
variables respeclively, and develop the notion of a possibility distribution for
these variables. The almost everywhere equality and dominance of fuzzy variables
is defined and studied. The proof is given for a Radon-Nikodym-like theorem in
possibility theory. Following the example set by the classical theory of
integration, product possibility measures and multiple possibility integrals are
introduced, and a Fubini-like theorem is proven. In this way, the groundwork is
laid for a unifying measure- and integral-theoretic treatment of conditional
possibility and possibilistic independence, discussed in more detail in Parts II
and III of this series of three papers.},
	author = {Gert {De Cooman}},
	doi = {10.1080/03081079708945160},
	journal = {International Journal of General Systems},
	keywords = {Fubini-like theorem; Possibility measure; Radon-Nikodym-like theorem; fuzzy variable; possibilistic variable; possibility distribution; seminormed fuzzy integral},
	localfile = {article/DeCooman-1997-postheo1.pdf},
	number = {4},
	pages = {291–323},
	title = {Possibility theory I: the measure- and integral-theoretic groundwork},
	url = {http://hdl.handle.net/1854/LU-182365},
	volume = {25},
	year = {1997}
}

@book{Kallenberg-2005,
	author = {Olav Kallenberg},
	publisher = {Springer},
	series = {probability and Its Applications},
	title = {Probabilistic Symmetries and Invariance Principles},
	year = {2005}
}

@article{Rommelfanger-1996,
	abstract = {This paper presents a survey on methods for solving fuzzy
linear programs. First LP models with soft constraints are discussed. Then LP
problems in which coefficients of constraints and/or of the objective function
may be fuzzy are outlined. Pivotal questions are the interpretation of the
inequality relation in fuzzy constraints and the meaning of fuzzy objectives. In
addition to the commonly applied extended addition, based on the min-operator
and used for the aggregation of the left-hand sides of fuzzy constraints and
fuzzy objectives, a more flexible procedure, based on Yager's parametrized
t-norm Tp, is presented. Finally practical applications of fuzzy linear programs
are listed.},
	author = {Heinrich Rommelfanger},
	doi = {10.1016/0377-2217(95)00008-9},
	issn = {0377-2217},
	journal = {European Journal of Operational Research},
	keywords = {Compromise solution; Extended addition of fuzzy intervals; Fuzzy sets; Inequality relation in fuzzy conslraints; Mathematical Programming},
	localfile = {article/Rommelfanger-1996.pdf},
	number = {3},
	pages = {512–527},
	title = {Fuzzy linear programming and applications},
	volume = {92},
	year = {1996}
}

@misc{DeCooman-2003,
	annote = {Transparanten voor ISIPTA'03},
	author = {Gert {De Cooman}},
	title = {Theory of Imprecise Probabilities (Basic ideas)},
	year = {2003}
}

@techreport{Coolen-Augustin-2006-cNPI1,
	author = {Frank P. A. Coolen and Thomas Augustin},
	title = {Nonparametric predictive inference for multinomial data - Notes 1 (m-functions and interval probabilities for events)},
	year = {2006}
}

@book{Walley-1991,
	address = {London},
	author = {Peter Walley},
	localfile = {book/Walley-1991-book.pdf; book/Walley-1991.pdf},
	publisher = {Chapman \& Hall},
	series = {Monographs on Statistics and Applied Probability},
	title = {Statistical Reasoning with Imprecise Probabilities},
	volume = {42},
	year = {1991}
}

@book{Prautzsch-Boehm-Paluszny-2002-Bezier,
	annote = {boek bij Gert},
	author = {Hartmut Prautzsch and Wolfgang Boehm and Marco Paluszny},
	publisher = {Springer},
	series = {Mathematics and visualization},
	title = {Bézier and B-spline Techniques},
	year = {2002}
}

@article{Wagner-1992-gen-probkin,
	author = {Carl G. Wagner},
	journal = {Erkenntnis},
	pages = {245–257},
	title = {Generalized probability Kinematics},
	volume = {36},
	year = {1992}
}

@proceedings{ICML-2002,
	booktitle = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
	title = {Proceedings of the 19th International Conference on Machine Learning (ICML 2002)},
	year = {2002}
}

@phdthesis{Strobl-2008,
	author = {Carolin Strobl},
	school = {Institut für Statistik, Fakultät für Mathematik, Informatik und Statistik, Ludwig-Maximilians-Universität München},
	title = {Statistical Issues in Machine Learning – Towards Reliable Split Selection and Variable Importance Measures},
	year = {2008}
}

@article{Alexandrov-Kopteva-Kutateladze-2005-Blashke,
	abstract = {This is an extended version of a talk on October 4, 2004 at
the research seminar “Differential geometry and applications” (headed by
Academician A. T. Fomenko) at Moscow State University. The paper contains an
overview of available (but far from well-known) results about the Blaschke
addition of convex bodies, some new theorems on the monotonicity of the volume
of convex bodies (in particular, convex polyhedra with parallel faces) as well
as description of a software for visualization of polyhedra with prescribed
outward normals and face areas.},
	author = {Victor Alexandrov and Natalia Kopteva and S. S. Kutateladze},
	journal = {Tr. Semin. Vektorn. Tenzorn. Anal.},
	pages = {8–30},
	title = {Blaschke addition and convex polyhedra},
	volume = {26},
	year = {2005}
}

@article{Miranda-DeCooman-2007-margext,
	abstract = {We generalise Walley's Marginal Extension Theorem to the
case of any finite number of conditional lower previsions. Unlike the procedure
of natural extension, our marginal extension always provides the smallest (most
conservative) coherent extensions. We show that they can also be calculated as
lower envelopes of marginal extensions of conditional linear (precise)
previsions. Finally, we use our version of the theorem to study the so-called
forward irrelevant product and forward irrelevant natural extension of a number
of marginal lower previsions.},
	author = {Enrique Miranda and Gert {De Cooman}},
	doi = {10.1016/j.ijar.2006.12.009},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Coherence; Epistemic irrelevance; Forward irrelevance; Forward irrelevant natural extension; Forward irrelevant product; Imprecise probabilities; Lower previsions; Marginal extension; Natural extension},
	localfile = {article/Miranda-DeCooman-2007-margext.pdf},
	number = {1},
	pages = {188–225},
	title = {Marginal extension in the theory of coherent lower previsions},
	volume = {46},
	year = {2007}
}

@misc{Lauritzen-2004a,
	annote = {Transparanten},
	author = {Steffen L. Lauritzen},
	title = {Sufficiency and Unbiased Estimation},
	year = {2004}
}

@article{Walley-1996-expert,
	abstract = {This paper compares four measures that have been advocated
as models for uncertainty in expert systems. The measures are additive
probabilities (used in the Bayesian theory), coherent lower (or upper)
previsions, belief functions (used in the Dempster-Shafer theory) and
possibility measures (fuzzy logic). Special emphasis is given to the theory of
coherent lower previsions, in which upper and lower probabilities, expectations
and conditional probabilities are constructed from initial assessments through a
technique of natural extension. Mathematically, all the measures can be regarded
as types of coherent lower or upper previsions, and this perspective gives some
insight into the properties of belief functions and possibility measures. The
measures are evaluated according to six criteria: clarity of interpretation;
ability to model partial information and imprecise assessments, especially
judgements expressed in natural language; rules for combining and updating
uncertainty, and their justification; consistency of models and inferences;
feasibility of assessment; and feasibility of computations. Each of the four
measures seems to be useful in special kinds of problems, but only lower and
upper previsions appear to be sufficiently general to model the most common
types of uncertainty.},
	annote = {geannoteerde kopie},
	author = {Peter Walley},
	doi = {10.1016/0004-3702(95)00009-7},
	journal = {Artificial Intelligence},
	keywords = {Bayesian theory; Belief functions; Conditional probability; Decision; Dempster–Shafer theory; Imprecise probabilities; Inference; Lower probability; Possibility theory; Prevision; Upper probability; lndependence},
	localfile = {article/Walley-1996-expert.pdf},
	pages = {1–58},
	title = {Measures of uncertainty in expert systems},
	volume = {83},
	year = {1996}
}

@incollection{Mura-2008-frontmatter,
	author = {Bruno de Finetti},
	doi = {10.1007/978-1-4020-8202-3},
	editor = {Alberto Mura},
	publisher = {Springer},
	series = {Synthese Library},
	title = {Front Matter},
	volume = {340},
	year = {2008}
}

@inproceedings{Quaeghebeur-DeCooman-2006-SMPS,
	address = {Bristol},
	annote = {Proceedings of the 2006 International Workshop on Soft Methods in Probability and Statistics},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	booktitle = {Soft Methods for Integrated Uncertainty Modelling},
	editor = {Jonathan Lawry and Enrique Miranda and A. Bugarin and S. Li and María Angeles Gil and P. Grzegorzewski and Olgierd Hryniewicz},
	month = sep,
	organization = {Artificial Intelligence Group, University of Bristol},
	pages = {211–221},
	publisher = {Springer},
	series = {Advances in Soft Computing},
	title = {Extreme lower probabilities},
	volume = {6},
	year = {2006}
}

@inproceedings{DeFinetti-1928,
	address = {Bologna},
	author = {Bruno de Finetti},
	booktitle = {Atti del congresso internationale dei matematici},
	pages = {179–190},
	publisher = {N. Zanichelli},
	title = {Funzione caratteristica di un fenomeno aleatorio},
	volume = {VI},
	year = {1928}
}

@article{Boute-2005,
	author = {Raymond T. Boute},
	doi = {10.1145/1086642.1086647},
	issn = {0164-0925},
	journal = {ACM Transactions on Programming Languages and Systems},
	localfile = {article/Boute-2005.pdf},
	month = sep,
	number = {5},
	pages = {988–1047},
	title = {Functional declarative language design and predicate calculus},
	url = {http://portal.acm.org/citation.cfm?doid=1086642.1086647},
	volume = {27},
	year = {2005}
}

@inproceedings{Wallner-2005,
	address = {Pittsburgh, Pennsylvania},
	author = {Anton Wallner},
	booktitle = {ISIPTA '05: Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Fabio Gagliardi Cozman and Robert Nau and Teddy Seidenfeld},
	organization = {SIPTA},
	pages = {388–395},
	title = {Maximal Number of Vertices of Polytopes Defined by F-Probabilities},
	year = {2005}
}

@techreport{Augustin-2003-note,
	author = {Thomas Augustin},
	institution = {LMU München},
	title = {A Note on Lower Envelopes},
	year = {2003}
}

@article{Cozman-Walley-2005-graphoid,
	abstract = {This paper investigates Walley's concepts of epistemic
irrelevance and epistemic independence for imprecise probability models. We
study the mathematical properties of irrelevance and independence, and their
relation to the graphoid axioms. Examples are given to show that epistemic
irrelevance can violate the symmetry, contraction and intersection axioms, that
epistemic independence can violate contraction and intersection, and that this
accords with informal notions of irrelevance and independence.},
	annote = {ook op papier},
	author = {Fabio Gagliardi Cozman and Peter Walley},
	doi = {10.1007/s10472-005-9004-z},
	journal = {Annals of Mathematics and Artificial Intelligence},
	localfile = {article/Cozman-Walley-2005-graphoid.pdf},
	number = {1},
	pages = {173–195},
	title = {Graphoid properties of epistemic irrelevance and independence},
	volume = {45},
	year = {2005}
}

@article{Zabell-1995,
	abstract = {In the 1920s the English philosopher W. E. Johnson
discovered a simple characterization of the Dirichlet family of conjugate priors
for a multinomial distribution having at least three categories. In the present
note Johnson's result is extended to the case of a Markov exchangeable
sequence.},
	author = {Sandy L. Zabell},
	doi = {10.1007/BF02213460},
	journal = {Journal of Theoretical Probability},
	keywords = {Markov exchangeable sequences; W. E. Johnson; conjugate prior; predictive probability},
	localfile = {article/Zabell-1995.pdf},
	number = {1},
	pages = {175–178},
	title = {Characterizing Markov exchangeable sequences},
	volume = {8},
	year = {1995}
}

@article{Miller-1980-gamma,
	abstract = {This paper presents a Bayesian analysis of shape, scale, and
mean of the two-parameter gamma distribution. Attention is given to conjugate
and "non-informative" priors, to simplifications of the numerical analysis of
posterior distributions, and to comparison of Bayesian and classical
inferences.},
	author = {Robert B. Miller},
	issn = {0040-1706},
	journal = {Technometrics},
	keywords = {Bayesian analysis; Gamma distribution},
	localfile = {article/Miller-1980-gamma.pdf},
	number = {1},
	pages = {65–69},
	publisher = {American Statistical Association and American Society for Quality},
	title = {Bayesian analysis of the two-parameter Gamma distribution},
	url = {http://www.jstor.org/stable/1268384},
	volume = {22},
	year = {1980}
}

@misc{Doumont-2001-feedback,
	author = {Jean-luc Doumont},
	title = {Giving Feedback},
	year = {2001}
}

@book{Kotz-Balakrishnan-Johnson-2000,
	author = {Samuel Kotz and N. Balakrishnan and Norman L. Johnson},
	edition = {second},
	publisher = {Wiley},
	series = {Wiley Series in Probability and Statistics},
	title = {Continuous Multivariate Distributions},
	volume = {1: Models},
	year = {2000}
}

@article{Fujimoto-Oshime-1994-Perron-Frobenius,
	abstract = {By a pointed closed convex cone K \subset of \mathbb{R}^N
with interior, an order is defined in \mathbb{R}^N. Let T: K\to K be a
set-valued nondecreasing subhomogeneous map. The main purpose of this paper
concerns the conditions on $\lambda$>0 under which $\lambda$u ∈ T(u)+c, u ∈ K,
is solvable for all c∈ K and how its solution depends on c. The homogenization
of T around infinity is also introduced and is proved to leave the solvability
condition for $\lambda$>0 unchanged.},
	author = {Takao Fujimoto and Yorimasa Oshime},
	doi = {10.1016/0304-4068(94)90028-0},
	journal = {Journal of Mathematical Economics},
	keywords = {Homo; Monotonicity; Nonlinear resolvent problems},
	localfile = {article/Fujimoto-Oshime-1994-Perron-Frobenius.pdf},
	number = {5},
	pages = {475–498},
	title = {The nonlinear Perron-Frobenius problem for set-valued maps in a closed convex cone in R^N},
	volume = {23},
	year = {1994}
}

@misc{Doumont-2001-training,
	author = {Jean-luc Doumont},
	title = {Training others},
	year = {2001}
}

@article{DeCooman-Aeyels-2000,
	abstract = {The relationship is studied between possibility and
necessity measures defined on arbitrary spaces, the theory of imprecise
probabilities, and elementary random set theory. It is shown how special random
sets can be used to generate normal possibility and necessity measures, as well
as their natural extensions. This leads to interesting alternative formulas for
the calculation of these natural extensions},
	author = {Gert {De Cooman} and Dirk Aeyels},
	doi = {10.1109/3468.833093},
	journal = {IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans},
	localfile = {article/DeCooman-Aeyels-2000.pdf},
	number = {2},
	pages = {124–130},
	publisher = {IEEE},
	title = {A random set description of a possibility measure and its natural extension},
	volume = {30},
	year = {2000}
}

@article{Antonucci-etal-2010-GL2U,
	abstract = {Credal networks generalize Bayesian networks by relaxing the
requirement of precision of probabilities. Credal networks are considerably more
expressive than Bayesian networks, but this makes belief updating NP-hard even
on polytrees. We develop a new efficient algorithm for approximate belief
updating in credal networks. The algorithm is based on an important
representation result we prove for general credal networks: that any credal
network can be equivalently reformulated as a credal network with binary
variables; moreover, the transformation, which is considerably more complex than
in the Bayesian case, can be implemented in polynomial time. The equivalent
binary credal network is then updated by L2U, a loopy approximate algorithm for
binary credal networks. Overall, we generalize L2U to non-binary credal
networks, obtaining a scalable algorithm for the general case, which is
approximate only because of its loopy nature. The accuracy of the inferences
with respect to other state-of-the-art algorithms is evaluated by extensive
numerical tests.},
	author = {Alessandro Antonucci and Yi Sun and Cassio Polpo de Campos and Marco Zaffalon},
	doi = {10.1016/j.ijar.2010.01.007},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Antonucci-etal-2010-GL2U.pdf},
	number = {5},
	pages = {474–484},
	title = {Generalized loopy 2U: a new algorithm for approximate inference in credal networks},
	volume = {51},
	year = {2010}
}

@article{Krantz-Kunreuther-2007,
	abstract = {We propose a constructed-choice model for general decision
making. The model departs from utility theory and prospect theory in its
treatment of multiple goals and it suggests several different ways in which
context can affect choice. It is particularly instructive to apply this model to
protective decisions, which are often puzzling. Among other anomalies, people
insure against non-catastrophic events, underinsure against catastrophic risks,
and allow extraneous factors to influence insurance purchases and other
protective decisions. Neither expected-utility theory nor prospect theory can
explain these anomalies satisfactorily. To apply this model to the above
anomalies, we consider many different insurance-related goals, organized in a
taxonomy, and we consider the effects of context on goals, resources, plans and
decision rules. The paper concludes by suggesting some prescriptions for
improving individual decision making with respect to protective measures.},
	annote = {ook op papier},
	author = {David H. Krantz and Howard C. Kunreuther},
	journal = {Judgment and Decision Making},
	keywords = {catastrophic risk; decision making; goals; insurance; plans; prospect theory; protective behavior; utility theory},
	localfile = {article/Krantz-Kunreuther-2007.pdf},
	month = jun,
	number = {3},
	pages = {137–168},
	title = {Goals and plans in decision making},
	url = {http://journal.sjdm.org/jdm7303b.pdf},
	volume = {2},
	year = {2007}
}

@proceedings{AI-2004,
	booktitle = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
	editor = {Geoffrey I. Webb and Xinghuo Yu},
	publisher = {Springer},
	series = {Lecture Notes in AI},
	title = {AI 2004: Advances in Artificial Intelligence: 17th Australian Joint Conference on Artificial Intelligence},
	year = {2004}
}

@article{Mardia-ElAtoum-1976,
	abstract = {The main aim of this note is to give a theoretical
discussion of Bayesian inference for the von Mises-Fisher distribution. The
choice of particular priors is considered and the admissibility of certain
Bayesian estimators studied. For the multisample case estimators are given. Some
problems of testing hypotheses are summarized in the form of posterior odds
against the null hypothesis.},
	author = {K. V. Mardia and S. A. M. El-Atoum},
	doi = {10.1093/biomet},
	journal = {Biometrika},
	keywords = {Bayesian directional data analysis; Loss function},
	localfile = {article/Mardia-ElAtoum-1976.pdf},
	number = {1},
	pages = {203–206},
	publisher = {Biometrika Trust},
	title = {Bayesian inference for the von Mises-Fisher distribution},
	volume = {63},
	year = {1976}
}

@article{Robinson-1951,
	annote = {ook op papier},
	author = {Julia Robinson},
	journal = {The Annals of Mathematics},
	localfile = {article/Robinson-1951.pdf},
	number = {2},
	pages = {296–301},
	title = {An iterative method of solving a game},
	url = {http://www.jstor.org/stable/1969530},
	volume = {54},
	year = {1951}
}

@book{Kadane-Schervish-Seidenfeld-1999,
	abstract = {This important collection of essays is a synthesis of
foundational studies in Bayesian decision theory and statistics. An overarching
topic of the collection is understanding how the norms for Bayesian decision
making should apply in settings with more than one rational decision maker and
then tracing out some of the consequences of this turn for Bayesian statistics.
There are four principal themes to the collection: cooperative, non-sequential
decisions; the representation and measurement of 'partially ordered'
preferences; non-cooperative, sequential decisions; and pooling rules and
Bayesian dynamics for sets of probabilities. The volume will be particularly
valuable to philosophers concerned with decision theory, probability, and
statistics, statisticians, mathematicians, and economists.},
	author = {Joseph B. Kadane and Mark J. Schervish and Teddy Seidenfeld},
	publisher = {Cambridge University Press},
	series = {Cambridge studies in probability, induction, and decision theory},
	title = {Rethinking the foundations of statistics},
	url = {http://books.google.com/books?id=SsBPTDFwnpoC},
	year = {1999}
}

@techreport{Dhillon-Sra-2003-directional,
	abstract = {Traditionally multi-variate normal distributions have been
the staple of data modeling in most domains. For some domains, the model they
provide is either inadequate or incorrect because of the disregard for the
directional components of the data. We present a generative model for data that
is suitable for modeling directional data (as can arise in text and gene
expression clustering). We use mixtures of von Mises-Fisher distributions to
model our data since the von Mises-Fisher distribution is the natural
distribution for directional data. We derive an Expectation Maximization (EM)
algorithm to find the maximum likelihood estimates for the parameters of our
mixture model, and provide various experimental results to evaluate the
“correctness” of our formulation. In this paper we also provide some of the
mathematical background necessary to carry out all the derivations and to gain
insight for an implementation.},
	address = {Austin, Texas},
	author = {Inderjit S. Dhillon and Sra Suvrit},
	institution = {The University of Texas at Austin},
	number = {TR-03-06},
	title = {Modeling Data using Directional Distributions},
	url = {http://www.cs.utexas.edu/~suvrit/work/research.html},
	year = {2003}
}

@article{Gaifman-2004,
	author = {Haim Gaifman},
	doi = {10.1023/B:SYNT.0000029944.99888.a7},
	journal = {Synthese},
	localfile = {article/Gaifman-2004.pdf},
	number = {1-2},
	pages = {97–119},
	title = {Reasoning with limited resources and assigning probabilities to arithmetical statements},
	volume = {140},
	year = {2004}
}

@article{Sundberg-Wagner-1990-capacities,
	annote = {op papier in Wagnerbundel},
	author = {Carl Sundberg and Carl G. Wagner},
	journal = {Journal of Theoretical Probability},
	number = {1},
	pages = {159–167},
	title = {Characterizations of Monotone and 2-Monotone Capacities},
	volume = {5},
	year = {1990}
}

@techreport{Katzoff-1964,
	annote = {Second Edition ook op papier},
	author = {S. Katzoff},
	organization = {NASA},
	title = {Clarity in technical reporting},
	year = {1964}
}

@mastersthesis{Quaeghebeur-2001,
	abstract = {We bestuderen een leermethode voor eindige spelen in de
strategische vorm. Hierbij maken de spelers gebruik van een geschiedenis van
reeds gespeelde sessies om inschattingen te maken over de waarschijnlijke
strategiekeuze van een tegenspeler. Op die waarschijnlijk geachte strategiekeuze
baseren de spelers zich om een eigen strategie te kiezen. We bestuderen hiervoor
eerst de wetenschapsdiscipline genaamd speltheorie. We gaan hier in op
kenmerkende elementen van spelen en bekijken mogelijke voorstellingswijzen. We
gaan dieper in op de voorstelling van een spel in de strategische vorm. Een
strategie, een geheel aan regels die de keuze van acties binnen een spel
volledig bepalen, en de opbrengstfunctie, die de uitslag van een spel voor de
speler vastlegt, worden gedefinieerd. We bekijken hoe we de strategieën van een
spel kunnen onderverdelen en welke strategieën optimaal zijn voor een speler. Zo
komen we tot volgende concepten: beste antwoord, wat de verzameling van
strategieën is die de opbrengst van de speler maximaliseert voor een gegeven
strategie van zijn tegenspeler; evenwicht, wat een strategiecombinatie is waar
geen enkele speler er voordeel bij heeft om van strategie te veranderen;
dominantie, wat een methode is om strategieën te identificeren die het best niet
gespeeld worden en maximin-strategie, wat een voorzichtige strategiekeuze is. In
tweede instantie bespreken we imprecieze waarschijnlijkheden, wat een
onzekerheidsmodel is waar niet alleen rekening kan worden gehouden met
onzekerheid, maar ook met onbepaaldheid. We gaan in op de gevolgen van
rationaliteitvereisten voor dit model en zijn rekenregels. Er wordt tevens een
interpretatie op basis van gokgedrag gegeven aan de grootheden die we gebruiken
om waarschijnlijkheden uit te drukken. Voortbouwend op het voorgaande wordt
onderzocht hoe we een beslissing kunnen maken tussen keuzes voor verschillende
mogelijke strategieën. Dit leidt tot de definitie van maximale strategieën, die
goede keuzes zijn voor een speler. Deze beslissingen worden gebaseerd op het
Dirichletmodel, dat inschattingen weergeeft over de strategiekeuze van een
tegenspeler. We leggen ook uit hoe dit model bijgesteld kan worden door
verrekening van de informatie bekomen uit reeds gespeelde sessies. Door het
voorgaande te combineren kunnen we de beoogde leermethode formuleren. We maken
hier een onderscheid tussen een precieze methode, waarbij er geen rekening wordt
gehouden met eventuele onbepaaldheid en twee imprecieze methodes, waarbij dit
wel gebeurt. Uiteindelijk bekijken we de dynamica van de leermethode, namelijk
de evolutie van de inschattingen van spelers en de door hen gespeelde
strategieën. We onderzoeken voornamelijk onder welke voorwaarden er enkel
convergentie naar een evenwicht zal optreden.},
	author = {Erik Quaeghebeur},
	keywords = {evenwicht; fictief spelen; imprecieze waarschijnlijkheden; leren; speltheorie},
	localfile = {mastersthesis/Quaeghebeur-2001.pdf},
	school = {Universiteit Gent},
	title = {Speltheoretisch leren met imprecieze waarschijnlijkheden: dynamische aspecten},
	url = {http://hdl.handle.net/1854/6279},
	year = {2001}
}

@inproceedings{Quaeghebeur-DeCooman-Aeyels-2005,
	address = {Ghent, Belgium},
	author = {Erik Quaeghebeur and Gert {De Cooman} and Dirk Aeyels},
	booktitle = {Proceedings of the Sixth UGent-FirW PhD Symposium},
	title = {Building classifiers that cope with small training sets},
	year = {2005}
}

@book{Weichselberger-2001,
	address = {Heidelberg},
	annote = {Unter Mitarbeit von T. Augustin und A. Wallner},
	author = {Kurt Weichselberger},
	publisher = {Physica-Verlag},
	title = {Elementare Grundbegriffe einer allgemeineren Wahrscheinlichkeitsrechnung I: Intervallwahrscheinlichkeit als Umfassendes Konzept},
	year = {2001}
}

@proceedings{ICLMPS-1975,
	booktitle = {Fifth International Congress of Logic, Methodology and Philosophy of Science},
	title = {Fifth International Congress of Logic, Methodology and Philosophy of Science},
	year = {1975}
}

@proceedings{UAI-1995,
	booktitle = {UAI-95: Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
	editor = {Philippe Besnard and Steve Hanks},
	publisher = {Morgan Kaufmann},
	title = {UAI-95: Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence},
	year = {1995}
}

@article{Gaubert-Gunawardena-2004-Perron,
	abstract = {If A is a nonnegative matrix whose associated directed graph
is strongly connected, the Perron-Frobenius theorem asserts that A has an
eigenvector in the positive cone, (\mathbb{R}^+)^n. We associate a directed
graph to any homogeneous, monotone function, f : (\mathbb{R}+)n \to
(\mathbb{R}^+)^n, and show that if the graph is strongly connected, then f has a
(nonlinear) eigenvector in (\mathbb{R}^+)^n. Several results in the literature
emerge as corollaries. Our methods show that the Perron-Frobenius theorem is
"really" about the boundedness of invariant subsets in the Hilbert projective
metric. They lead to further existence results and open problems.},
	annote = {ook op papier als arXiv preprint},
	author = {Stéphane Gaubert and Jeremy Gunawardena},
	journal = {Transactions of the American Mathematical Society},
	keywords = {Collatz-Wielandt property; Hilbert projective met; Hilbert projective metric; nonexpansive function; nonlinear eigenvalue},
	localfile = {article/Gaubert-Gunawardena-2004-Perron.pdf},
	number = {12},
	pages = {4931–4950},
	title = {The Perron-Frobenius theorem for homogeneous, monotone functions},
	url = {http://www.ams.org/journals/tran/2004-356-12/S0002-9947-04-03470-1},
	volume = {356},
	year = {2004}
}

@book{Burrill-1972-measure,
	author = {Claude W. Burrill},
	publisher = {McGraw-Hill},
	title = {Measure, Integration, and Probability},
	year = {1972}
}

@article{Skulj-2009-impmarkov,
	abstract = {The parameters of Markov chain models are often not known
precisely. Instead of ignoring this problem, a better way to cope with it is to
incorporate the imprecision into the models. This has become possible with the
development of models of imprecise probabilities, such as the interval
probability model. In this paper we discuss some modelling approaches which
range from simple probability intervals to the general interval probability
models and further to the models allowing completely general convex sets of
probabilities. The basic idea is that precisely known initial distributions and
transition matrices are replaced by imprecise ones, which effectively means that
sets of possible candidates are considered. Consequently, sets of possible
results are obtained and represented using similar imprecise probability models.
We first set up the model and then show how to perform calculations of the
distributions corresponding to the consecutive steps of a Markov chain. We
present several approaches to such calculations and compare them with respect to
the accuracy of the results. Next we consider a generalisation of the concept of
regularity and study the convergence of regular imprecise Markov chains. We also
give some numerical examples to compare different approaches to calculations of
the sets of probabilities.},
	author = {Damjan Škulj},
	doi = {10.1016/j.ijar.2009.06.007},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Imprecise Markov chains; Imprecise probabilities; Interval probabilities; Markov chains; Regularity},
	localfile = {article/Skulj-2009-impmarkov.pdf},
	number = {8},
	pages = {1314–1329},
	title = {Discrete time Markov chains with interval probabilities},
	volume = {50},
	year = {2009}
}

@techreport{Bruening-Dennenberg-2003-belELP,
	annote = {Direct proof (of Choquet's implicit proof) that {0,1}-valued belief measures are the extreme points of the set of belief measures.},
	author = {Martin Brüning and Dieter Denneberg},
	institution = {Universität Bremen},
	title = {The $\sigma$-additive Möbius Transform of Belief Measures via Choquet's Theorem},
	year = {2003}
}

@inproceedings{DeCooman-Miranda-Quaeghebeur-2007-ISIPTA,
	abstract = {We consider immediate predictive inference, where a subject,
using a number of observations of a finite number of exchangeable random
variables, is asked to coherently model his beliefs about the next observation,
in terms of a predictive lower prevision. We study when such predictive lower
previsions are representation insensitive, meaning that they are essentially
independent of the choice of the (finite) set of possible values for the random
variables. Such representation insensitive predictive models have very
interesting properties, and among such models, the ones produced by the
Imprecise Dirichlet-Multinomial Model are quite special in a number of ways.},
	address = {Prague, Czech Republic},
	author = {Gert {De Cooman} and Enrique Miranda and Erik Quaeghebeur},
	editor = {Gert {De Cooman} and Jiřina Vejnarová and Marco Zaffalon},
	keywords = {coherence; exchangeability; immediate prediction},
	organization = {SIPTA},
	title = {Immediate prediction under exchangeability and represenation insensitivity},
	year = {2007}
}

@article{Aitchison-1964-tolerance,
	abstract = {In the theory of statistical tolerance regions, as usually
presented in frequentist terms, there are inherent difficulties of formulation,
development and interpretation. The present paper re-examines the basic problem
from a Bayesian point of view and suggests that such an approach provides a set
of widely applicable, mathematically tractable tools, often more tailored to the
requirements of users than the corresponding frequentist tools. For the
one-dimensional case, Bayesian intervals are quoted for a number of standard
distributions and prior densities, and the customary feature of a Bayesian
analysis–that special prior densities give rise to standard frequentist
results–is briefly demonstrated. A problem which seems to be of greater
practical significance, namely the selection of an optimum tolerance region from
a set of possible tolerance regions, is also investigated and the overwhelming
advantages of the Bayesian approach are indicated.},
	author = {J. Aitchison},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Aitchison-1964-tolerance.pdf},
	number = {2},
	pages = {161–175},
	title = {Two Papers on the Comparison of Bayesian and Frequentist Approaches to Statistical Problems of Prediction: Bayesian Tolerance Regions},
	url = {http://links.jstor.org/stable/2984416},
	volume = {26},
	year = {1964}
}

@article{Baroni-Vicig-2005-interchange,
	abstract = {This paper addresses the problem of exchanging uncertainty
assessments in multi-agent systems. Since it is assumed that each agent might
completely ignore the internal representation of its partners, a common
interchange format is needed. We analyze the case of an interchange format
deﬁned by means of imprecise probabilities, pointing out the reasons of this
choice. A core problem with the interchange format concerns transformations from
imprecise probabilities into other formalisms (in particular, precise
probabilities, possibilities, belief functions). We discuss this so far little
investigated question, analyzing how previous proposals, mostly regarding
special instances of imprecise probabilities, would ﬁt into this problem. We
then propose some general transformation procedures, which take also account of
the fact that information can be partial, i.e. may concern an arbitrary (ﬁnite)
set of events.},
	author = {Pietro Baroni and Paolo Vicig},
	doi = {10.1016/j.ijar.2005.03.001},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Imprecise probability theory; Multi-agent systems; Partial possibilities; Pignistic probability; Uncertainty transformations},
	localfile = {article/Baroni-Vicig-2005-interchange.pdf},
	pages = {147–180},
	title = {An uncertainty interchange format with imprecise probabilities},
	volume = {40},
	year = {2005}
}

@book{Holmes-1975,
	address = {New York},
	author = {Richard B. Holmes},
	number = {24},
	publisher = {Springer-Verlag},
	series = {Graduate Texts in Mathematics},
	title = {Geometric Functional Analysis and its Applications},
	year = {1975}
}

@incollection{Mura-2008-ch12,
	author = {Bruno de Finetti},
	doi = {10.1007/978-1-4020-8202-3},
	editor = {Alberto Mura},
	publisher = {Springer},
	series = {Synthese Library},
	title = {Complete additivity and zero probabilities},
	volume = {340},
	year = {2008}
}

@misc{Walley-0a,
	annote = {written notes},
	author = {Peter Walley},
	title = {The theory of natural extension}
}

@article{Cooman2007,
	author = {Gert {De Cooman}},
	number = {1556},
	title = {Representing and assessing exchangeable lower previsions},
	year = {2007}
}

@article{Wasserman-Kadane-1992,
	abstract = {One method for evaluating the sensitivity of a Bayesian
analysis is to embed the prior into a class of priors. Then bounds on prior and
posterior quantities of interest must be computed. This approach to inference,
often called robust Bayesian inference, has received much attention lately.
Implementing robust Bayesian methods entails difficult computations, especially
if the parameter space is high dimensional. In this article we develop a Monte
Carlo approach to computing these bounds and also explore some interesting
theoretical properties of certain classes of priors. The methods can be useful
in other situations in which bounds on expectations are required.},
	author = {Larry Wasserman and Joseph B. Kadane},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Wasserman-Kadane-1992.pdf},
	number = {418},
	pages = {516–522},
	publisher = {American Statistical Association},
	title = {Computing bounds on expectations},
	url = {http://www.jstor.org/stable/2290285},
	volume = {87},
	year = {1992}
}

@article{Walley-2000-towards,
	author = {Peter Walley},
	doi = {10.1016/S0888-613X(00)00031-1},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Choquet capacity; Comparative probability; coherence; credal sets; desirable gambles; foundations of probability; interval-valued probability; lower prevision; lower probability; partial preference ordering; uncertainty measures},
	localfile = {article/Walley-2000-towards.pdf},
	number = {2-3},
	pages = {125–148},
	title = {Towards a unified theory of imprecise probability},
	volume = {24},
	year = {2000}
}

@article{Diaconis-Zabell-1982,
	author = {Persi Diaconis and Sandy L. Zabell},
	issn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	localfile = {article/Diaconis-Zabell-1982.pdf},
	number = {380},
	pages = {822–830},
	publisher = {American Statistical Association},
	title = {Updating subjective probability},
	url = {http://www.jstor.org/stable/2287313},
	volume = {77},
	year = {1982}
}

@inproceedings{Lemmer-Kyburg-1991,
	author = {John F. Lemmer and Jr. Henry E. Kyburg},
	booktitle = {AAAI-91 Proceedings},
	pages = {488–493},
	title = {Conditions for the existence of belief functions corresponding to intervals of beliefs},
	year = {1991}
}

@article{Johnson-1932,
	author = {W. E. Johnson},
	issn = {0026-4423},
	journal = {Mind},
	localfile = {article/Johnson-1932.pdf},
	number = {164},
	pages = {409–423},
	publisher = {Oxford University Press on behalf of the Mind Association},
	title = {Probability: The Deductive and Inductive Problems},
	url = {http://www.jstor.org/stable/2250183},
	volume = {41},
	year = {1932}
}

@inproceedings{Moral-Wilson-1996,
	annote = {ook op papier},
	author = {Serafín Moral and Nic Wilson},
	booktitle = {Proceedings of the Sixth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 96},
	pages = {1337–1344},
	title = {Importance sampling algorithms for the calculation of Dempster-Shafer belief},
	volume = {3},
	year = {1996}
}

@proceedings{ISIPTA-2007,
	address = {Prague, Czech Republic},
	editor = {Gert {De Cooman} and Jiřina Vejnarová and Marco Zaffalon},
	localfile = {proceedings/ISIPTA-2007.pdf},
	organization = {SIPTA},
	publisher = {Action M Agency for SIPTA},
	title = {ISIPTA '07: Proceedings of the Fifth International Symposium on Imprecise Probabilities: Theories and Applications},
	url = {http://www.sipta.org/isipta07/proceedings/proceedings-optimised.pdf},
	year = {2007}
}

@article{Cano-etal-2007-cn,
	abstract = {This paper proposes two new algorithms for inference in
credal networks. These algorithms enable probability intervals to be obtained
for the states of a given query variable. The first algorithm is approximate and
uses the hill-climbing technique in the Shenoy-Shafer architecture to propagate
in join trees ; the second is exact and is a modification of Rocha and Cozman's
branch-and-bound algorithm, but applied to general directed acyclic graphs.},
	author = {Andrés Cano and Manuel Gómez and Serafín Moral and Joaquín Abellán},
	doi = {10.1016/j.ijar.2006.07.020},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Bayesian networks; branch-and-bound algorithms; credal network; hill-climbing; probability intervals; strong independence},
	localfile = {article/Cano-etal-2007-cn.pdf},
	number = {3},
	pages = {261–280},
	title = {Hill-climbing and branch-and-bound algorithms for exact and approximate inference in credal networks},
	volume = {44},
	year = {2007}
}

@article{Lo-1986-finite-sampling,
	author = {Albert Y. Lo},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Lo-1986-finite-sampling.pdf},
	number = {3},
	pages = {1226–1233},
	publisher = {Institute of Mathematical Statistics},
	title = {Bayesian statistical inference for sampling a finite population},
	volume = {14},
	year = {1986}
}

@article{Hall-Lawry-2004-approx,
	abstract = {Random set theory provides a convenient mechanism for
representing uncertain knowledge including probabilistic and set-based
information, and extending it through a function. This paper focuses upon the
situation when the available information is in terms of coherent lower and upper
probabilities, which are encountered, for example, when a probability
distribution is speciﬁed by interval parameters. We propose an Iterative
Rescaling Method (IRM) for constructing a random set with corresponding belief
and plausibility measures that are a close outer approximation to the lower and
upper probabilities. The approach is compared with the discrete approximation
method of Williamson and Downs (sometimes referred to as the p-box), which
generates a closer approximation to lower and upper cumulative probability
distributions but in most cases a less accurate approximation to the lower and
upper probabilities on the remainder of the power set. Four combination methods
are compared by application to example random sets generated using the IRM.},
	author = {Jim W. Hall and Jonathan Lawry},
	doi = {10.1016/j.ress.2004.03.005},
	journal = {Reliability Engineering \& System Safety},
	keywords = {Coherent lower and upper probabilities; Dempster–Shafer theory; Iterative rescaling method; Möbius inversion; Random set theory; p-Box},
	localfile = {article/Hall-Lawry-2004-approx.pdf},
	pages = {89–101},
	title = {Generation, combination and extension of random set approximations to coherent lower and upper probabilities},
	volume = {85},
	year = {2004}
}

@article{Xie-Beerel-1998-stateclassif,
	abstract = {This paper presents an efficient method for state
classification of finite-state Markov chains using binary-decision diagram-based
symbolic techniques. The method exploits the fundamental properties of a Markov
chain and classifies the state space by iteratively applying reachability
analysis. We compare our method with the state-of-the-art technique, which
requires the transitive closure of the transition relation of a Markov chain.
Experiments in over a dozen synchronous and asynchronous systems and queueing
networks demonstrate that our method dramatically reduces the CPU time needed
and solves much larger problems because of the reduced memory requirements},
	annote = {Geannoteerde versie op papier},
	author = {Aiguo Xie and P. A. Beerel},
	doi = {10.1109/43.736573},
	issn = {0278-0070},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	keywords = {BDD-based symbolic techniques; CPU time reduction; Markov processes; asynchronous systems; binary decision diagrams; binary-decision diagram; circuit analysis computing; finite-state Markov chains; queueing networks; reachability analysis; state classification; state space; synchronous systems},
	localfile = {article/Xie-Beerel-1998-stateclassif.pdf},
	number = {12},
	pages = {1334–1339},
	title = {Efficient state classification of finite-state Markov chains},
	volume = {17},
	year = {1998}
}

@article{Haldane-1948,
	author = {J. B. S. Haldane},
	journal = {Biometrika},
	localfile = {article/Haldane-1948.pdf},
	number = {3–4},
	pages = {297–300},
	title = {The precision of observed values of small frequencies},
	url = {http://www.jstor.org/stable/2332350},
	volume = {35},
	year = {1948}
}

@article{Pinkus-2005-approx,
	abstract = {Approximation theory is concerned with the ability to
approximate functions by simpler and more easily calculated functions. The first
question we ask in approximation theory concerns the possibility of
approximation. Is the given family of functions from which we plan to
approximate dense in the set of functions we wish to approximate? In this work,
we survey some of the main density results and density methods.},
	archiveprefix = {arXiv},
	arxivid = {math/0501328},
	author = {Allan Pinkus},
	eprint = {0501328},
	journal = {Surveys in Approximation Theory},
	localfile = {article/Pinkus-2005-approx.pdf},
	pages = {1–45},
	primaryclass = {math},
	title = {Density in Approximation Theory},
	url = {http://www.math.technion.ac.il/sat/papers/1},
	volume = {1},
	year = {2005}
}

@book{Hatcher-2002,
	author = {Allan Hatcher},
	publisher = {Cambridge University Press},
	title = {Algebraic Topology},
	year = {2002}
}

@inproceedings{DeCooman-Quaeghebeur-Miranda-2007-ISI,
	abstract = {This paper deals with belief models, and in particular lower
previsions, for both (finite) collections and (infinite) sequences of
exchangeable random variables taking a finite number of values. When such
collections or sequences are assumed to be exchangeable, this more or less means
that their specific order is irrelevant. We show that exchangeable lower
previsions can be written as a combination of (i) a coherent prevision
expressing that permutations of realisations of such collections or sequences
are considered equally likely, and (ii) a coherent lower prevision for the
`frequency' of occurrence of the different values the random variables can take.
This is the essence of representation in de Finetti's sense: we generalise his
results to coherent lower previsions, both for finite collections and infinite
sequences. We also solve a more practical problem: how to extend a number of
lower prevision assessments to an exchangeable lower prevision that is as
conservative as possible.},
	address = {Lisboa, Portugal},
	author = {Gert {De Cooman} and Erik Quaeghebeur and Enrique Miranda},
	booktitle = {Bulletin of the International Statistical Institute 56th Session – Proceedings},
	number = {1556},
	organization = {International Statistical Institute},
	title = {Representing and assessing exchangeable lower previsions},
	year = {2007}
}

@inproceedings{Skulj-2007-Markov,
	abstract = {In Markov chain theory a stochastic matrix P is regular if
some matrix power P^n contains only strictly positive elements. Regularity of
transition matrix of a Markov chain guarantees the existence of a unique
invariant distribution which is also the limiting distribution. In the present
paper a similar result is shown for the generalized Markov chain model that
replaces classical probabilities with interval probabilities. We generalize the
concept of regularity and show that for a regular interval transition matrix
sets of probabilities corresponding to consecutive steps of a Markov chain
converge to a unique limiting set of distributions that only depends on
transition matrix and is independent of the initial distribution. A similar
convergence result is also shown for approximations of the invariant set.},
	author = {Damjan Škulj},
	keywords = {Markov chain; interval probability},
	pages = {405–414},
	title = {Regular finite Markov chains with interval probabilities},
	year = {2007}
}

@article{Dempster-1967,
	abstract = {A multivalued mapping from a space X to a space S carries a
probability measure defined over subsets of X into a system of upper and lower
probabilities over subsets of S. Some basic properties of such systems are
explored in Sections 1 and 2. Other approaches to upper and lower probabilities
are possible and some of these are related to the present approach in Section 3.
A distinctive feature of the present approach is a rule for conditioning, or
more generally, a rule for combining sources of information, as discussed in
Sections 4 and 5. Finally, the context in statistical inference from which the
present theory arose is sketched briefly in Section 6.},
	annote = {ook op papier},
	author = {Arthur P. Dempster},
	journal = {The Annals of Mathematical Statistics},
	localfile = {article/Dempster-1967.pdf},
	number = {2},
	pages = {325–339},
	publisher = {Springer},
	title = {Upper and lower probabilities induced by a multivalued mapping},
	url = {http://www.jstor.org/stable/2239146},
	volume = {38},
	year = {1967}
}

@inproceedings{Quaeghebeur-DeCooman-2005,
	address = {Pittsburgh, Pennsylvania},
	author = {Erik Quaeghebeur and Gert {De Cooman}},
	booktitle = {ISIPTA '05: Proceedings of the Fourth International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Fabio Gagliardi Cozman and Robert Nau and Teddy Seidenfeld},
	organization = {SIPTA},
	pages = {287–296},
	title = {Imprecise probability models for inference in exponential families},
	year = {2005}
}

@inproceedings{Verheest-Hellberg-Mace-1998,
	annote = {reprint},
	author = {Frank Verheest and Manfred A. Hellberg and Richard L. Mace},
	booktitle = {AIP Conference Proceedings},
	organization = {American Institute of Physics},
	title = {New aspects of the Jeans instability in dusty plasmas},
	year = {1998}
}

@inproceedings{Alessio-Zaffalon-Miranda-2009-filtering,
	abstract = {We extend hidden Markov models for continuous variables
taking into account imprecision in our knowledge about the probabilistic
relationships involved. To achieve that, we consider sets of probabilities, also
called coherent lower previsions. In addition to the general formulation, we
study in detail a particular case of interest: linear-vacuous mixtures. We also
show, in a practical case, that our extension outperforms the Kalman filter when
modelling errors are present in the system.},
	address = {Seattle, Washington},
	annote = {ook op papier},
	author = {Alessio Benavoli and Marco Zaffalon and Enrique Miranda},
	booktitle = {FUSION 2009: Proceedings of the 12th International Conference on Information Fusion.},
	organization = {IEEE},
	pages = {1743–1750},
	title = {Reliable hidden Markov model filtering through coherent lower previsions},
	year = {2009}
}

@article{Wagner-2004-modus-tollens,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {British Journal of the Philosophy of Science},
	pages = {747–753},
	title = {Modus Tollens probabilized},
	volume = {55},
	year = {2004}
}

@article{VantVeer-etal-2002,
	annote = {ook op papier},
	author = {Laura J. {van 't Veer} and Hongyue Dai and Marc J. {Van de Vijver} and Yudong D. He and Augustinus A. M. Hart and Mao Mao and Hans L. Peterse and Karin {Van der Kooy} and Matthew J. Marton and Anke T. Witteveen and George J. Schreiber and Ron M. Kerkhoven and Chris Roberts and Peter S. Linsley and René Bernards and Stephen H. Friend},
	doi = {10.1038/415530a},
	journal = {Nature},
	localfile = {article/VantVeer-etal-2002.pdf},
	number = {6871},
	pages = {530–536},
	publisher = {Nature Publishing Group},
	title = {Gene expression profiling predicts clinical outcome of breastcancer},
	volume = {415},
	year = {2002}
}

@article{Jamison-Lodwick-2001,
	abstract = {In this paper we begin with a standard form of the linear
programming problem. We replace each constant in the problem with a fuzzy
number. We then reformat the objective and constraints into an unconstrained
fuzzy function by penalizing the objective for possible constraint violations.
The range of this fuzzy function lies in the space of fuzzy numbers. The
objective is then redefined as optimizing the expected midpoint of the image of
this fuzzy function. We show that this objective defines a concave function
which, therefore, can be maximized globally. We present an algorithm for finding
the optimum.},
	author = {K. David Jamison and Weldon A. Lodwick},
	doi = {10.1016/S0165-0114(99)00082-2},
	issn = {0165-0114},
	journal = {Fuzzy Sets and Systems},
	keywords = {Fuzzy function; Fuzzy number; Linear programming; possibility distribution},
	localfile = {article/Jamison-Lodwick-2001.pdf},
	number = {1},
	pages = {97–110},
	title = {Fuzzy linear programming using a penalty method},
	volume = {119},
	year = {2001}
}

@misc{Doumont-2001-persuading,
	author = {Jean-luc Doumont},
	title = {Persuading others},
	year = {2001}
}

@article{Steuer-1981,
	abstract = {This paper presents three algorithms for solving linear
programming problems in which some or all of the objective function coefficients
are specified in terms of intervals. Which algorithm is applicable depends upon
(a) the number of interval objective function coefficients, (b) the number of
nonzero objective function coefficients, and (c) whether or not the feasible
region is bounded. The algorithms output all extreme points and unbounded edge
directions that are "multiparametrically optimal" with respect to the ranges
placed on the objective function coefficients. The algorithms are most suitable
to linear programs in which the objective function coefficients are
deterministic but are likely to vary from time period to time period (as for
example in blending problems).},
	author = {Ralph E. Steuer},
	issn = {0364-765X},
	journal = {Mathematics of Operations Research},
	localfile = {article/Steuer-1981.pdf},
	number = {3},
	pages = {333–348},
	publisher = {INFORMS},
	title = {Algorithms for linear programming problems with interval objective function coefficients},
	url = {http://www.jstor.org/stable/3689177},
	volume = {6},
	year = {1981}
}

@proceedings{Klee-1963,
	editor = {V. L. {Klee, Jr. }},
	publisher = {American Mathematical Society},
	series = {Proceedings of Symposia in Pure Mathematics},
	title = {Convexity: Proceedings of the Seventh Symposium in Pure Mathematics of the American Mathematical Society},
	url = {http://books.google.com/books?id=MuEFJR7Ek4EC},
	year = {1963}
}

@book{Grinstead-Snell-2006-probintro,
	annote = {GNU FDL version, source available},
	author = {Charles M. Grinstead and J. Laurie Snell},
	publisher = {American Mathematical Society},
	title = {Introduction to Probability},
	year = {2006}
}

@article{Haddad-Moreaux-2007,
	abstract = {Performance evaluation of complex systems is a critical
issue and bounds computation provides confidence about service quality,
reliability, etc. of such systems. The stochastic ordering theory has generated
a lot of works on bounds computation. Maximal lower and minimal upper bounds of
a Markov chain by a st-monotone one exist and can be efficiently computed. In
the present work, we extend simultaneously this last result in two directions.
On the one hand, we handle the case of a maximal monotone lower bound of a
family of Markov chains where the coefficients are given by numerical intervals.
On the other hand, these chains are sub-chains associated to sub-stochastic
matrices. We prove the existence of this maximal bound and we provide polynomial
time algorithms to compute it both for discrete and continuous Markov chains.
Moreover, it appears that the bounding sub-chain of a family of strictly
sub-stochastic ones is not necessarily strictly sub-stochastic. We establish a
characterization of the families of sub-chains for which these bounds are
strictly sub-stochastic. Finally, we show how to apply these results to a
classical model of repairable system. A forthcoming paper will present detailed
numerical results and comparison with other methods.},
	author = {Serge Haddad and Patrice Moreaux},
	doi = {10.1016/j.ejor.2005.08.016},
	issn = {0377-2217},
	journal = {European Journal of Operational Research},
	keywords = {Markov process; Stochastic bound; Stochastic process; Strong stochastic ordering; Sub-Markov chain},
	localfile = {article/Haddad-Moreaux-2007.pdf},
	number = {2},
	pages = {999–1015},
	title = {Sub-stochastic matrix analysis for bounds computation–Theoretical results},
	volume = {176},
	year = {2007}
}

@article{Boratynska-1997,
	abstract = {The problem of estimating the unknown parameter of a
one-parameter exponential family with the conjugate prior is considered. Some
uncertainty about the prior is assumed by introducing a class of priors Gamma.
The most robust and conditional Gamma-minimax estimators are constructed. The
situations when those estimators coincide are presented. The paper is a
generalization of the result for the Poisson distribution obtained in Mezarski
and Zielinski (1991).},
	annote = {ook op papier},
	author = {Agata Boratyńska},
	doi = {10.1016/S0167-7152(97)00060-6},
	journal = {Statistics \& Probability Letters},
	keywords = {Bayes estimators; classes of priors; one-parameter exponential family; robust Bayesian estimation},
	localfile = {article/Boratynska-1997.pdf},
	number = {2},
	pages = {173–178},
	publisher = {Elsevier},
	title = {Stability of Bayesian inference in exponential families},
	volume = {36},
	year = {1997}
}

@article{Schervish-Seidenfeld-Kadane-2002-incoherence,
	author = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
	journal = {Sankhya Series A},
	localfile = {article/Schervish-Seidenfeld-Kadane-2002-incoherence.pdf},
	number = {3},
	pages = {561–587},
	title = {Measuring Incoherence},
	url = {http://repository.cmu.edu/statistics/29},
	volume = {64},
	year = {2002}
}

@article{Roy-1987,
	author = {Nina M. Roy},
	doi = {10.2307/2322725},
	journal = {The American Mathematical Monthly},
	localfile = {article/Roy-1987.pdf},
	number = {5},
	pages = {409–422},
	title = {Extreme points of convex sets in infinite dimensional spaces},
	url = {http://www.jstor.org/stable/2322725},
	volume = {94},
	year = {1987}
}

@inproceedings{Walter-Augustin-Peters-2007-regression,
	abstract = {Regression is the central concept in applied statistics for
analyzing multivariate, heterogenous data: The influence of a group of variables
on one other variable is quantified by the regression parameter $\beta$. In this
paper, we extend standard Bayesian inference on $\beta$ in linear regression
models by considering imprecise conjugated priors. Inspired by a variation and
an extension of a method for inference in i.i.d. exponential families presented
at ISIPTA'05 by Quaeghebeur and de Cooman, we develop a general framework for
handling linear regression models including analysis of variance models, and
discuss obstacles in direct implementation of the method. Then properties of the
interval-valued point estimates for a two-regressor model are derived and
illustrated with simulated data. As a practical example we take a small data set
from the AIRGENE study and consider the influence of age and body mass index on
the concentration of an inflammation marker.},
	address = {Prague, Czech Republic},
	author = {Gero Walter and Thomas Augustin and Annette Peters},
	editor = {Gert {De Cooman} and Jiřina Vejnarová and Marco Zaffalon},
	organization = {SIPTA},
	title = {Linear Regression Analysis under Sets of Conjugate Priors},
	year = {2007}
}

@article{Seidenfeld-Wasserman-1993,
	abstract = {Suppose that a probability measure P is known to lie in a
set of probability measures M. Upper and lower bounds on the probability of any
event may then be computed. Sometimes, the bounds on the probability of an event
A conditional on an event B may strictly contain the bounds on the unconditional
probability of A. Surprisingly, this might happen for every B in a partition
\mathscr{B}. If so, we say that dilation has occurred. In addition to being an
interesting statistical curiosity, this counterintuitive phenomenon has
important implications in robust Bayesian inference and in the theory of upper
and lower probabilities. We investigate conditions under which dilation occurs
and we study some of its implications. We characterize dilation immune
neighborhoods of the uniform measure.},
	annote = {ook op papier},
	author = {Teddy Seidenfeld and Larry Wasserman},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	localfile = {article/Seidenfeld-Wasserman-1993.pdf},
	number = {3},
	pages = {1139–1154},
	publisher = {Institute of Mathematical Statistics},
	title = {Dilation for sets of probabilities},
	volume = {21},
	year = {1993}
}

@article{Giron-Rios-1980,
	abstract = {In this paper the theoretical and practical implications of
dropping-from the basic Bayesian coherence principles- the assumption of
comparability of every pair of acts is examined. The resulting theory is shown
to be still perfectly coherent and has Bayesian theory as a particular case. In
particular we question the need of weakening or ruling out some of the axioms
that constitute the coherence principles; what are their practical implications;
how this drive to the notion of partial information or partial uncertainty in a
certain sense; how this partial information is combined with sample information
and how this relates to Bayesian methods. We also point out the relation of this
approach to rational behaviour with the more (and apparently unrelated) general
notion of domination structures as applied to multicrieria decision making.},
	author = {F. J. Girón and S. Rios},
	doi = {10.1007/BF02888345},
	journal = {Trabajos de Estadística y de Investigación Operativa},
	localfile = {article/Giron-Rios-1980.pdf},
	number = {1},
	pages = {17–38},
	title = {Quasi-Bayesian behaviour: a more realistic approach to decision making?},
	volume = {31},
	year = {1980}
}

@book{Grunbaum-1967,
	address = {London},
	author = {Branko Grünbaum},
	publisher = {Interscience Publishers},
	title = {Convex Polytopes},
	year = {1967}
}

@article{Sine-1990-Perron-Frobenius,
	abstract = {If T is a nonexpansive map on a domain in a
finite-dimensional sup-norm space then there is a universal bound on the periods
of periodic points. This yields the same result for T nonexpansive on a domain
in a finite-dimensional Banach space which has a polyhedral unit ball. Similar
results are obtained for certain nonexpansive maps defined on all of an
infinite-dimensional L\_p space with 1<p<∞.},
	author = {Robert Sine},
	journal = {Proceedings of the American Mathematical Society},
	localfile = {article/Sine-1990-Perron-Frobenius.pdf},
	number = {2},
	pages = {331–336},
	title = {A nonlinear Perron-Frobenius theorem},
	url = {http://www.ams.org/proc/1990-109-02/S0002-9939-1990-0948156-X/S0002-9939-1990-0
948156-X.pdf},
	volume = {109},
	year = {1990}
}

@article{Skyrms-1993,
	abstract = {Maher (1992b) advances an objection to dynamic Dutch-book
arguments, partly inspired by the discussion in Levi (1987; in particular by
Levi's case 2, p. 204). Informally, the objection is that the decision maker
will "see the dutch book coming" and consequently refuse to bet, thus escaping
the Dutch book. Maher makes this explicit by modeling the decision maker's
choices as a sequential decision problem. On this basis he claims that there is
a mistake in dynamic coherence arguments. There is really no formal mistake in
classical dynamic coherence arguments, but the discussions in Maher and Levi do
suggest interesting ways in which the definition of dynamic coherence might be
strengthened. Such a strengthened "sequentialized" notion of dynamic coherence
is explored here. It so happens that even on the strengthened standards for a
Dutch book, the classic dynamic coherence argument for conditioning still goes
through.},
	author = {Brian Skyrms},
	issn = {0031-8248},
	journal = {Philosophy of Science},
	localfile = {article/Skyrms-1993.pdf},
	number = {2},
	pages = {320–328},
	publisher = {The University of Chicago Press on behalf of the Philosophy of Science Association},
	title = {A mistake in dynamic coherence arguments?},
	url = {http://www.jstor.org/stable/188357},
	volume = {60},
	year = {1993}
}

@book{Brown-1986,
	address = {Hayward, California},
	annote = {Geselecteerde delen kopies},
	author = {Lawrence D. Brown},
	editor = {Shanti S. Gupta},
	publisher = {Institute of Mathematical Statistics},
	series = {Institute of Mathematical Statistics: Lecture Notes—Monograph Series},
	title = {Fundamentals of Statistical Exponential Families (with Applications in Statistical Decision Theory)},
	volume = {9},
	year = {1986}
}

@inproceedings{Feron-1981,
	address = {New York},
	annote = {Proceedings of the International Congress on Applied Systems Research and Cybernetics, Acapulco, Mexico, December 1980 enkel op papier},
	author = {R. Feron},
	booktitle = {Applied Systems and Cybernetics},
	editor = {G. E. Lasker},
	pages = {2831–2836},
	publisher = {Pergamom Press},
	series = {Fuzzy Sets and Systems, Possibility Theory and Special Topics in Systems Research},
	title = {Probabilistic and statistical study of random fuzzy sets whose referential is R^N},
	volume = {VI},
	year = {1981}
}

@inproceedings{Baroni-Vicig-2000-interchange,
	author = {Pietro Baroni and Paolo Vicig},
	booktitle = {Proceedings of IPMU 2000},
	pages = {1027–1034},
	title = {An uncertainty interchange format for multi-agent systems based on imprecise probabilities},
	year = {2000}
}

@article{Fudenberg-Levine-1995,
	abstract = {We study a variation of fictitious play, in which the
probability of each action is an exponential function of that action's utility
against the historical frequency of opponents' play. Regardless of the
opponents' strategies, the utility received by an agent using this rule is
nearly the best that could be achieved against the historical frequency. Such
rules are approximately optimal in i.i.d. environments, and guarantee nearly the
minmax regardless of opponents' behavior. Fictitious play shares these
properties provided it switches ‘infrequently’ between actions. We also study
the long-run outcomes when all players use consistent and cautious rules.},
	author = {Drew Fudenberg and David K. Levine},
	doi = {10.1016/0165-1889(94)00819-4},
	journal = {Journal of Economic Dynamics and Control},
	keywords = {Games; Learning; Rationality; consistency},
	localfile = {article/Fudenberg-Levine-1995.pdf},
	number = {5-7},
	pages = {1065–1089},
	publisher = {Elsevier},
	title = {Consistency and cautious fictitious play},
	volume = {19},
	year = {1995}
}

@article{DeFinetti-1937,
	annote = {geannoteerde kopie},
	author = {Bruno de Finetti},
	journal = {Annales de l'Institut Henri Poincaré},
	localfile = {article/DeFinetti-1937.pdf},
	number = {1},
	pages = {1–68},
	publisher = {Institut Henri Poincaré},
	title = {La prévision: ses lois logiques, ses sources subjectives},
	url = {http://www.numdam.org/item?id=AIHP_1937__7_1_1_0},
	volume = {7},
	year = {1937}
}

@article{Arnold-Castillo-Sarabia-1993,
	annote = {ook op papier},
	author = {Barry C. Arnold and Enrique Castillo and Jose María Sarabia},
	doi = {10.1080/02331889308802432},
	journal = {Statistics},
	localfile = {article/Arnold-Castillo-Sarabia-1993.pdf},
	pages = {71–77},
	publisher = {Taylor \& Francis},
	title = {Conjugate exponential family priors for exponential family likelihoods},
	volume = {25},
	year = {1993}
}

@inproceedings{Schervish-Seidenfeld-Kadane-2003-incoherence,
	address = {Oxford},
	annote = {Proceedings of the Seventh Valencia International Meeting, 2–6 June 2002},
	author = {Mark J. Schervish and Teddy Seidenfeld and Joseph B. Kadane},
	booktitle = {Bayesian Statistics 7},
	editor = {José M. Bernardo and others},
	organization = {ISBA},
	pages = {385–402},
	publisher = {Clarendon Press},
	title = {Measures of incoherence: How not to gamble if you must},
	year = {2003}
}

@article{Levi-1977,
	author = {Isaac Levi},
	journal = {The Journal of Philosophy},
	localfile = {article/Levi-1977.pdf},
	number = {1},
	pages = {5–29},
	title = {Direct inference},
	url = {http://www.jstor.org/stable/2025732},
	volume = {74},
	year = {1977}
}

@article{Mosimann-1962,
	annote = {ook op papier},
	author = {James E. Mosimann},
	journal = {Biometrika},
	localfile = {article/Mosimann-1962.pdf},
	number = {1/2},
	pages = {65–82},
	title = {On the compound multinomial distribution, the multivariate $\beta$-distribution, and correlations among proportions},
	url = {http://www.jstor.org/stable/2333468},
	volume = {49},
	year = {1962}
}

@article{Diaconis-Freedman-1982-exchangeability,
	abstract = {Let X\_1, X\_2,⋯, X\_k, X\_{k+1},⋯, X\_n be exchangeable
random variables taking values in the set S. The variation distance between the
distribution of X\_1, X\_2,⋯, X\_k and the closest mixture of independent,
identically distributed random variables is shown to be at most 2 ck/n, where c
is the cardinality of S. If c is infinite, the bound k(k - 1)/n is obtained.
These results imply the most general known forms of de Finetti's theorem.
Examples are given to show that the rates k/n and k(k - 1)/n cannot be improved.
The main tool is a bound on the variation distance between sampling with and
without replacement. For instance, suppose an urn contains n balls, each marked
with some element of the set S, whose cardinality c is finite. Now k draws are
made at random from this urn, either with or without replacement. This generates
two probability distributions on the set of k-tuples, and the variation distance
between them is at most 2 ck/n.},
	author = {Persi Diaconis and D. Freedman},
	journal = {The Annals of Probability},
	localfile = {article/Diaconis-Freedman-1982-exchangeability.pdf},
	number = {4},
	pages = {745–764},
	title = {Finite exchangeable sequences},
	url = {http://www.jstor.org/stable/2242823},
	volume = {8},
	year = {1980}
}

@book{Durbin-etal-1998-seqalign,
	author = {R. Durbin and S. R. Eddy and Anders Krogh and G. Mitchison},
	publisher = {Cambridge University Press},
	title = {Biological sequence analysis: probabilistic models of proteins and nucleic acids},
	year = {1998}
}

@article{Coolen-1993,
	abstract = {Reconsidering generalizations of the original Bayesian
framework that have been suggested during the last three decades, imprecise
conjugate prior densities are proposed for members of the one-parameter
exponential family of distributions.},
	annote = {reprint},
	author = {Frank P. A. Coolen},
	doi = {10.1016/0167-7152(93)90066-R},
	journal = {Statistics \& Probability Letters},
	keywords = {Bayesian theory; conjugate priors; imprecise pro},
	localfile = {article/Coolen-1993.pdf},
	number = {5},
	pages = {337–342},
	publisher = {Elsevier},
	title = {Imprecise conjugate prior densities for the one-parameter exponential family of distributions},
	volume = {16},
	year = {1993}
}

@article{Campos-Cozman-2007-epistemic,
	abstract = {This paper investigates the computation of lower/upper
expectations that must cohere with a collection of probabilistic assessments and
a collection of judgements of epistemic independence. New algorithms, based on
multilinear programming, are presented, both for independence among events and
among random variables. Separation properties of graphical models are also
investigated.},
	author = {Cassio Polpo de Campos and Fabio Gagliardi Cozman},
	doi = {10.1016/j.ijar.2006.07.013},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {concepts of independence; epistemic independence; imprecise probabilities; multilinear programming; sets of probability measures},
	localfile = {article/Campos-Cozman-2007-epistemic.pdf},
	number = {3},
	pages = {244–260},
	publisher = {Elsevier},
	title = {Computing lower and upper expectations under epistemic independence},
	url = {http://www.sciencedirect.com/science/article/pii/S0888613X0600096X},
	volume = {44},
	year = {2007}
}

@article{Kreinovich-Xiang-Ferson-2006,
	abstract = {In many real-life situations, we only have partial
information about the actual probability distribution. For example, under
Dempster-Shafer uncertainty, we only know the masses m1, ... ,mn assigned to
different sets S1, ... ,Sn, but we do not know the distribution within each set
Si. Because of this uncertainty, there are many possible probability
distributions consistent with our knowledge; different distributions have, in
general, different values of standard statistical characteristics such as mean
and variance. It is therefore desirable, given a Dempster-Shafer knowledge base,
to compute the ranges and of possible values of mean E and of variance V. In
their recent paper, Langewisch and Choobineh show how to compute these ranges in
polynomial time. In particular, they reduce the problem of computing to the
problem of minimizing a convex quadratic function, a problem which can be solved
in time O(n2 [middle dot] log(n)). We show that the corresponding quadratic
optimization problem can be actually solved faster, in time O(n [middle dot]
log(n)); thus, we can compute the bounds V and in time O(n [middle dot]
log(n)).},
	author = {Vladik Kreinovich and Gang Xiang and Scott Ferson},
	doi = {10.1016/j.ijar.2005.12.001},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Kreinovich-Xiang-Ferson-2006.pdf},
	number = {3},
	pages = {212–227},
	title = {Computing mean and variance under Dempster-Shafer uncertainty: Towards faster algorithms},
	volume = {42},
	year = {2006}
}

@article{Inuiguchi-2007,
	abstract = {In this paper, we treat fuzzy linear programming problems
with uncertain parameters whose ranges are specified as fuzzy polytopes. The
problem is formulated as a necessity measure optimization model. It is shown
that the problem can be reduced to a semi-infinite programming problem and
solved by a combination of a bisection method and a relaxation procedure. An
algorithm in which the bisection method and the relaxation procedure converge
simultaneously is proposed. A simple numerical example is given to illustrate
the solution procedure.},
	author = {Masahiro Inuiguchi},
	doi = {10.1016/j.fss.2007.04.004},
	issn = {0165-0114},
	journal = {Fuzzy Sets and Systems},
	keywords = {Bisection method; Fuzzy polytope; Necessity measure; Possibilistic linear programming; Relaxation procedure; Semi-infinite programming},
	localfile = {article/Inuiguchi-2007.pdf},
	number = {17},
	pages = {1882–1891},
	title = {Necessity measure optimization in linear programming problems with fuzzy polytopes},
	volume = {158},
	year = {2007}
}

@book{Letac-1992,
	annote = {Hoofdstukken 1, 2 en 4 kopies},
	author = {Gérard Letac},
	number = {50},
	publisher = {Conselho Nacional de Desenvolvimento Cientifico e Tecnológico, Instituto de Matemática Pura e Aplicada (IMPA)},
	series = {Monografias de Matemática},
	title = {Lectures on natural exponential families and their variance functions},
	year = {1992}
}

@article{Tanaka-1993,
	author = {Yoshihiro Tanaka},
	journal = {Economic Journal of Hokkaido University},
	localfile = {article/Tanaka-1993.pdf},
	pages = {159–166},
	title = {On Convexity of A System of Linear Interval Equations},
	url = {http://hdl.handle.net/2115/30499},
	volume = {22},
	year = {1993}
}

@book{Whittle-1992,
	author = {Peter Whittle},
	edition = {3},
	series = {Springer Texts in Statistics},
	title = {Probability via Expectation},
	volume = {XVIII},
	year = {1992}
}

@proceedings{SMPS-2004,
	address = {Oviedo, Spain},
	booktitle = {Soft Methodology and Random Information Systems},
	editor = {Miguel Lopéz-Díaz and María Angeles Gil and Przemysław Grzegorzewski and Olgierd Hryniewicz and Jonathan Lawry},
	publisher = {Springer},
	series = {Advances in soft computing},
	title = {Soft Methodology and Random Information Systems},
	year = {2004}
}

@article{DeCooman-Miranda-Quaeghebeur-2009-RIP,
	abstract = {We consider immediate predictive inference, where a subject,
using a number of observations of a finite number of exchangeable random
variables, is asked to coherently model his beliefs about the next observation,
in terms of a predictive lower prevision. We study when such predictive lower
previsions are representation insensitive, meaning that they are essentially
independent of the choice of the (finite) set of possible values for the random
variables. We establish that such representation insensitive predictive models
have very interesting properties, and show that among such models, the ones
produced by the Imprecise Dirichlet-Multinomial Model are quite special in a
number of ways. In the Conclusion, we discuss the open question as to how unique
the predictive lower previsions of the Imprecise Dirichlet-Multinomial Model are
in being representation insensitive.},
	author = {Gert {De Cooman} and Enrique Miranda and Erik Quaeghebeur},
	doi = {10.1016/j.ijar.2008.03.010},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Coherence; Exchangeability; Immediate prediction; Imprecise Dirichlet-Multinomial Model; Imprecise probabilities; Johnson’s sufficientness postulate; Lower prevision; Predictive inference; Representation insensitivity; Representation invariance principle; Rule of Succession},
	localfile = {article/DeCooman-Miranda-Quaeghebeur-2009-RIP.pdf},
	number = {2},
	pages = {204–216},
	publisher = {Elsevier},
	title = {Representation insensitivity in immediate prediction under exchangeability},
	volume = {50},
	year = {2009}
}

@book{Boyd-Vandenberghe-2004,
	author = {Stephen Boyd and Lieven Vandenberghe},
	publisher = {Cambridge University Press},
	title = {Convex Optimization},
	url = {http://www.stanford.edu/~boyd/cvxbook},
	year = {2004}
}

@book{Golub-VanLoan-1989,
	author = {Gene H. Golub and Charles F. {Van Loan}},
	edition = {2},
	publisher = {Johns Hopkins University Press},
	series = {Johns Hopkins Series in the Mathematical Sciences},
	title = {Matrix Computation},
	year = {1989}
}

@article{Shapley-1971,
	abstract = {The core of an n-person game is the set of feasible outcomes
that cannot be improved upon by any coalition of players. A convex game is
defined as one that is based on a convex set function. In this paper it is shown
that the core of a convex game is not empty and that it has an especially
regular structure. It is further shown that certain other cooperative solution
concepts are related in a simple way to the core: The value of a convex game is
the center of gravity of the extreme points of the core, and the von
Neumann-Morgenstern stable set solution of a convex game is unique and coincides
with the core.},
	annote = {ook op papier},
	author = {Lloyd S. Shapley},
	doi = {10.1007/BF01753431},
	journal = {International Journal of Game Theory},
	localfile = {article/Shapley-1971.pdf},
	number = {1},
	pages = {11–26},
	publisher = {Springer},
	title = {Cores of convex games},
	volume = {1},
	year = {1971}
}

@article{Nowak-May-Sigmund-1995connor,
	author = {Martin A. Nowak and Robert M. May and Karl Sigmund},
	journal = {Scientific American},
	pages = {76–81},
	title = {The Arithmetics of Mutual Help},
	year = {1995}
}

@article{Frechet-1948,
	author = {Maurice Fréchet},
	journal = {Annales de l'Institut Henri Poincaré},
	localfile = {article/Frechet-1948.pdf},
	number = {4},
	pages = {215–310},
	title = {Les éléments aléatoires de nature quelconque dans un espace distancié},
	url = {http://www.numdam.org/item?id=AIHP_1948__10_4_215_0},
	volume = {10},
	year = {1948}
}

@proceedings{NIPS2003,
	booktitle = {NIPS},
	editor = {Sebastian Thrun and Lawrence K Saul and Bernhard Schölkopf},
	isbn = {0-262-20152-6},
	publisher = {MIT Press},
	title = {Advances in Neural Information Processing Systems 16 [Neural Information Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British Columbia, Canada]},
	year = {2004}
}

@proceedings{ICML-2000,
	booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
	title = {Proceedings of the 17th International Conference on Machine Learning (ICML-2000)},
	year = {2000}
}

@inproceedings{Williams-1975-coherence,
	author = {Peter M. Williams},
	booktitle = {Fifth International Congress of Logic, Methodology and Philosophy of Science},
	pages = {29–33},
	title = {Coherence, strict coherence and zero probabilities},
	volume = {VI},
	year = {1975}
}

@article{Miranda-2008-survey,
	abstract = {This paper presents a summary of Peter Walley's theory of
coherent lower previsions. We introduce three representations of coherent
assessments: coherent lower and upper previsions, closed and convex sets of
linear previsions, and sets of desirable gambles. We show also how the notion of
coherence can be used to update our beliefs with new information, and a number
of possibilities to model the notion of independence with coherent lower
previsions. Next, we comment on the connection with other approaches in the
literature: de Finetti's and Williams' earlier work, Kuznetsov's and
Weischelberger's work on interval-valued probabilities, Dempster-Shafer theory
of evidence and Shafer and Vovk's game-theoretic approach. Finally, we present a
brief survey of some applications and summarize the main strengths and
challenges of the theory.},
	author = {Enrique Miranda},
	doi = {10.1016/j.ijar.2007.12.001},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Avoiding sure loss; Coherence; Conditional lower previsions; Desirability; Imprecision; Independence; Subjective probability},
	localfile = {article/Miranda-2008-survey.pdf},
	number = {2},
	pages = {628–658},
	title = {A survey of the theory of coherent lower previsions},
	volume = {48},
	year = {2008}
}

@book{Munro-1999,
	editor = {N. Munro},
	publisher = {The Institution of Electrical Engineers (IEE)},
	title = {The Use of Symbolic Methods in Control System Analysis and Design},
	year = {1999}
}

@book{Mura-2008,
	author = {Bruno de Finetti},
	doi = {10.1007/978-1-4020-8202-3},
	editor = {Alberto Mura},
	publisher = {Springer},
	series = {Synthese Library},
	title = {Philosophical Lectures on Probability},
	volume = {340},
	year = {2008}
}

@article{DeCooman-Hermans-Quaeghebeur-2009-PES,
	author = {Gert {De Cooman} and Filip Hermans and Erik Quaeghebeur},
	doi = {10.1017/S0269964809990039},
	journal = {Probability in the Engineering and Informational Sciences},
	number = {4},
	pages = {597–635},
	title = {Imprecise Markov chains and their limit behavior},
	volume = {23},
	year = {2009}
}

@book{Keynes-1921,
	author = {John Maynard Keynes},
	publisher = {Macmillan},
	title = {A Treatise on Probability},
	url = {http://www.gutenberg.org/ebooks/32625},
	year = {1921}
}

@article{Moslehian-2006,
	archiveprefix = {arXiv},
	arxivid = {math/0501048},
	author = {Mohammad Sal Moslehian},
	eprint = {0501048},
	journal = {Trends in Mathematics},
	keywords = {Complemented subspace; L1-predual space; Schauder basis; basis; complementary minimal subspace; prime space; quasi-complemented subspace; sequence spaces; weakly complemented subspace},
	localfile = {article/Moslehian-2006.pdf},
	number = {1},
	pages = {91–98},
	primaryclass = {math},
	title = {A survey of the complemented subspace problem},
	volume = {9},
	year = {2006}
}

@book{Martin-1966,
	author = {J. J. Martin},
	editor = {David B. Hertz},
	number = {13},
	publisher = {Wiley},
	series = {Publications in Operations Research},
	title = {Bayesian Decision problems and Markov Chains},
	year = {1966}
}

@article{Walley-Gurrin-Burton-1996,
	abstract = {This paper describes a new method, based on the theory of
imprecise probabilities, for analysing clinical data in the form of a
contingency table. The method is applied to a well-known set of statistical data
from randomized clinical trials of two treatments for severe cardiorespiratory
failure in newborn babies. Two problems are distinguished. The inference problem
is to draw conclusions about which treatment is more effective. The decision
problem is to determine whether one treatment should be preferred to another for
the next patient, or whether it is ethical to select the treatment by
randomization. The two problems are analysed using three possible models for
prior ignorance about the statistical parameters, and one of the models is
modified to take account of earlier clinical data. In this example the four
models produce essentially the same conclusions.},
	annote = {ook op papier},
	author = {Peter Walley and Lyle Gurrin and Paul Burton},
	journal = {The Statistician},
	localfile = {article/Walley-Gurrin-Burton-1996.pdf},
	number = {4},
	pages = {457–485},
	title = {Analysis of clinical data using imprecise prior probabilities},
	url = {http://www.jstor.org/stable/2988546},
	volume = {45},
	year = {1996}
}

@inproceedings{DeCooman-etal-ISIPTA09-Markovtrees,
	abstract = {We replace strong independence in credal networks with the
weaker notion of epistemic irrelevance. Focusing on directed trees, we show how
to combine local credal sets into a global model, and we use this to construct
and justify an exact message-passing algorithm that computes updated beliefs for
a variable in the tree. The algorithm, which is essentially linear in the number
of nodes, is formulated entirely in terms of coherent lower previsions. We
supply examples of the algorithm's operation, and report an application to
on-line character recognition that illustrates the advantages of our model for
prediction.},
	address = {Durham, United Kingdom},
	annote = {ook op papier},
	author = {Gert {De Cooman} and Filip Hermans and Alessandro Antonucci and Marco Zaffalon},
	booktitle = {ISIPTA '09: Proceedings of the Sixth International Symposium on Imprecise Probabilities: Theories and Applications},
	editor = {Thomas Augustin and Frank P. A. Coolen and Serafin Moral and Matthias C. M. Troffaes},
	organization = {SIPTA},
	pages = {149–158},
	title = {Epistemic irrelevance in credal networks: the case of imprecise Markov trees},
	year = {2009}
}

@inproceedings{Maass-2003-ISIPTA,
	address = {Waterloo, Ontario, Canada},
	author = {Sebastian Maaß},
	booktitle = {ISIPTA '03: Proceedings of the Third International Symposium on Imprecise Probabilities and Their Applications},
	editor = {Jean-Marc Bernard and Teddy Seidenfeld and Marco Zaffalon},
	location = {Lugano, Switzerland},
	pages = {372–382},
	publisher = {Carleton Scientific},
	series = {Proceedings in Informatics},
	title = {Continuous Linear Representations of Coherent Lower Previsions},
	volume = {18},
	year = {2003}
}

@article{Miranda-DeCooman-2003,
	abstract = {Numerical possibility measures can be interpreted as systems
of upper betting rates for events. As such, they have a special part in the
unifying behavioural theory of imprecise probabilities, proposed by Walley. On
this interpretation, they should arguably satisfy certain rationality, or
consistency, requirements, such as avoiding sure loss and coherence. Using a
version of Walley's notion of epistemic independence suitable for possibility
measures, we study in detail what these rationality requirements tell us about
the construction of independent product possibility measures from given
marginals, and we obtain necessary and sufficient conditions for a product to
satisfy these criteria. In particular, we show that the well-known minimum and
product rules for forming independent joint distributions from marginal ones,
are only coherent when at least one of these distributions assume just the
values zero and one.},
	annote = {reprint},
	author = {Enrique Miranda and Gert {De Cooman}},
	doi = {10.1016/S0888-613X(02)00087-7},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Coherence; Conditioning; Epistemic independence; Independent product; Possibility theory; Upper probability},
	localfile = {article/Miranda-DeCooman-2003.pdf},
	pages = {23–42},
	title = {Epistemic independence in numerical possibility theory},
	volume = {32},
	year = {2003}
}

@article{Dawid-1985-symmetry,
	author = {A. Philip Dawid},
	doi = {10.1093/bjps},
	journal = {The British Journal for the Philosophy of Science},
	localfile = {article/Dawid-1985-symmetry.pdf},
	number = {2},
	pages = {107–128},
	publisher = {British Society for the Philosophy of Science},
	title = {Probability, symmetry and frequency},
	volume = {36},
	year = {1985}
}

@article{Consonni-Veronese-1992,
	annote = {ook op papier},
	author = {Guido Consonni and Piero Veronese},
	journal = {Journal of the American Statistical Association},
	keywords = {Bayesian statistics; least favorable prior; partial prior information},
	localfile = {article/Consonni-Veronese-1992.pdf},
	number = {420},
	pages = {1123–1127},
	title = {Conjugate priors for exponential families having quadratic variance functions},
	url = {http://www.jstor.org/stable/2290650},
	volume = {87},
	year = {1992}
}

@article{GutierrezPena-Rueda-2003,
	abstract = {Reference analysis, introduced by Bernardo (J. Roy. Statist.
Soc. 41 (1979) 113) and further developed by Berger and Bernardo (On the
development of reference priors (with discussion). In: J.M. Bernardo, J.O.
Berger, A.P. Dawid, A.F.M. Smith (Eds.), Bayesian Statistics, Vol. 4, Clarendon
Press, Oxford, pp. 35-60), has proved to be one of the most successful general
methods to derive noninformative prior distributions. In practice, however,
reference priors are typically difficult to obtain. In this paper we show how to
find reference priors for a wide class of exponential family likelihoods.},
	author = {Eduardo Gutiérrez-Peña and R. Rueda},
	doi = {10.1016/S0378-3758(01)00281-6},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Affine dual foliations; Bayesian inference; Natural exponential family; Quadratic variance function; Reference prior; cut},
	localfile = {article/GutierrezPena-Rueda-2003.pdf},
	number = {1-2},
	pages = {35–54},
	title = {Reference priors for exponential families},
	volume = {110},
	year = {2003}
}

@inproceedings{Troffaes-2004,
	author = {Matthias C. M. Troffaes},
	booktitle = {Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems: IPMU 2004},
	pages = {571–578},
	title = {Efficient and robust global amino acid sequence alignment with uncertain evolutionary distances},
	volume = {1},
	year = {2004}
}

@article{Bernard-1997-specificity,
	annote = {ook op papier},
	author = {Jean-Marc Bernard},
	journal = {Revue Internationale de Systémique},
	localfile = {article/Bernard-1997-specificity.pdf},
	number = {1},
	pages = {11–29},
	title = {Bayesian analysis of tree-structured data},
	volume = {11},
	year = {1997}
}

@inbook{Boute-2004,
	annote = {Op papier},
	author = {Raymond T. Boute},
	booktitle = {Information Technology},
	pages = {85–114},
	publisher = {Kluwer Academic Publishers},
	title = {Formal Reasoning about Systems, Software and Hardware using Functionals, Predicates and Relations},
	year = {2004}
}

@phdthesis{DeMunck-2009-PhD,
	author = {Maarten {De Munck}},
	month = apr,
	school = {Katholieke Universiteit Leuven},
	title = {Efficient optimization approaches for interval and fuzzy finite element analysis},
	year = {2009}
}

@book{Liu-2007-uncertain-programming,
	author = {Baoding Liu},
	edition = {2},
	title = {Theory and Practice of Uncertain Programming},
	year = {2007}
}

@article{Kubis-2002,
	abstract = {The purpose of this paper is to investigate some separation
properties of sets with axiomatically defined convexity structures. We state a
general separation theorem for pairs of convexities, improving some known
results. As an application, we discuss separation properties of lattices, real
vector spaces and modules.},
	address = {Basel},
	author = {Wiesław Kubiś},
	doi = {10.1007/PL00012529},
	issn = {0047-2468},
	journal = {Journal of Geometry},
	keywords = {Mathematics; Statistics},
	localfile = {article/Kubis-2002.pdf},
	number = {1},
	pages = {110–119},
	publisher = {Birkhäuser},
	title = {Separation properties of convexity spaces},
	volume = {74},
	year = {2002}
}

@article{Pelessoni-Vicig-2005-convex,
	abstract = {Two classes of imprecise previsions, which we termed convex
and centered convex previsions, are studied in this paper in a framework close
to Walley's and Williams' theory of imprecise previsions. We show that convex
previsions are related with a concept of convex natural extension, which is
useful in correcting a large class of inconsistent imprecise probability
assessments, characterised by a condition of avoiding unbounded sure loss.
Convexity further provides a conceptual framework for some uncertainty models
and devices, like unnormalised supremum preserving functions. Centered convex
previsions are intermediate between coherent previsions and previsions avoiding
sure loss, and their not requiring positive homogeneity is a relevant feature
for potential applications. We discuss in particular their usage in (financial)
risk measurement. In a final part we introduce convex imprecise previsions in a
conditional environment and investigate their basic properties, showing how
several of the preceding notions may be extended and the way the generalised
Bayes rule applies.},
	author = {Renato Pelessoni and Paolo Vicig},
	doi = {10.1016/j.ijar.2004.10.007},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Convex conditional imprecise previsions; Convex imprecise previsions; Convex natural extension; Generalised Bayes rule; Risk measures},
	localfile = {article/Pelessoni-Vicig-2005-convex.pdf},
	number = {2-3},
	pages = {297–319},
	title = {Uncertainty modelling and conditioning with convex imprecise previsions},
	volume = {39},
	year = {2005}
}

@article{Berger-1994-robust-overview,
	abstract = {Robust Bayesian analysis is the study of the sensitivity of
Bayesian answers to uncertain inputs. This paper seeks to provide an overview of
the subject, one that is accessible to statisticians outside the field. Recent
developments in the area are also reviewed, though with very uneven emphasis.},
	author = {James Berger and Elías Moreno and Luis Raúl Pericchi and M. Bayarri and José M. Bernardo and Juan Cano and Julián {De la Horra} and Jacinto Martín and David Ríos-Insúa and Bruno Betrò and A. Dasgupta and Paul Gustafson and Larry Wasserman and Joseph B. Kadane and Srinivasan Cid and Michael Lavine and Anthony O'Hagan and Wolfgang Polasek and Christian Robert and Constantinos Goutis and Fabrizio Ruggeri and Gabriella Salinetti and Siva Sivaganesan},
	doi = {10.1007/BF02562676},
	journal = {Test},
	number = {1},
	pages = {5–124},
	title = {An overview of Robust Bayesian analysis},
	volume = {3},
	year = {1994}
}

@book{Shafer-1976,
	author = {Glenn Shafer},
	publisher = {Princeton University Press},
	title = {A mathematical theory of evidence},
	year = {1976}
}

@article{Hsu-et-al-2005,
	abstract = {Much is known about how people make decisions under varying
levels of probability (risk). Less is known about the neural basis of
decision-making when probabilities are uncertain because of missing information
(ambiguity). In decision theory, ambiguity about probabilities should not affect
choices. Using functional brain imaging, we show that the level of ambiguity in
choices correlates positively with activation in the amygdala and orbitofrontal
cortex, and negatively with a striatal system. Moreover, striatal activity
correlates positively with expected reward. Neurological subjects with
orbitofrontal lesions were insensitive to the level of ambiguity and risk in
behavioral choices. These data suggest a general neural circuit responding to
degrees of uncertainty, contrary to decision theory.},
	author = {Ming Hsu and Meghana Bhatt and Ralph Adolphs and Daniel Tranel and Colin F. Camerer},
	doi = {10.1126/science.1115327},
	journal = {Science},
	localfile = {article/Hsu-et-al-2005.pdf},
	month = dec,
	number = {5754},
	pages = {1680–1683},
	publisher = {American Association for the Advancement of Science},
	title = {Neural systems responding to degrees of uncertainty in human decision-making},
	volume = {310},
	year = {2005}
}

@phdthesis{Maass-2003-PhD,
	author = {Sebastian Maaß},
	school = {Universität Bremen},
	title = {Exact functionals, functionals preserving linear inequalities, Lévy's metric},
	year = {2003}
}

@article{Good-1952,
	abstract = {This paper deals first with the relationship between the
theory of probability and the theory of rational behaviour. A method is then
suggested for encouraging people to make accurate probability estimates, a
connection with the theory of information being mentioned. Finally Wald's theory
of statistical decision functions is summarised and generalised and its relation
to the theory of rational behaviour is discussed.},
	author = {I. J. Good},
	issn = {0035-9246},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	localfile = {article/Good-1952.pdf},
	number = {1},
	pages = {107–114},
	publisher = {Blackwell Publishing for the Royal Statistical Society},
	title = {Rational decisions},
	url = {http://www.jstor.org/stable/2984087},
	volume = {14},
	year = {1952}
}

@article{Sundberg-Wagner-1992,
	annote = {op papier in Wagnerbundel},
	author = {Carl Sundberg and Carl G. Wagner},
	journal = {Advances in Applied Mathematics of Operations Research},
	pages = {262–272},
	title = {Generalized Differences and Bayesian Conditioning of Choquet Capacities},
	volume = {13},
	year = {1992}
}

@article{DeCooman-Aeyels-1999,
	abstract = {We study the relation between possibility measures and the
theory of imprecise probabilities, and argue that possibility measures have an
important part in this theory. It is shown that a possibility measure is a
coherent upper probability if and only if it is normal. A detailed comparison is
given between the possibilistic and natural extension of an upper probability,
both in the general case and for upper probabilities defined on a class of
nested sets. We prove in particular that a possibility measure is the
restriction to events of the natural extension of a special kind of upper
probability, defined on a class of nested sets. We show that possibilistic
extension can be interpreted in terms of natural extension. We also prove that
when either the upper or the lower cumulative distribution function of a random
quantity is specified, possibility measures very naturally emerge as the
corresponding natural extensions. Next, we go from upper probabilities to upper
previsions. We show that if a coherent upper prevision defined on the convex
cone of all non-negative gambles is supremum preserving, then it must take the
form of a Shilkret integral associated with a possibility measure. But at the
same time, we show that such a supremum preserving upper prevision is never
coherent unless it is the vacuous upper prevision with respect to a non-empty
subset of the universe of discourse.},
	author = {Gert {De Cooman} and Dirk Aeyels},
	doi = {10.1016/S0020-0255(99)00007-9},
	issn = {0020-0255},
	journal = {Information Sciences},
	keywords = {choquet integral; coherence; lower cumulative distribution function; natural extension; possibilistic extension; possibility measure; upper cumulative distribution function; upper probability},
	localfile = {article/DeCooman-Aeyels-1999.pdf},
	number = {1-4},
	pages = {173–212},
	publisher = {Elsevier},
	title = {Supremum preserving upper probabilities},
	volume = {118},
	year = {1999}
}

@misc{Bernard-2003,
	annote = {Tutorial for ISIPTA'03},
	author = {Jean-Marc Bernard},
	title = {An Introduction to the Imprecise Dirichlet Model for Multinomial Data},
	year = {2003}
}

@misc{Doumont-2001-speaking,
	author = {Jean-luc Doumont},
	title = {Speaking in public},
	year = {2001}
}

@book{Raiffa-Schlaifer-1961,
	address = {Cambridge, Massachusetts},
	annote = {Boek bij René},
	author = {Howard Raiffa and Robert Schlaifer},
	publisher = {MIT Press},
	title = {Applied Statistical decision Theory},
	year = {1968}
}

@article{Barabasi-Oltvai-2004,
	abstract = {A key aim of postgenomic biomedical research is to
systematically catalogue all molecules and their interactions within a living
cell. There is a clear need to understand how these molecules and the
interactions between them determine the function of this enormously complex
machinery, both in isolation and when surrounded by other cells. Rapid advances
in network biology indicate that cellular networks are governed by universal
laws and offer a new conceptual framework that could potentially revolutionize
our view of biology and disease pathologies in the twenty-first century.},
	annote = {ook op papier},
	author = {Albert-László Barabási and Zoltán N. Oltvai},
	doi = {10.1038/nrg1272},
	journal = {Nature Reviews Genetics},
	localfile = {article/Barabasi-Oltvai-2004.pdf},
	number = {2},
	pages = {101–113},
	publisher = {Nature Publishing Group},
	title = {Network biology: understanding the cell's functional organization},
	volume = {5},
	year = {2004}
}

@article{Jordan-2004-graphical,
	author = {Michael I. Jordan},
	doi = {10.1214/088342304000000026},
	journal = {Statistical Science},
	localfile = {article/Jordan-2004-graphical.pdf},
	number = {1},
	pages = {140–155},
	title = {Graphical Models},
	volume = {19},
	year = {2004}
}

@book{Johnson-Kemp-Kotz-2005,
	author = {Norman L. Johnson and Adrienne W. Kemp and Samuel Kotz},
	edition = {3},
	publisher = {Wiley},
	title = {Univariate Discrete Distributions},
	year = {2005}
}

@article{DeCooman-Troffaes-Miranda-2008-exact,
	abstract = {We study n-monotone functionals, which constitute a
generalisation of n-monotone set functions. We investigate their relation to the
concepts of exactness and natural extension, which generalise coherence and
natural extension in the behavioural theory of imprecise probabilities. We
improve upon a number of results in the literature, and prove among other things
a representation result for exact n-monotone functionals in terms of Choquet
integrals.},
	author = {Gert {De Cooman} and Matthias C. M. Troffaes and Enrique Miranda},
	doi = {10.1016/j.jmaa.2008.05.071},
	issn = {0022-247X},
	journal = {Journal of Mathematical Analysis and Applications},
	keywords = {Choquet integral; coherence; comonotone additivi},
	localfile = {article/DeCooman-Troffaes-Miranda-2008-exact.pdf},
	number = {1},
	pages = {143–156},
	publisher = {Elsevier},
	title = {n-Monotone exact functionals},
	volume = {347},
	year = {2008}
}

@proceedings{MTNS-2000,
	booktitle = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
	title = {Fourteenth International Symposium on Mathematical Theory of Networks and systems: MTNS 2000},
	year = {2000}
}

@techreport{Troffaes-idmfacts,
	author = {Matthias C. M. Troffaes},
	title = {The imprecise Dirichlet model: facts and formulas}
}

@techreport{Berger-1993,
	annote = {ook op papier},
	author = {James Berger},
	institution = {Purdue University, Department of Statistics},
	number = {93-53C},
	title = {An Overview of Robust Bayesian Analysis},
	year = {1993}
}

@inproceedings{DeCooman-Miranda-2011,
	address = {Innsbruck, Austria},
	author = {Gert {De Cooman} and Enrique Miranda},
	booktitle = {ISIPTA'11: Proceedings of the Seventh International Symposium on Imprecise Probability: Theories and Applications},
	editor = {Frank Coolen and Gert {De Cooman} and Thomas Fetz and Michael Oberguggenberger},
	pages = {169–178},
	publisher = {SIPTA},
	title = {Independent natural extension for sets of desirable gambles},
	year = {2011}
}

@article{Wagner-1997-old+new-III,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Philosophy of Science},
	pages = {165–175},
	title = {Old Evidence and New Explanation},
	volume = {68},
	year = {2001}
}

@misc{Fukuda-polyfaq,
	author = {Komei Fukuda},
	title = {Frequently Asked Questions in Polyhedral Computation},
	url = {http://www.ifor.math.ethz.ch/~fukuda/polyfaq/polyfaq.html},
	year = {2004}
}

@book{Jeffreys-1983,
	author = {Harold Jeffreys},
	edition = {corrected},
	publisher = {Oxford University Press},
	title = {Theory of Probability},
	year = {1983}
}

@article{Daniell-1918,
	author = {P. J. Daniell},
	journal = {The Annals of Mathematics},
	localfile = {article/Daniell-1918.pdf},
	number = {4},
	pages = {279–294},
	title = {A general form of integral},
	url = {http://www.jstor.org/stable/1967495},
	volume = {19},
	year = {1918}
}

@book{Hartfiel-1998-book,
	author = {Darald J. Hartfiel},
	number = {1695},
	publisher = {Springer},
	series = {Lecture Notes in Mathematics},
	title = {Markov Set-Chains},
	url = {http://books.google.com/books?id=79wZAQAAIAAJ},
	year = {1998}
}

@book{Laplace-1920-theorie,
	author = {Pierre-Simon Laplace},
	edition = {3},
	publisher = {Gauthier-Villars},
	series = {Oeuvres complètes de Laplace},
	title = {Théorie analytique des probabilités},
	volume = {7},
	year = {1820}
}

@incollection{DeCooman-Miranda-2007-symmetry,
	address = {London},
	annote = {in svn-repo},
	author = {Gert {De Cooman} and Enrique Miranda},
	booktitle = {Probability and Inference: Essays in Honor of Henry E. Kyburg, Jr.},
	editor = {William Harper and Gregory Wheeler},
	pages = {67–149},
	publisher = {King's College Publications},
	title = {Symmetry of models versus models of symmetry},
	year = {2007}
}

@article{Seidenfeld-Schervish-Kadane-1995-preference,
	author = {Teddy Seidenfeld and Mark J. Schervish and Joseph B. Kadane},
	doi = {10.1214/aos},
	journal = {The Annals of Statistics},
	keywords = {Axioms of decision theory; partial order; robust},
	localfile = {article/Seidenfeld-Schervish-Kadane-1995-preference.pdf},
	month = dec,
	number = {6},
	pages = {2168–2217},
	publisher = {Institute of Mathematical Statistics},
	title = {A representation of partially ordered preferences},
	volume = {23},
	year = {1995}
}

@article{Kaplan-etal-2011,
	abstract = {During early stages of development, regulatory proteins bind
DNA and control the expression of nearby genes, thereby driving spatial and
temporal patterns of gene expression during development. But the biochemical
forces that determine where these regulatory proteins bind are poorly
understood. We gathered experimental data on the activities of several key
regulators of early development of the fruit fly (Drosophila melanogaster) and
developed a computational method to predict where and how strongly they will
bind. We find that competition, cooperativity, and other interactions among
individual regulatory proteins have a limited effect on their binding, while the
global accessibility of DNA to protein binding has a significant impact on the
binding of all factors. Our results suggest a practical method for predicting
regulatory binding by combining experimental DNA accessibility assays with
computational algorithms to determine where will binding occur among the
accessible regions of the genome.},
	author = {Tommy Kaplan and Li Xiao-Yong and Peter J. Sabo and Sean Thomas and John A Stamatoyannopoulos and Mark D. Biggin and Michael B. Eisen},
	doi = {10.1371/journal.pgen.1001290},
	journal = {PLoS Genetics},
	localfile = {article/Kaplan-etal-2011.pdf},
	number = {2},
	pages = {e1001290},
	publisher = {Public Library of Science},
	title = {Quantitative Models of the Mechanisms That Control Genome-Wide Patterns of Transcription Factor Binding during Early Drosophila Development},
	volume = {7},
	year = {2011}
}

@article{Efron-1978-expfam,
	abstract = {There are two important spaces connected with every
multivariate exponential family, the natural parameter space and the expectation
parameter space. We describe some geometric results relating the two. (In the
simplest case, that of a normal translation family, the two spaces coincide and
the geometry is the familiar Euclidean one.) Maximum likelihood estimation,
within one-parameter curved subfamilies of the multivariate family, has two
simple and useful geometric interpretations. The geometry also relates to the
Fisherian question: to what extent can the Fisher information be replaced by
-\partial^2/\partial$\theta$^2\lbrack\log
f\_$\theta$(x)\rbrack\mid\_{$\theta$=\hat{$\theta$}} in the variance bound for
\hat{$\theta$}, the maximum likelihood estimator?},
	author = {Bradley Efron},
	doi = {10.1214/aos},
	issn = {0090-5364},
	journal = {The Annals of Statistics},
	keywords = {Curvature; Kullback-Leibler distance; duality; maximum likelihood estimation},
	localfile = {article/Efron-1978-expfam.pdf},
	month = mar,
	number = {2},
	pages = {362–376},
	publisher = {Institute of Mathematical Statistics},
	title = {The geometry of exponential families},
	volume = {6},
	year = {1978}
}

@article{Consonni-Veronese-2001,
	abstract = {Consider a standard conjugate family of prior distributions
for a vector-parameter indexing an exponential family. Two distinct model
parameterizations may well lead to standard conjugate families which are not
consistent, i.e. one family cannot be derived from the other by the usual
change-of-variable technique. This raises the problem of finding suitable
parameterizations that may lead to enriched conjugate families which are more
flexible than the traditional ones. The previous remark motivates the definition
of a new property for an exponential family, named conditional reducibility.
Features of conditionally-reducible natural exponential families are
investigated thoroughly. In particular, we relate this new property to the
notion of cut, and show that conditionally-reducible families admit a
reparameterization in terms of a vector having likelihood-independent
components. A general methodology to obtain enriched conjugate distributions for
conditionally-reducible families is described in detail, generalizing previous
works and more recent contributions in the area. The theory is illustrated with
reference to natural exponential families having simple quadratic variance
function.},
	author = {Guido Consonni and Piero Veronese},
	doi = {10.1111/1467-9469.00243},
	journal = {Scandinavian Journal of Statistics},
	keywords = {Bayesian inference; conditional reducibility; cut; enriched prior; exponential family; simple quadratic variance function},
	localfile = {article/Consonni-Veronese-2001.pdf},
	number = {2},
	pages = {377–406},
	title = {Conditionally Reducible Natural Exponential Families and Enriched Conjugate Priors},
	volume = {28},
	year = {2001}
}

@article{Lodwick-Bachman-2005,
	abstract = {Fuzzy and possibilistic optimization methods are
demonstrated to be effective tools in solving large-scale problems. In
particular, an optimization problem in radiation therapy with various orders of
complexity from 1000 to 62,250 constraints for fuzzy and possibilistic linear
and nonlinear programming implementations possessing (1) fuzzy or soft
inequalities, (2) fuzzy right-hand side values, and (3) possibilistic right-hand
side is used to demonstrate that fuzzy and possibilistic optimization methods
are tractable and useful. We focus on the uncertainty in the right side of
constraints which arises, in the context of the radiation therapy problem, from
the fact that minimal and maximal radiation tolerances are ranges of values,
with preferences within the range whose values are based on research results,
empirical findings, and expert knowledge, rather than fixed real numbers. The
results indicate that fuzzy/possibilistic optimization is a natural and
effective way to model various types of optimization under uncertainty problems
and that large fuzzy and possibilistic optimization problems can be solved
efficiently.},
	author = {Weldon A. Lodwick and Katherine Bachman},
	doi = {10.1007/s10700-005-3663-4},
	issn = {1568-4539},
	journal = {Fuzzy Optimization and Decision Making},
	localfile = {article/Lodwick-Bachman-2005.pdf},
	pages = {257–278},
	publisher = {Springer Netherlands},
	title = {Solving Large-Scale Fuzzy and Possibilistic Optimization Problems},
	volume = {4},
	year = {2005}
}

@article{Goldberg-1991-float,
	abstract = {Floating-point arithmetic is considered as esoteric subject
by many people. This is rather surprising, because floating-point is ubiquitous
in computer systems: Almost every language has a floating-point datatype;
computers from PCs to supercomputers have floating-point accelerators; most
compilers will be called upon to compile floating-point algorithms from time to
time; and virtually every operating system must respond to floating-point
exceptions such as overflow. This paper presents a tutorial on the aspects of
floating-point that have a direct impact on designers of computer systems. It
begins with background on floating-point representation and rounding error,
continues with a discussion of the IEEE floating point standard, and concludes
with examples of how computer system builders can better support floating
point.},
	author = {David Goldberg},
	doi = {10.1145/103162.103163},
	issn = {0360-0300},
	journal = {ACM Computing Surveys},
	localfile = {article/Goldberg-1991-float.pdf},
	month = mar,
	number = {1},
	pages = {5–48},
	publisher = {ACM},
	title = {What every computer scientist should know about floating-point arithmetic},
	url = {http://portal.acm.org/citation.cfm?doid=103162.103163},
	volume = {23},
	year = {1991}
}

@article{Davis-1954,
	author = {Chandler Davis},
	issn = {0002-9327},
	journal = {American Journal of Mathematics},
	localfile = {article/Davis-1954.pdf},
	number = {4},
	pages = {733–746},
	publisher = {The Johns Hopkins University Press},
	title = {Theory of positive linear dependence},
	url = {http://www.jstor.org/stable/2372648},
	volume = {76},
	year = {1954}
}

@techreport{Hutter-2006,
	address = {Manno-Lugano, Switzerland},
	author = {Marcus Hutter},
	institution = {IDSIA},
	number = {IDSIA-03-06},
	title = {On the Foundations of Universal Sequence Prediction},
	year = {2006}
}

@article{Wagner-2002-probkin+comm,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Philosophy of Science},
	pages = {266–278},
	title = {Probability Kinematics and Commutativity},
	volume = {69},
	year = {2002}
}

@proceedings{FMMES-1974,
	address = {Warsaw},
	booktitle = {Formal methods in the methodology of empirical sciences},
	editor = {Marian Przełęcki and Klemens Szaniawski},
	publisher = {D. Reidel Publishing Company, Dordrecht, Holland / Boston, U.S.A; Ossolineum Publishing company, Wrocław, Poland},
	title = {Proceedings of the conference for formal methods in the methodology of empirical sciences},
	year = {1974}
}

@article{Grabisch-1995,
	author = {Michel Grabisch},
	journal = {IEEE Transactions on Fuzzy Systems},
	localfile = {article/Grabisch-1995.pdf},
	number = {1},
	pages = {96–109},
	title = {On Equivalence Classes of Fuzzy Connectives—The Case of Fuzzy Integrals},
	volume = {3},
	year = {1995}
}

@article{Hartfiel-Seneta-1994,
	abstract = {In the theory of homogeneous Markov chains, states are
classified according to their connectivity to other states and this
classification leads to a classification of the Markov chains themselves. In
this paper we classify Markov set-chains analogously, particularly into ergodic,
regular, and absorbing Markov set-chains. A weak law of large numbers is
developed for regular Markov set-chains. Examples are used to illustrate
analysis of behavior of Markov set-chains.},
	author = {Darald J. Hartfiel and E. Seneta},
	issn = {0001-8678},
	journal = {Advances in Applied Probability},
	localfile = {article/Hartfiel-Seneta-1994.pdf},
	number = {4},
	pages = {947–964},
	publisher = {Applied Probability Trust},
	title = {On the theory of Markov set-chains},
	url = {http://www.jstor.org/stable/1427899},
	volume = {26},
	year = {1994}
}

@techreport{Griffiths-Ghahramani-2005,
	author = {Thomas L. Griffiths and Zoubin Ghahramani},
	institution = {Gatsby Unit, University College London},
	number = {GCNU TR 2005-001},
	title = {Infinite Latent Feature Models and the Indian Buffet Process},
	year = {2005}
}

@article{Destercke-etal-2008-unifying1,
	author = {Sébastien Destercke and Didier Dubois and E. Chojnacki},
	doi = {10.1016/j.ijar.2008.07.003},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Destercke-etal-2008-unifying1.pdf},
	pages = {649–663},
	title = {Unifying practical uncertainty representations: I. Generalized p-boxes},
	volume = {49},
	year = {2008}
}

@incollection{Henk-RichterGebert-Ziegler-1997,
	author = {Martin Henk and Jürgen Richter-Gebert and Günter M. Ziegler},
	booktitle = {Handbook of Discrete and Computational Geometry},
	editor = {J. E. Goodman and J. O'Rourke},
	pages = {243–270},
	publisher = {CRC Press},
	title = {Basic properties of convex polytopes},
	url = {http://fma2.math.uni-magdeburg.de/~henk/preprints/henk; richter-gebert ziegler&basic properties of convex polytopes.pdf},
	year = {1997}
}

@article{Levi-1974,
	author = {Isaac Levi},
	journal = {The Journal of Philosophy},
	localfile = {article/Levi-1974.pdf},
	number = {13},
	pages = {391–418},
	title = {On indeterminate probabilities},
	url = {http://www.jstor.org/stable/2025161},
	volume = {71},
	year = {1974}
}

@article{Huang-Huang-Tsai-2006-Hilbert-metric,
	abstract = {The purpose of this paper is to study the eigenvalue
problems for a class of positive nonlinear operators. Using projective metric
techniques and the contraction mapping principle, we establish existence,
uniqueness and continuity results for positive eigensolutions of a particular
type of positive nonlinear operator. In addition, we prove the existence of a
unique fixed point of the operator with explicit norm-estimates. Applications to
nonlinear systems of equations and to matrix equations are considered.},
	author = {Min-Jei Huang and Chao-Ya Huang and Tzong-Mo Tsai},
	doi = {10.1016/j.laa.2005.08.024},
	journal = {Linear Algebra and its Applications},
	keywords = {Concave operator; Cone; Eigenvalue problem; Hilbert’s projective metric; Matrix equation; Nonlinear system of equations},
	localfile = {article/Huang-Huang-Tsai-2006-Hilbert-metric.pdf},
	number = {1},
	pages = {202–211},
	title = {Applications of Hilbert's projective metric to a class of positive nonlinear operators},
	volume = {413},
	year = {2006}
}

@book{Koller-Friedman-2009,
	author = {Daphne Koller and Nir Friedman},
	isbn = {978-0-262-01319-2},
	publisher = {MIT Press},
	series = {Adaptive Computation and Machine Learning},
	title = {Probabilistic Graphical Models},
	year = {2009}
}

@mastersthesis{Dhaenens-2007,
	author = {Stefaan Dhaenens},
	localfile = {mastersthesis/Dhaenens-2007.pdf},
	school = {Universiteit Gent},
	title = {Onderzoek van Imprecieze Markov-modellen},
	url = {http://hdl.handle.net/1854/LU-470245},
	year = {2007}
}

@article{Nau-2006-shape,
	abstract = {Incomplete preferences provide the epistemic foundation for
models of imprecise subjective probabilities and utilities that are used in
robust Bayesian analysis and in theories of bounded rationality. This paper
presents a simple axiomatization of incomplete preferences and characterizes the
shape of their representing sets of probabilities and utilities. Deletion of the
completeness assumption from the axiom system of Anscombe and Aumann yields
preferences represented by a convex set of state-dependent expected utilities,
of which at least one must be a probability/utility pair. A strengthening of the
state-independence axiom is needed to obtain a representation purely in terms of
a set of probability/utility pairs.},
	author = {Robert F. Nau},
	doi = {10.1214/009053606000000740},
	journal = {The Annals of Statistics},
	keywords = {Axioms of decision theory; Bayesian robustness; coherence; imprecise probabilities and utilities; partial order; state-dependent utility},
	localfile = {article/Nau-2006-shape.pdf},
	number = {5},
	pages = {2430–2448},
	title = {The shape of incomplete preferences},
	volume = {34},
	year = {2006}
}

@misc{Doumont-2001-internet,
	author = {Jean-luc Doumont},
	title = {How Internet works},
	year = {2001}
}

@article{Miranda-DeCooman-Quaeghebeur-2007-finitely,
	abstract = {We study the information that a distribution function
provides about the finitely additive probability measure inducing it. We show
that in general there is an infinite number of finitely additive probabilities
associated with the same distribution function. Secondly, we investigate the
relationship between a distribution function and its given sequence of moments.
We provide formulae for the sets of distribution functions, and finitely
additive probabilities, associated with some moment sequence, and determine
under which conditions the moments determine the distribution function uniquely.
We show that all these problems can be addressed efficiently using the theory of
coherent lower previsions.},
	author = {Enrique Miranda and Gert {De Cooman} and Erik Quaeghebeur},
	doi = {10.1016/j.ijar.2007.07.007},
	journal = {International Journal of Approximate Reasoning},
	localfile = {article/Miranda-DeCooman-Quaeghebeur-2007-finitely.pdf},
	number = {1},
	pages = {132–155},
	title = {Finitely additive extensions of distribution functions and moment sequences: The coherent lower prevision approach},
	volume = {48},
	year = {2008}
}

@inproceedings{Fukuda-Prodon-1996-cdd,
	author = {Komei Fukuda and Alain Prodon},
	booktitle = {Combinatorics and Computer Science},
	doi = {10.1007/3-540-61576-8_77},
	editor = {M. Deza and R. Euler and I. Manoussakis},
	pages = {91–111},
	publisher = {Springer-Verlag},
	series = {Lecture Notes in Computer Science},
	title = {Double Description Method Revisited},
	url = {http://www.ifor.math.ethz.ch/~fukuda/cdd_home},
	volume = {1120},
	year = {1996}
}

@book{Pearl-1988,
	address = {San Francisco, California},
	author = {Judea Pearl},
	publisher = {Morgan Kaufmann},
	title = {Probabilistic Reasoning in Intelligent Systems},
	year = {1988}
}

@article{Mraz-1998,
	abstract = {The paper deals with computing the exact upper and lower
bounds of optimal values forlinear programming problems whose coefficients vary
in given intervals. The theoreticalbackground for calculating these bounds is
described and corresponding algorithms aregiven. A comparison with other
approaches, some applications and a software package arementioned.},
	author = {František Mráz},
	doi = {10.1023/A:1018985914065},
	issn = {0254-5330},
	journal = {Annals of Operations Research},
	keywords = {inexact data; interval coefficients; linear programming problem},
	localfile = {article/Mraz-1998.pdf},
	pages = {51–62},
	publisher = {Springer},
	title = {Calculating the exact bounds of optimal valuesin LP with interval coefficients},
	volume = {81},
	year = {1998}
}

@article{Aach-Church-2001,
	annote = {ook op papier},
	author = {John Aach and George M. Church},
	doi = {10.1093/bioinformatics},
	journal = {Bioinformatics},
	localfile = {article/Aach-Church-2001.pdf},
	pages = {495–508},
	title = {Aligning gene expression time series with time warping algorithms},
	volume = {17},
	year = {2001}
}

@article{DeFinetti-1967,
	author = {Bruno de Finetti},
	journal = {Revue Roumaine des Mathémathiques Pures et Appliquées},
	pages = {1227–1233},
	title = {Quelques conventions qui semblent utiles},
	volume = {12},
	year = {1967}
}

@techreport{Kaymak-Sousa-2001,
	abstract = {Many practical optimization problems are characterized by
some flexibility in the problem constraints, where this flexibility can be
exploited for additional trade-off between improving the objective function and
satisfying the constraints. Especially in decision making, this type of
flexibility could lead to workable solutions, where the goals and the
constraints specified by different parties involved in the decision making are
traded off against one another and satisfied to various degrees. Fuzzy sets have
proven to be a suitable representation for modeling this type of soft
constraints. Conventionally, the fuzzy optimization problem in such a setting is
defined as the simultaneous satisfaction of the constraints and the goals. No
additional distinction is assumed to exist amongst the constraints and the
goals. This report proposes an extension of this model for satisfying the
problem constraints and the goals, where preference for different constraints
and goals can be specified by the decision-maker. The difference in the
preference for the constraints is represented by a set of associated weight
factors, which influence the nature of trade-off between improving the
optimization objectives and satisfying various constraints. Simultaneous
weighted satisfaction of various criteria is modeled by using the recently
proposed weighted extensions of Archimedean) fuzzy t-norms. The weighted
satisfaction of the problem constraints and goals are demonstrated by using a
simple general, and it can also be applied to fuzzy mathematical programming
problems and multi-objective fuzzy optimization.},
	author = {U. Kaymak and J. M. Sousa},
	institution = {Erasmus Universiteit Amsterdam},
	number = {ERS-2001-19-LIS},
	title = {Weighted constraints in fuzzy optimization},
	url = {http://repub.eur.nl/res/pub/85},
	year = {2001}
}

@book{Dunford-Schwartz-1958,
	address = {New York},
	author = {Nelson Dunford and Jacob T. Schwartz},
	publisher = {Interscience Publishers},
	series = {Pure and Applied Mathematics},
	title = {Linear Operators Part I},
	volume = {VII},
	year = {1958}
}

@article{DeCooman-Troffaes-2004,
	abstract = {We discuss why coherent lower previsions provide a good
uncertainty model for solving generic uncertainty problems involving possibly
conflicting expert information. We study various ways of combining expert
assessments on different domains, such as natural extension, independent natural
extension and the type-I product, as well as on common domains, such as
conjunction and disjunction. We provide each of these with a clear
interpretation, and we study how they are related. Observing that in combining
expert assessments no information is available about the order in which they
should be combined, we suggest that the final result should be independent of
the order of combination. The rules of combination we study here satisfy this
requirement.},
	annote = {reprint},
	author = {Gert {De Cooman} and Matthias C. M. Troffaes},
	doi = {10.1016/j.ress.2004.03.007},
	journal = {Reliability Engineering \& System Safety},
	keywords = {coherent lower previsions; conjunction; disjunction; expert information; independence; marginal extension; natural extension; type-1 product},
	localfile = {article/DeCooman-Troffaes-2004.pdf},
	number = {1},
	pages = {113–134},
	publisher = {Elsevier},
	title = {Coherent lower previsions in systems modelling: products and aggregation rules},
	volume = {85},
	year = {2004}
}

@book{Menezes-VanOorschot-Vanstone-1996,
	author = {Alfred J. Menezes and Paul C. {Van Oorschot} and Scott A. Vanstone},
	publisher = {CRC Press},
	title = {Handbook of Applied Cryptography},
	year = {1996}
}

@article{Choquet-1954,
	abstract = {C'est un essai de théorie générale des fonctions croissantes
d'ensemble. On est amené à mettre en évidence diverses classes importantes de
telles fonctions, en particulier la classe des fonctions fortement
sous-additives, pour lesquelles existe une théorie analogue à celle de la
mesure, et une sous-classe de celle-ci, à savoir la classe des fonctions
alternées d'ordre infini, analogues aux fonctions numériques complètement
monotones. La capacité classique est une telle fonction d'ensemble ; il en
résulte l'identité des capacités intérieure et extérieure de tout ensemble
borélien ou analytique. Un outil de recherche puissant est la représentation
intégrale des fonctions d'une classe additive et convexe au moyen des éléments
extrémaux d'une telle classe. Cette représentation permet d'identifier les
fonctions alternées d'ordre infini avec certaines probabilités associées à
l'ensemble variable.},
	author = {Gustave Choquet},
	doi = {10.5802/aif.53},
	journal = {Annales de l'Institut Fourier},
	localfile = {article/Choquet-1954.pdf},
	pages = {131–295},
	title = {Theory of capacities},
	url = {http://www.numdam.org/item?id=AIF_1954__5__131_0},
	volume = {5},
	year = {1954}
}

@inproceedings{Miranda-DeCooman-Quaeghebeur-2006-IPMU,
	address = {Paris},
	author = {Enrique Miranda and Gert {De Cooman} and Erik Quaeghebeur},
	booktitle = {Proceedings of the Eleventh International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
	pages = {89–96},
	title = {The moment problem for finitely additive probabilities},
	volume = {1},
	year = {2006}
}

@article{Wagner-1997-old+new-II,
	annote = {op papier in Wagnerbundel},
	author = {Carl G. Wagner},
	journal = {Philosophy of Science},
	pages = {283–288},
	title = {Old Evidence and New Explanation},
	volume = {66},
	year = {1999}
}

@article{Hartman-Watson-1974,
	author = {Philip Hartman and Geoffrey S. Watson},
	doi = {10.1214/aop},
	journal = {The Annals of Probability},
	localfile = {article/Hartman-Watson-1974.pdf},
	month = aug,
	number = {4},
	pages = {593–607},
	title = {“Normal” distribution functions on spheres and the modified Bessel functions},
	volume = {2},
	year = {1974}
}

@article{Grunbaum-Shepard-1969,
	author = {Branko Grünbaum and G. C. Shepard},
	doi = {10.1112/blms},
	journal = {Bulletin of the London Mathematical Society},
	localfile = {article/Grunbaum-Shepard-1969.pdf},
	pages = {257–300},
	publisher = {Springer Verlag},
	title = {Convex polytopes},
	volume = {1},
	year = {1969}
}

@misc{Quaeghebeur-murasyp,
	author = {Erik Quaeghebeur},
	title = {murasyp: Python software for accept/reject statement-based uncertainty modeling},
	url = {http://equaeghe.github.com/murasyp},
	year = {in progress}
}

@inproceedings{Schrage-IJzendoorn-VanderGaag-2005,
	author = {Martijn M. Schrage and Arjan van IJzendoorn and Linda C. van der Gaag},
	booktitle = {Haskell '05: Proceedings of the 2005 ACM SIGPLAN Workshop on Haskell},
	doi = {10.1145/1088348.1088351},
	isbn = {1-59593-071-X},
	location = {Tallinn, Estonia},
	month = sep,
	pages = {17–26},
	publisher = {ACM Press},
	title = {Haskell ready to Dazzle the real world},
	url = {http://www.cs.uu.nl/dazzle},
	year = {2005}
}

@article{Quaeghebeur-DeCooman-Hermans-2012,
	author = {Erik Quaeghebeur and Gert {De Cooman} and Filip Hermans},
	note = {Journal paper},
	title = {Accept \& Reject Statement-Based Uncertainty Models},
	year = {in preparation}
}

@inproceedings{VanderGaag-etal-2010-BNAIC,
	author = {Linda C. van der Gaag and Silja Renooij and Hermi J. M. Schijf and A. R. Elbers and W. L. Loeffen},
	booktitle = {BNAIC 2010: Proceedings of the 22nd Benelux Conference on Artificial Intelligence},
	location = {Luxembourg},
	title = {Probability assessments from multiple experts: qualitative information is more robust},
	year = {2010}
}

@article{Coupe-VanderGaag-2002,
	abstract = {The assessments for the various conditional probabilities of
a Bayesian belief network inevitably are inaccurate, influencing the reliability
of its output. By subjecting the network to a sensitivity analysis with respect
to its conditional probabilities, the reliability of its output can be
investigated. Unfortunately, straightforward sensitivity analysis of a belief
network is highly time-consuming. In this paper, we show that by qualitative
considerations several analyses can be identified as being uninformative as the
conditional probabilities under study cannot affect the output. In addition, we
show that the analyses that are informative comply with simple mathematical
functions. More specifically, we show that a belief network's output can be
expressed as a quotient of two functions that are linear in a conditional
probability under study. These properties allow for considerably reducing the
computational burden of sensitivity analysis of Bayesian belief networks.},
	author = {Veerle M. H. Coupé and Linda C. van der Gaag},
	doi = {10.1023/A:1016398407857},
	issn = {1012-2443},
	journal = {Annals of Mathematics and Artificial Intelligence},
	number = {4},
	pages = {323–356},
	publisher = {Springer Netherlands},
	title = {Properties of sensitivity analysis of Bayesian belief networks},
	volume = {36},
	year = {2002}
}

@inproceedings{VanderGaag-Renooij-2004,
	author = {Linda C. van der Gaag and Silja Renooij},
	booktitle = {IPMU : Proceedings of the Tenth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems},
	location = {Perugia},
	pages = {1675–1682},
	title = {On the sensitivity of probabilistic networks to test reliability},
	year = {2004}
}

@inproceedings{Renooij-VanderGaag-2004-UAI,
	author = {Silja Renooij and Linda C. van der Gaag},
	booktitle = {UAI-04: Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence},
	isbn = {0-9749039-0-6},
	location = {Banff, Canada},
	pages = {479–486},
	publisher = {AUAI Press},
	title = {Evidence-invariant sensitivity bounds},
	year = {2004}
}

@inproceedings{Kjaerulff-VanderGaag-2000,
	author = {Uffe Kjærulff and Linda C. van der Gaag},
	booktitle = {UAI-00: Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence},
	editor = {Craig Boutilier and Moisés Goldszmidt},
	isbn = {1-55860-709-9},
	pages = {317–325},
	publisher = {Morgan Kaufmann},
	title = {Making Sensitivity Analysis Computationally Efficient.},
	year = {2000}
}

@inproceedings{VanderGaag-Renooij-2001,
	author = {Linda C. van der Gaag and Silja Renooij},
	booktitle = {UAI-01: Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
	editor = {Jack S. Breese and Daphne Koller},
	isbn = {1-55860-800-1},
	pages = {530–537},
	publisher = {Morgan Kaufmann},
	title = {Analysing sensitivity data from probabilistic networks},
	year = 2001
}

@incollection{deFinetti-1937-foresight,
	author = {Bruno de Finetti},
	booktitle = {Studies in Subjective Probability},
	editor = {Henry E Kyburg and Smokler},
	localfile = {inbook/deFinetti-1937-foresight.pdf},
	pages = {93–158},
	publisher = {Wiley},
	title = {Foresight: Its Logical Laws, Its Subjective Sources},
	year = {1964}
}

@article{DeCampos-etal-2009,
	abstract = {This paper explores the application of semi-qualitative
probabilistic networks (SQPNs) that combine numeric and qualitative information
to computer vision problems. Our version of SQPN allows qualitative influences
and imprecise probability measures using intervals. We describe an Imprecise
Dirichlet model for parameter learning and an iterative algorithm for evaluating
posterior probabilities, maximum a posteriori and most probable explanations.
Experiments on facial expression recognition and image segmentation problems are
performed using real data.},
	annote = {doi: 10.1080/15598608.2009.10411920},
	author = {Cassio Polpo de Campos and Lei Zhang and Yan Tong and Qiang Ji},
	doi = {10.1080/15598608.2009.10411920},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Computer vision applications; Imprecise probabilities; Probabilistic networks; Qualitative relations},
	localfile = {article/DeCampos-etal-2009.pdf},
	month = mar,
	number = {1},
	pages = {197–210},
	publisher = {Taylor \& Francis},
	title = {Semi-Qualitative Probabilistic Networks in Computer Vision Problems},
	volume = {3},
	year = {2009}
}

@article{DeFinetti-Jacob-1935,
	author = {Bruno de Finetti and M. Jacob},
	journal = {Giornale dell'Istituto italiano degli attuari},
	localfile = {article/DeFinetti-Jacob-1935.pdf},
	pages = {303–319},
	title = {Sull'integrale di Stieltjes-Riemann},
	volume = {6},
	year = {1935}
}

@article{Hampel2009,
	abstract = {The paper gives a short survey about the occurrence
(sometimes hidden in the background) of nonadditive probabilities in statistics.
It starts with the original meaning of ?probability? in statistics in the Ars
Conjectandi by Jakob (James) Bernoulli, and the ensuing misunderstanding which
gave the term its present meaning. One chapter is about robustness theory, its
use of (nonadditive) Choquet-capacities, and an attempt to clarify some
widespread misunderstandings about it, which have consequences for the use of
upper and lower probabilities. Also the uncertainty about model choice
(including the conflict between purely mathematical reasoning and good
statistical practice) and treatment of outliers is briefly discussed. The
partial arbitrariness of additivity both in Bayes' famous Scholium and in modern
Bayes theory is outlined. The infamous and almost forgotten fiducial
probabilities can actually be corrected and find their place in a more general
paradigm using upper and lower probabilities. Finally, a new (?) qualitative
theory of inference is mentioned which may contain some essentials of inductive
reasoning in real life.},
	author = {Frank Hampel},
	doi = {10.1080/15598608.2009.10411908},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Ars Conjectandi; Bayes’ Scholium; Inaccuracy and uncertainty; Misleading logic in data analysis; Misunder- standings about robustness theory; Nonadditive probabilites; Original meaning of probability; Proper fiducial probabilities; Qualitative reasoning in real life.},
	localfile = {article/Hampel-2009.pdf},
	month = mar,
	number = {1},
	pages = {11–23},
	publisher = {Taylor \& Francis},
	title = {Nonadditive Probabilities in Statistics},
	volume = {3},
	year = {2009}
}

@article{Kozine-Krymsky-2009,
	abstract = {This paper describes how one can compute interval-valued
statistical measures given limited information about the underlying
distribution. The particular focus is on a bounded derivative of a probability
density function and its combination with other available statistical evidence
for computing quantities of interest. To be able to utilise the evidence about
the derivative it is suggested to adapt the ?conventional? problem statement to
variational calculus and the way to do so is demonstrated. A number of examples
are given throughout the paper.},
	author = {Igor O. Kozine and Victor Krymsky},
	doi = {10.1080/15598608.2009.10411909},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bounded derivative; Bounded probability distribution; Interval- valued measures; Natural extension; Variational calculus},
	localfile = {article/Kozine-Krymsky-2009.pdf},
	month = mar,
	number = {1},
	pages = {25–38},
	publisher = {Taylor \& Francis},
	title = {Bounded Densities and Their Derivatives: Extension to other Domains},
	volume = {3},
	year = {2009}
}

@article{Stoye-2009,
	abstract = {This paper applies recently developed methods to robust
assessment of treatment outcomes and robust treatment choice based on
nonexperimental data. The substantive question is whether young offenders should
be assigned to residential or nonresidential treatment in order to prevent
subsequent recidivism. A large data set on past offenders exists, but treatment
assignment was by judges and not by experimenters, hence counterfactual outcomes
are not identified unless one imposes strong assumptions. The analysis is
carried out in two steps. First, I show how to compute identified bounds on
expected outcomes under various assumptions that are too weak to restore
conventional identification but may be accordingly credible. The bounds are
estimated, and confidence regions that take current theoretical developments
into account are computed. I then ask which treatment to assign to future
offenders if the identity of the best treatment will not be learned from the
data. This is a decision problem under ambiguity. I characterize and compute
decision rules that are asymptotically efficient under the minimax regret
criterion. The substantive conclusion is that both bounds and recommended
decisions vary significantly across the assumptions. The data alone do not
permit conclusions or decisions that are globally robust in the sense of holding
uniformly over reasonable assumptions.},
	author = {Jörg Stoye},
	doi = {10.1080/15598608.2009.10411923},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bounds; Minimax regret; Partial identification; Statistical decision rules; Treatment choice; Treatment evaluation},
	localfile = {article/Stoye-2009.pdf},
	month = mar,
	number = {1},
	pages = {239–254},
	publisher = {Taylor \& Francis},
	title = {Partial Identification and Robust Treatment Choice: An Application to Young Offenders},
	volume = {3},
	year = {2009}
}

@article{CoolenSchrijner-etal-2009,
	abstract = {More and more often the traditional (classical) concept of
probability, and the statistical methods based on it, have been criticized for
being unable to cope with the multidimensional nature of uncertainty. Careful
handling of imprecision is essential to draw reliable conclusions from complex
data. This paper presents a short introductory discussion on the general area of
imprecision in statistical theory and practice, and briefly introduces the
further papers in this collection, demonstrating the importance of the adequate
modelling of imprecision in different areas of application.},
	author = {Pauline Coolen-Schrijner and Frank P. A. Coolen and Matthias C. M. Troffaes and Thomas Augustin},
	doi = {10.1080/15598608.2009.10411907},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Decision making; Elicition; Inference; Lower and upper probabilities; Robustness; Theory},
	localfile = {article/CoolenSchrijner-etal-2009.pdf},
	month = mar,
	number = {1},
	pages = {1–9},
	publisher = {Taylor \& Francis},
	title = {Imprecision in Statistical Theory and Practice},
	volume = {3},
	year = {2009}
}

@article{Pelessoni-Vicig-2009,
	abstract = {In this paper we consider some bounds for lower previsions
that are either coherent or, more generally, centered convex. We focus on bounds
concerning the classical product and Bayes' rules, discussing first weak product
rules and some of their implications for coherent lower previsions. We then
generalise a well-known lower bound, which is a (weak) version for events and
coherent lower probabilities of Bayes' theorem, to the case of random variables
and (centered) convex previsions. We obtain a family of bounds and show that one
of them is undominated in all cases. Some applications are outlined, and it is
shown that 2-monotonicity, which ensures that the bound is sharp in the case of
events, plays a much more limited role in this general framework.},
	author = {Renato Pelessoni and Paolo Vicig},
	doi = {10.1080/15598608.2009.10411913},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {2-monotonicity; Bayes’ theorem; Centered convex previsions; Conditional lower previsions; Product rule},
	localfile = {article/Pelessoni-Vicig-2009.pdf},
	month = mar,
	number = {1},
	pages = {85–101},
	publisher = {Taylor \& Francis},
	title = {Bayes' Theorem Bounds for Convex Lower Previsions},
	volume = {3},
	year = {2009}
}

@article{Bose-2009-imposition,
	abstract = {We consider the problem of imposing shape constraints on a
neighborhood class – the density ratio class (DeRobertis and Hartigan, 1981).
Bose (1994) used mixture distributions to impose shape and smoothness
constraints simultaneously. We discuss how one may impose either or both
unimodality and symmetry without requiring simultaneous imposition of a
smoothness constraint.},
	author = {Sudip Bose},
	doi = {10.1080/15598608.2009.10411910},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bayesian robustness; Convexity; Density bounded; Density ratio; Likelihood; Minimax; Neighborhood class; Posterior regret; Smoothness; Symmetry; Unimodality; $\Gamma$-minimax},
	localfile = {article/Bose-2009-imposition.pdf},
	month = mar,
	number = {1},
	pages = {39–55},
	publisher = {Taylor \& Francis},
	title = {On the imposition of shape constraints in a robust Bayesian analysis},
	volume = {3},
	year = {2009}
}

@article{Wilson-Huzurbazar-Sentz-2009,
	abstract = {In this paper we expand on recent advances in Bayesian
inference for multilevel data in fault trees and Bayesian networks. As a first
example, we compare the Bayesian fault tree and incomplete data approaches to
statistical inference for multilevel data in fault trees. As a second example,
we consider two a priori representations of uncertainty about the parameters of
a Bayesian network: A multinomial-Dirichlet model and an extension of the
imprecise Dirichlet model. We calculate the a posteriori uncertainty after
updating with data using Markov chain Monte Carlo and compare the results.},
	author = {Alyson G. Wilson and Aparna V. Huzurbazar and Kari Sentz},
	doi = {10.1080/15598608.2009.10411921},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bayesian network; Fault tree; Imprecise Dirichlet model; Multilevel data; Multinomial-Dirichlet model; Reliability},
	localfile = {article/Wilson-Huzurbazar-Sentz-2009.pdf},
	month = mar,
	number = {1},
	pages = {211–223},
	publisher = {Taylor \& Francis},
	title = {The Imprecise Dirichlet Model for Multilevel System Reliability},
	volume = {3},
	year = {2009}
}

@article{deFinetti-1981-BJPS,
	author = {Bruno de Finetti},
	doi = {10.1093/bjps},
	journal = {The British Journal for the Philosophy of Science},
	localfile = {article/deFinetti-1981-BJPS.pdf},
	number = {1},
	pages = {55–56},
	publisher = {British Society for the Philosophy of Science},
	title = {The role of `Dutch books' and of `proper scoring rules'},
	url = {http://www.jstor.org/stable/687386},
	volume = {32},
	year = {1981}
}

@article{Fuchs-Neumaier-2009,
	abstract = {Robust design optimization methods applied to real life
problems face some major difficulties: How to deal with the estimation of
probability densities when data are sparse, how to cope with high dimensional
problems and how to use valuable information in the form of unformalized expert
knowledge. In this paper we introduce in detail the clouds formalism as a means
to process available uncertainty information reliably, even if limited in amount
and possibly lacking a formal description. This enables a worst-case analysis
with confidence regions of relevant scenarios which can be involved in an
optimization problem formulation for robust design.},
	author = {Martin Fuchs and Arnold Neumaier},
	doi = {10.1080/15598608.2009.10411922},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Clouds; Confidence regions; Design optimization; Potential clouds; Robust design; Uncertainty modeling},
	localfile = {article/Fuchs-Neumaier-2009.pdf},
	month = mar,
	number = {1},
	pages = {225–238},
	publisher = {Taylor \& Francis},
	title = {Potential Based Clouds in Robust Design Optimization},
	volume = {3},
	year = {2009}
}

@article{Walter-Augustin-2009,
	abstract = {A great advantage of imprecise probability models over
models based on precise, traditional probabilities is the potential to reflect
the amount of knowledge they stand for. Consequently, imprecise probability
models promise to offer a vivid tool for handling situations of prior-data
conflict in (generalized) Bayesian inference. In this paper we consider a
general class of recently studied imprecise probability models, including the
Imprecise Dirichlet Model under prior information, and more generally the
framework of Quaeghebeur and de Cooman for imprecise inference in canonical
exponential families. We demonstrate that such models, in their originally
proposed form, prove to be insensitive to the extent of prior-data conflict. We
propose an extension reestablishing the natural relationship between knowledge
and imprecision: The higher the discrepancy between the observed sample and what
was expected from prior knowledge, the higher the imprecision in the posterior,
producing cautious inferences if, and only if, caution is needed. Our approach
is illustrated by some examples and simulation results.},
	author = {Gero Walter and Thomas Augustin},
	doi = {10.1080/15598608.2009.10411924},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Canonical exponential family; Generalized Bayesian inference; Generalized Bayes’ Rule; Imprecise Dirichlet Model (IDM); Imprecise priors; Prior-data conflict; Posterior imprecision; Robust Bayesian analysis},
	localfile = {article/Walter-Augustin-2009.pdf},
	month = mar,
	number = {1},
	pages = {255–271},
	publisher = {Taylor \& Francis},
	title = {Imprecision and Prior-Data Conflict in Generalized Bayesian Inference},
	volume = {3},
	year = {2009}
}

@article{Danielson-Ekenberg-Riabacke-2009,
	abstract = {Most current decision analytical tools and elicitation
methods are built on the assumption that decision-makers are able to make their
probability and utility assessments in a proper manner. This is, however, often
not the case. The specification and execution of elicitation processes are in
the majority of cases left to the discretion of the users, not least in
user-driven cases such as public information and e-democracy projects. A number
of studies have shown, among other things, that people's natural choice
behaviour deviates from normative assumptions, and that the results display an
inertia gap due to differently framed prospects. One reason for the occurrence
of the inertia gap is people's inability to express their preferences as single
numbers. Instead of considering this as being a human error, this paper uses the
gap in order to develop a class of methods more aligned to the observed
behaviour. The core idea of the class is to acknowledge the existence of the gap
and, as a consequence, not elicit single point numbers.},
	author = {Mats Danielson and Love Ekenberg and Ari Riabacke},
	doi = {10.1080/15598608.2009.10411917},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Decision analysis; Elicitation method; Imprecise information; Interval assessments},
	localfile = {article/Danielson-Ekenberg-Riabacke-2009.pdf},
	month = mar,
	number = {1},
	pages = {157–168},
	publisher = {Taylor \& Francis},
	title = {A Prescriptive Approach to Elicitation of Decision Data},
	volume = {3},
	year = {2009}
}

@article{Strobl-Augustin-2009,
	abstract = {Classification and regression trees are a popular and easy
to interpret non-parametric regression approach, but are known to be very
instable: Small changes in the learning sample can produce completely different
trees. Therefore recently it has become state-of-the-art to consider ensembles
(i.e. sets) of trees. The present paper contributes to the so-called TWIX
approach, which produces ensembles by extra splits in additional cutpoints. This
approach can be considered as a compromise between the interpretable but
instable single tree models and the stable but no longer interpretable ensemble
methods bagging and random forests. Based on the idea to study the sensitivity
of a split to some virtual, yet unseen observations, we develop a new, data
driven, cutpoint selection criterion, that technically turns out to be closely
related to an upper entropy approach based on an Imprecise Dirichlet Model. Our
criterion combines several attractive features: By adding extra cutpoints only
iff the underlying cutpoint is instable, the tree is robustified parsimoniously
and the computational expense of the resulting TWIX ensemble is reduced
considerably. As a welcome by-product we moreover obtain a vivid diagnostic
measure for the robustness of a single tree model. The rationale and benefit of
our new adaptive criterion are illustrated by means of a small data example and
a simulation study. Credal classification rules for robust aggregated
predictions from sets of trees are briefly sketched in an outlook.},
	author = {Carolin Strobl and Thomas Augustin},
	doi = {10.1080/15598608.2009.10411915},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Aggregation; Bagging; C4.5; Classification trees; Credal classification; CART; Cutpoint selection; Imprecise Dirichlet Model; Gini index; Random forests; Shannon entropy; TWIX},
	localfile = {article/Strobl-Augustin-2009.pdf},
	month = mar,
	number = {1},
	pages = {119–135},
	publisher = {Taylor \& Francis},
	title = {Adaptive Selection of Extra Cutpoints—Towards Reconciling Robustness and Interpretability in Classification Trees},
	volume = {3},
	year = {2009}
}

@article{Montgomery-Coolen-Hart-2009,
	abstract = {Refined risk assessments should increase realism compared
with the first tier deterministic risk assessment. This may involve using
probabilistic methods which account separately for uncertainty and variability.
Analysts use cumulative distribution functions to represent variability, and
bounds around these to illustrate uncertainty. In probability bounds analysis,
parametric probability boxes (p-boxes) are usually formed using intervals for
each parameter. In this paper a Bayesian framework is adopted, which takes
account of dependencies between parameters. Bayesian p-boxes use imprecision
represented by bounds to summarise the uncertainty surrounding the risk
distribution parameters.},
	author = {Victoria J. Montgomery and Frank P. A. Coolen and Andy D. M. Hart},
	doi = {10.1080/15598608.2009.10411912},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bayesian methods; Cumulative distribution functions; Highest posterior density regions; Risk assessment; Probability boxes},
	localfile = {article/Montgomery-Coolen-Hart-2009.pdf},
	month = mar,
	number = {1},
	pages = {69–83},
	publisher = {Taylor \& Francis},
	title = {Bayesian Probability Boxes in Risk Assessment},
	volume = {3},
	year = {2009}
}

@article{Farrow-Goldstein-2009,
	abstract = {We develop methods for analysing decision problems based on
multi-attribute utility hierarchies, structured by mutual utility independence,
which are not precisely specified due to unwillingness or inability of an
individual or group to agree on precise values for the trade-offs between the
various attributes. Instead, our analysis is based on whatever limited
collection of preferences we may assert between attribute collections. These
preferences identify a class of Pareto optimal decisions. We show how to reduce
the class further by combining rules which are almost equivalent and introduce
general principles appropriate to selecting decisions in an imprecise hierarchy.
The approach is illustrated by the design of a university course module.},
	author = {M. Farrow and Michael Goldstein},
	doi = {10.1080/15598608.2009.10411916},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Imprecise utilities; Mutual utility independence; Pareto optimality; Robust decisions; Utility hierarchies},
	localfile = {article/Farrow-Goldstein-2009.pdf},
	month = mar,
	number = {1},
	pages = {137–155},
	publisher = {Taylor \& Francis},
	title = {Almost-Pareto Decision Sets in Imprecise Utility Hierarchies},
	volume = {3},
	year = {2009}
}

@article{Bickis-2009,
	abstract = {Given data on inter-arrival times, the imprecise Dirichlet
model can be used to determine upper and lower values on the survival function.
Similar bounds on the hazard function can be quite irregular without some
structural assumptions. To address this problem, a family of prior distributions
for a binomial success probability is contructed by assuming that the logit of
the probability has a normal distribution. Posterior distributions so defined
form a three-dimensional exponential family of which the beta family is a
limiting case. This family is extended to the multivariate case, which provides
for the inclusion of prior information about autocorrelation in the parameters.
By restricting the hyperparameters to a suitably chosen subset, this model is
proposed as an alternative to the usual imprecise Dirichlet model of Walley,
having the advantage of providing smoother estimates of the hazard function. The
methods are applied to data on inter-occurrence times of pandemic influenza.},
	annote = {doi: 10.1080/15598608.2009.10411919},
	author = {Miķelis Bickis},
	doi = {10.1080/15598608.2009.10411919},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Autocorrelation; Hazard function; Imprecise inference},
	localfile = {article/Bickis-2009.pdf},
	month = mar,
	number = {1},
	pages = {183–195},
	publisher = {Taylor \& Francis},
	title = {The Imprecise Logit-Normal Model and its Application to Estimating Hazard Functions},
	volume = {3},
	year = {2009}
}

@incollection{Schempp-1977-Bernstein,
	author = {Walter Schempp},
	booktitle = {Constructive Theory of Functions of Several Variables},
	doi = {10.1007/BFb0086576},
	editor = {Walter Schempp and Karl Zeller},
	localfile = {inbook/Schempp-1977-Bernstein.pdf},
	pages = {212–219},
	publisher = {Springer},
	series = {Lecture Notes in Mathematics},
	title = {Bernstein Polynomials in Several Variables},
	volume = {571},
	year = {1977}
}

@article{Crossman-CoolenSchrijner-Coolen-2009,
	abstract = {This paper concerns discrete-time time-homogeneous
birth-death processes on a finite state space, containing a single absorbing
state, with interval-valued transition probabilities. As absorption is certain,
the quasi-stationary behaviour of the process is studied with the distribution
of the process conditional on non-absorption. It is shown that the set of all
possible limiting conditional distributions is the set of all possible
quasi-stationary distributions. An approximation of the possibly infinite set of
conditional distributions at time n is presented, together with an example.},
	author = {Richard J. Crossman and Pauline Coolen-Schrijner and Frank P. A. Coolen},
	doi = {10.1080/15598608.2009.10411914},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Absorbing state; Birth-death process; Interval probability; Limiting conditional distribu- tion; Time-homogeneity; Quasi-stationary distribution},
	localfile = {article/Crossman-CoolenSchrijner-Coolen-2009.pdf},
	month = mar,
	number = {1},
	pages = {103–118},
	publisher = {Taylor \& Francis},
	title = {Time-Homogeneous Birth-Death Processes with Probability Intervals and Absorbing State},
	volume = {3},
	year = {2009}
}

@article{Smithson-Segale-2009,
	abstract = {On grounds of insufficient reason, a probability of 1/K is
assigned to K mutually exclusive possible events when nothing is known about the
likelihood of those events. Fox and Rottenstreich (2003) present evidence that
subjective probability judgments are typically biased towards this ignorance
prior, and therefore depend on the partition K. Results from two studies
indicate that lower-upper (imprecise) probability judgments by naive judges also
exhibit partition dependence, despite the potential that imprecise probabilities
provide for avoiding it. However, beta regression reveals two kinds of priming
effects, one of which is modeled by mixture distributions. Another novel finding
suggests that when partition primes conflict with a normatively correct
partition some judges widen their probability intervals to encompass both
partitions. The results indicate that imprecise probability judgments may be
better suited than precise probabilities for handling conflicting or ambiguous
information about partitions.},
	author = {Michael Smithson and Carl Segale},
	doi = {10.1080/15598608.2009.10411918},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Imprecise probability; Judgment; Partition; Subjective probability},
	localfile = {article/Smithson-Segale-2009.pdf},
	month = mar,
	number = {1},
	pages = {169–181},
	publisher = {Taylor \& Francis},
	title = {Partition Priming in Judgments of Imprecise Probabilities},
	volume = {3},
	year = {2009}
}

@article{CoolenSchrijner-Maturi-Coolen-2009.pdf,
	abstract = {This paper presents a statistical method for comparison of
two groups based on nonparametric predictive inference (NPI). NPI is a
statistical approach based on few modelling assumptions, with inferences
strongly based on data and uncertainty quantified via lower and upper
probabilities. Life- times of units from groups X and Y are compared, based on
observed lifetimes from an experiment that may have ended before all units
failed. We present upper and lower probabilities for the event that the lifetime
of a future unit from X is less than the lifetime of a future unit from Y, and
we compare this approach with traditional precedence testing.},
	author = {Pauline Coolen-Schrijner and Tahani A. Maturi and Frank P. A. Coolen},
	doi = {10.1080/15598608.2009.10411925},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Lower and upper probabilities; Nonparametric predictive inference; Pairwise comparison; Precedence tests},
	localfile = {article/CoolenSchrijner-Maturi-Coolen-2009.pdf},
	month = mar,
	number = {1},
	pages = {273–287},
	publisher = {Taylor \& Francis},
	title = {Nonparametric predictive precedence testing for two groups},
	volume = {3},
	year = {2009}
}

@article{Bose-2009-smoothness,
	abstract = {We examine the role of the likelihood in Bayesian robustness
with the density ratio class (DeRobertis and Hartigan, 1981. Ann. Stat.) and
show how to impose smoothness on the density ratio class after imposing shape
constraints. We discuss how to impose shape constraints on the density bounded
class (Lavine, 1991. JASA)},
	annote = {doi: 10.1080/15598608.2009.10411911},
	author = {Sudip Bose},
	doi = {10.1080/15598608.2009.10411911},
	issn = {1559-8608},
	journal = {Journal of Statistical Theory and Practice},
	keywords = {Bayesian robustness; Density bounded; Density ratio; Likelihood; Neighborhood class; Smoothness; Symmetry; Unimodality; Convexity; Minimax; Posterior regret; $\Gamma$-minimax},
	localfile = {article/Bose-2009-smoothness.pdf},
	month = mar,
	number = {1},
	pages = {57–67},
	publisher = {Taylor \& Francis},
	title = {On smoothness constraints with shape constraints in a robust Bayesian analysis},
	volume = {3},
	year = {2009}
}

@article{Renooij-VanderGaag-2008-EQPN,
	abstract = {Qualitative probabilistic networks were designed to
overcome, to at least some extent, the quantification problem known to
probabilistic networks. Qualitative networks abstract from the numerical
probabilities of their quantitative counterparts by using signs to summarise the
probabilistic influences between their variables. One of the major drawbacks of
these qualitative abstractions, however, is the coarse level of representation
detail that does not provide for indicating strengths of influences. As a
result, the trade-offs modelled in a network remain unresolved upon inference.
We present an enhanced formalism of qualitative probabilistic networks to
provide for a finer level of representation detail. An enhanced qualitative
probabilistic network differs from a basic qualitative network in that it
distinguishes between strong and weak influences. Now, if a strong influence is
combined, upon inference, with a conflicting weak influence, the sign of the net
influence may be readily determined. Enhanced qualitative networks are purely
qualitative in nature, as basic qualitative networks are, yet allow for
resolving some trade-offs upon inference.},
	author = {Silja Renooij and Linda C. van der Gaag},
	doi = {10.1016/j.artint.2008.04.001},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Probabilistic reasoning; Qualitative reasoning; Trade-off resolution},
	number = {12-13},
	pages = {1470–1494},
	title = {Enhanced qualitative probabilistic networks for resolving trade-offs},
	volume = {172},
	year = {2008}
}

@article{Bolt-VanderGaag-Renooij-2005,
	abstract = {A qualitative probabilistic network is a graphical model of
the probabilistic influences among a set of statistical variables, in which each
influence is associated with a qualitative sign. A non-monotonic influence
between two variables is associated with the ambiguous sign ‘?’, which indicates
that the actual sign of the influence depends on the state of the network. The
presence of such ambiguous signs is undesirable as it tends to lead to
uninformative results upon inference. In this paper, we argue that, although a
non-monotonic influence may have varying effects, in each specific state of the
network, its effect is unambiguous. To capture the current effect of the
influence, we introduce the concept of situational sign. We show how situational
signs can be used upon inference and how they are updated as the state of the
network changes. By means of a real-life qualitative network in oncology, we
show that the use of situational signs can effectively forestall uninformative
results upon inference.},
	author = {Janneke H. Bolt and Linda C. van der Gaag and Silja Renooij},
	doi = {10.1016/j.ijar.2004.05.009},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	number = {3},
	pages = {333–354},
	title = {Introducing situational signs in qualitative probabilistic networks},
	volume = {38},
	year = {2005}
}

@article{Renooij-VanderGaag-Parsons-2002,
	abstract = {Qualitative probabilistic networks are qualitative
abstractions of probabilistic networks, summarising probabilistic influences by
qualitative signs. As qualitative networks model influences at the level of
variables, knowledge about probabilistic influences that hold only for specific
values cannot be expressed. The results computed from a qualitative network, as
a consequence, can be weaker than strictly necessary and may in fact be rather
uninformative. We extend the basic formalism of qualitative probabilistic
networks by providing for the inclusion of context-specific information about
influences and show that exploiting this information upon reasoning has the
ability to forestall unnecessarily weak results.},
	author = {Silja Renooij and Linda C. van der Gaag and Simon Parsons},
	doi = {10.1016/S0004-3702(02)00247-3},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Probabilistic reasoning; Qualitative reasoning; Context-specific independence; Non-monotonicity},
	number = {1-2},
	pages = {207–230},
	title = {Context-specific sign-propagation in qualitative probabilistic networks},
	volume = {140},
	year = {2002}
}

@inproceedings{Renooij-VanderGaag-2002-UAI,
	address = {San Francisco, California},
	author = {Silja Renooij and Linda C. van der Gaag},
	booktitle = {UAI-02: Proceedings of the Eighteenth Conference on Uncertainty in Artificial Intelligence},
	pages = {422–429},
	publisher = {Morgan Kaufmann},
	title = {From qualitative to quantitative probabilistic networks},
	year = {2002}
}

@article{Renooij-VanderGaag-2000-IPMU,
	author = {Silja Renooij and Linda C. van der Gaag},
	booktitle = {IPMU 2000: Proceedings of the Eighth International Conference on Information Processing and Management of Uncertainty in Knowledge-based Systems},
	location = {Madrid},
	pages = {1285–1290},
	title = {Exploiting non-monotonic influences in qualitative belief networks},
	year = {2000}
}

@article{Wellman-1990-QPN,
	abstract = {Graphical representations for probabilistic relationships
have recently received considerable attention in AI. Qualitative probabilistic
networks abstract from the usual numeric representations by encoding only
qualitative relationships, which are inequality constraints on the joint
probability distribution over the variables. Although these constraints are
insufficient to determine probabilities uniquely, they are designed to justify
the deduction of a class of relative likelihood conclusions that imply useful
decision-making properties. Two types of qualitative relationship are defined,
each a probabilistic form of monotonicity constraint over a group of variables.
Qualitative influences describe the direction of the relationship between two
variables. Qualitative synergies describe interactions among influences. The
probabilistic definitions chosen justify sound and efficient inference
procedures based on graphical manipulations of the network. These procedures
answer queries about qualitative relationships among variables separated in the
network and determine structural properties of optimal assignments to decision
variables.},
	author = {Michael P. Wellman},
	doi = {10.1016/0004-3702(90)90026-V},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {3},
	pages = {257–303},
	title = {Fundamental concepts of qualitative probabilistic networks},
	volume = {44},
	year = {1990}
}

@article{Steeneveld-etal-2010,
	abstract = {Automatic milking systems (AMS) generate alert lists
reporting cows likely to have clinical mastitis (CM). Dutch farmers indicated
that they use non-AMS cow information or the detailed alert information from the
AMS to decide whether to check an alerted cow for CM. However, it is not yet
known to what extent such information can be used to discriminate between
true-positive and false-positive alerts. The overall objective was to
investigate whether selection of the alerted cows that need further
investigation for CM can be made. For this purpose, non-AMS cow information and
detailed alert information were used. During a 2-yr study period, 11,156 alerts
for CM, including 159 true-positive alerts, were collected at one farm in the
Netherlands. Non-AMS cow information on parity, days in milk, season of the
year, somatic cell count history, and CM history was added to each alert. In
addition, 6 alert information variables were defined. These were the height of
electrical conductivity, the alert origin (electrical conductivity, color, or
both), whether or not a color alert for mastitic milk was given, whether or not
a color alert for abnormal milk was given, deviation from the expected milk
yield, and the number of alerts of the cow in the preceding 12 to 96\&\#xa0;h.
Subsequently, naive Bayesian networks (NBN) were constructed to compute the
posterior probability of an alert being truly positive based only on non-AMS cow
information, based on only alert information, or based on both types of
information. The NBN including both types of information had the highest area
under the receiver operating characteristic curve (AUC; 0.78), followed by the
NBN including only alert information (AUC\&\#xa0;=\&\#xa0;0.75) and the NBN
including only non-AMS cow information (AUC\&\#xa0;=\&\#xa0;0.62). By combining
the 2 types of information and by setting a threshold on the computed
probabilities, the number of false-positive alerts on a mastitis alert list was
reduced by 35\%, and 10\% of the true-positive alerts would not be identified.
To detect CM cases at a farm with an AMS, checking all alerts is still the best
option but would result in a high workload. Checking alerts based on a single
alert information variable would result in missing too many true-positive cases.
Using a combination of alert information variables, however, is the best way to
select cows that need further investigation. The effect of adding non-AMS cow
information on making a distinction between true-positive and false-positive
alerts would be minor.},
	author = {W. Steeneveld and Linda C. van der Gaag and W. Ouweltjes and H. Mollenhorst and H. Hogeveen},
	doi = {10.3168/jds.2009-3020},
	issn = {0022-0302},
	journal = {Journal of Dairy Science},
	keywords = {clinical mastitis; detection; automatic milking; dairy cow},
	number = {6},
	pages = {2559–2568},
	title = {Discriminating between true-positive and false-positive clinical mastitis alerts from automatic milking systems},
	volume = {93},
	year = {2010}
}

@inproceedings{TabachneckSchijf-etal-2008,
	author = {Hermi J. M. Tabachneck-Schijf and Linda C. van der Gaag and Petra L. Geenen and Martijn M. Schrage and W. L. A. Loeffen and A. R. W. Elbers},
	booktitle = {Proceedings of the 20th International Pig Veterinary Science Congress},
	editor = {P. Evans},
	location = {Durban, South Africa},
	title = {Designing a personal digital assistant for early on-site detection of classical swine fever in a pig unit},
	year = {2008}
}

@inproceedings{Geenen-VanderGaag-2005,
	author = {Petra L. Geenen and Linda C. van der Gaag},
	booktitle = {Proceedings of the Third Bayesian Modelling Applications Workshop},
	location = {Edinburgh},
	title = {Developing a Bayesian network for clinical diagnosis in veterinary medicine: from the individual to the herd},
	year = {2005}
}

@article{Grunwald-Halpern-2011,
	author = {Peter D. Grünwald and Joseph Y. Halpern},
	doi = {10.1613/jair.3374},
	journal = {Journal of Artificial Intelligence Research},
	pages = {393–426},
	title = {Making decisions using sets of probabilities: updating, time consistency, and calibration.},
	volume = 42,
	year = 2011
}

@book{Pourret-Naim-Marcot-2008,
	editor = {Olivier Pourret and Patrick Naïm and Bruce Marcot},
	isbn = {978-0-470-06030-8},
	publisher = {Wiley},
	title = {Bayesian networks: a practical guide to applications},
	year = {2008}
}

@inproceedings{VanderGaag-1990,
	abstract = {Many AI researchers argue that probability theory is only
capable of dealing with uncertainty in situations where a full specification of
a joint probability distribution is available, and conclude that it is not
suitable for application in knowledge-based systems. Probability intervals,
however, constitute a means for expressing incompleteness of information. We
present a method for computing such probability intervals for probabilities of
interest from a partial specification of a joint probability distribution. Our
method improves on earlier approaches by allowing for independency relationships
between statistical variables to be exploited.},
	address = {Amsterdam, the Netherlands},
	author = {Linda C. van der Gaag},
	booktitle = {UAI-90: Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence},
	pages = {457–466},
	publisher = {Elsevier},
	title = {Computing Probability Intervals Under Independency Constraints},
	year = {1990}
}

@inproceedings{VanderGaag-etal-1999,
	abstract = {In building Bayesian belief networks, the elicitation of all
probabilities required can be a major obstacle. We learned the extent of this
often-cited observation in the construction of the probabilistic part of a
complex influence diagram in the field of cancer treatment. Based upon our
negative experiences with existing methods, we designed a new method for
probability elicitation from domain experts. The method combines various ideas,
among which are the ideas of transcribing probabilities and of using a scale
with both numerical and verbal anchors for marking assessments. In the
construction of the probabilistic part of our influence diagram, the method
proved to allow for the elicitation of many probabilities in little time.},
	address = {San Francisco, California},
	author = {Linda C. van der Gaag and Silja Renooij and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
	booktitle = {UAI-99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence},
	keywords = {belief networks; expert elicitation},
	pages = {647–654},
	publisher = {Morgan Kaufmann},
	title = {How to elicit many probabilities},
	year = {1999}
}

@article{VanderGaag-etal-2002,
	abstract = {With the help of two experts in gastrointestinal oncology
from The Netherlands Cancer Institute, Antoni van Leeuwenhoekhuis, a
decision-support system is being developed for patient-specific therapy
selection for oesophageal cancer. The kernel of the system is a probabilistic
network that describes the presentation characteristics of cancer of the
oesophagus and the pathophysiological processes of invasion and metastasis.
While the construction of the graphical structure of the network was relatively
straightforward, probability elicitation with existing methods proved to be a
major obstacle. To overcome this obstacle, we designed a new method for
eliciting probabilities from experts that combines the ideas of transcribing
probabilities as fragments of text and of using a scale with both numerical and
verbal anchors for marking assessments. In this paper, we report experiences
with our method in eliciting the probabilities required for the oesophagus
network. The method allowed us to elicit many probabilities in reasonable time.
To gain some insight in the quality of the probabilities obtained, we conducted
a preliminary evaluation study of our network, using data from real patients. We
found that for 85\% of the patients, the network predicted the correct cancer
stage.},
	author = {Linda C. van der Gaag and Silja Renooij and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
	doi = {10.1016/S0933-3657(02)00012-X},
	issn = {0933-3657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Elicitation of judgemental probabilities; Probabilistic networks},
	number = {2},
	pages = {123–148},
	title = {Probabilities for a probabilistic network: a case study in oesophageal cancer},
	volume = {25},
	year = {2002}
}

@inproceedings{Helsper-VanderGaag-Groenendaal-2004,
	author = {E. Helsper and Linda C. van der Gaag and F. Groenendaal},
	booktitle = {Engineering Knowledge in the Age of the Semantic Web},
	editor = {E. Motta and N. R. Shadbolt and A. Stutt and N. Gibbins},
	pages = {280–292},
	publisher = {Springer},
	title = {Designing a procedure for the acquisition of probability constraints for Bayesian networks},
	year = {2004}
}

@article{Sent-etal-2005,
	author = {D. Sent and Linda C. van der Gaag and Cilia L. M. Witteman and Berthe M. P. Aleman and Babs G. Taal},
	journal = {Interdisciplinary Journal of Artificial Intelligence and the Simulation of Behaviour},
	number = {6},
	pages = {543–561},
	title = {Eliciting test-selection strategies for a decision-support system in oncology},
	volume = {1},
	year = {2005}
}

@inproceedings{Geenen-etal-2006,
	author = {Petra L. Geenen and A. R. W. Elbers and Linda C. van der Gaag and W. L. A. Loeffen},
	booktitle = {Proceedings of the Eleventh Symposium of the International Society for Veterinary Epidemiology and Economics},
	pages = {667–669},
	title = {Development of a probabilistic network for clinical detection of classical swine fever},
	year = {2006}
}

@article{Laskey-1995,
	abstract = {When eliciting a probability model from experts, knowledge
engineers may compare the results of the model with expert judgment on test
scenarios, then adjust model parameters to bring the behavior of the model more
in line with the experts intuition. This paper presents a methodology for
analytic computation of sensitivity values in Bayesian network models.
Sensitivity values are partial derivatives of output probabilities with respect
to parameters being varied in the sensitivity analysis. They measure the impact
of small changes in a network parameter on a target probability value or
distribution. Sensitivity values can be used to focus knowledge elicitation
effort on those parameters having the most impact on outputs of concern.
Analytic sensitivity values are computed for an example and compared to
sensitivity analysis by direct variation of parameters},
	author = {K. B. Laskey},
	doi = {10.1109/21.384252},
	issn = {0018-9472},
	journal = {IEEE Transactions on Systems, Man and Cybernetics},
	keywords = {Bayesian networks; knowledge elicitation; knowledge engineering; probability assessments; sensitivity analysis; symbolic reasoning; target probability value; uncertainty representation; Bayes methods; inference mechanisms; knowledge acquisition; probability; sensitivity analysis},
	month = jun,
	number = {6},
	pages = {901–909},
	title = {Sensitivity analysis for probability assessments in Bayesian networks},
	volume = {25},
	year = {1995}
}

@inproceedings{Chan-Darwiche-2004,
	abstract = {Previous work on sensitivity analysis in Bayesian networks
has focused on single parameters, where the goal is to understand the
sensitivity of queries to single parameter changes, and to identify single
parameter changes that would enforce a certain query constraint. In this paper,
we expand the work to multiple parameters which may be in the CPT of a single
variable, or the CPTs of multiple variables. Not only do we identify the
solution space of multiple parameter changes that would be needed to enforce a
query constraint, but we also show how to find the optimal solution, that is,
the one which disturbs the current probability distribution the least (with
respect to a specific measure of disturbance). We characterize the computational
complexity of our new techniques and discuss their applications to developing
and debugging Bayesian networks, and to the problem of reasoning about the value
(reliability) of new information.},
	address = {Arlington, Virginia},
	author = {Hei Chan and Adnan Darwiche},
	booktitle = {UAI-04: Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence},
	pages = {67–75},
	publisher = {AUAI Press},
	title = {Sensitivity analysis in Bayesian networks: from single to multiple parameters},
	year = {2004}
}

@misc{BNatWork,
	author = {BN@Work},
	title = {European Community for Researchers on Probabilistic Graphical Models},
	url = {http://www.bnatwork.org}
}

@misc{SIKS,
	author = {SIKS},
	title = {Netherlands research school for Information and Knowledge Systems},
	url = {http://www.siks.nl}
}

@misc{AUAI,
	author = {AUAI},
	title = {Association for Uncertainty in Artificial Intelligence},
	url = {http://www.auai.org}
}

@misc{ECCAI,
	author = {ECCAI},
	title = {European Coordinating Committee for Artificial Intelligence},
	url = {http://www.eccai.org}
}

@misc{SIPTA,
	author = {SIPTA},
	title = {Society for Imprecise Probability: Theories and Applications},
	url = {http://www.sipta.org}
}

@inproceedings{Blanco-etal-2004,
	author = {Rosa Blanco and Linda C. van der Gaag and Iñaki Inza and Pedro Larrañaga},
	booktitle = {ISBMDA 2004: Proceedings of the 5th International Symposium on Biological and Medical Data Analysis},
	editor = {José María Barreiro and Fernando Martín-Sánchez and Victor Maojo and Ferran Sanz},
	isbn = {3-540-23964-2},
	pages = {212–223},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Selective classifiers can be too restrictive: a case-study in oesophageal cancer},
	volume = {3337},
	year = {2004}
}

@article{Feelders-VanderGaag-2006,
	abstract = {We consider the problem of learning the parameters of a
Bayesian network from data, while taking into account prior knowledge about the
signs of influences between variables. Such prior knowledge can be readily
obtained from domain experts. We show that this problem of parameter learning is
a special case of isotonic regression and provide a simple algorithm for
computing isotonic estimates. Our experimental results for a small Bayesian
network in the medical domain show that taking prior knowledge about the signs
of influences into account leads to an improved fit of the true distribution,
especially when only a small sample of data is available. More importantly,
however, the isotonic estimator provides parameter estimates that are consistent
with the specified prior knowledge, thereby resulting in a network that is more
likely to be accepted by experts in its domain of application.},
	author = {Ad Feelders and Linda C. van der Gaag},
	doi = {10.1016/j.ijar.2005.10.003},
	issn = {0888-613X},
	journal = {International Journal of Approximate Reasoning},
	keywords = {Bayesian networks; Parameter learning; Order-constrained estimation},
	number = {1-2},
	pages = {37–53},
	title = {Learning Bayesian network parameters under order constraints},
	volume = {42},
	year = {2006}
}

@inproceedings{VanderGaag-etal-2009,
	author = {Linda C. van der Gaag and Silja Renooij and Ad Feelders and Arend de Groote and Marinus J. C. Eijkemans and Frank J. Broekmans and Bart C. J. M. Fauser},
	booktitle = {Machine Learning and Data Mining in Pattern Recognition},
	doi = {10.1007/978-3-642-03070-3_59},
	editor = {Petra Perner},
	isbn = {978-3-642-03069-7},
	pages = {787–801},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Aligning Bayesian Network Classifiers with Medical Contexts.},
	volume = 5632,
	year = 2009
}

@incollection{Chateauneuf-Jaffray-1995,
	abstract = {The concept of local Möbius transform of a capacity is
introduced and shown to provide a handier characterization of K-monotonicity
than the standard Möbius transformation. It is moreover used to give a new proof
of the preservation of K monotonicity by conditional lower probabilities.},
	author = {Alain Chateauneuf and Jean-Yves Jaffray},
	booktitle = {Symbolic and Quantitative Approaches to Reasoning and Uncertainty},
	doi = {10.1007/3-540-60112-0_14},
	editor = {Christine Froidevaux and Jürg Kohlas},
	isbn = {978-3-540-60112-8},
	localfile = {inbook/Chateauneuf-Jaffray-1995.pdf},
	number = {1},
	pages = {115–124},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Local Möbius transforms of monotone capacities},
	volume = {946},
	year = {1995}
}

@inproceedings{Quaeghebeur-Shariatmadar-DeCooman-2010-FLINS,
	abstract = {We investigate a constrained optimization problem for which
there is uncertainty about a constraint parameter. Our aim is to reformulate it
as a (constrained) optimization problem without uncertainty. This is done by
recasting the original problem as a decision problem under uncertainty. We give
results for a number of different types of uncertainty models—linear and vacuous
previsions, and possibility distributions—and for two different optimality
criteria for decision problems under uncertainty—maximinity and maximality.},
	address = {Singapore},
	author = {Erik Quaeghebeur and Keivan Shariatmadar and Gert {De Cooman}},
	booktitle = {Computational intelligence: foundations and applications: proceedings of the 9th international FLINS conference},
	editor = {Da Ruan and Yianrui Li and Yang Xu and Guoqing Chen and Etienne E. Kerre},
	keywords = {possibility distribution; linear prevision; maximinity; maximality; vacuous prevision; constrained optimization},
	location = {Chengdu, China},
	pages = {791–796},
	publisher = {World Scientific},
	series = {World Scientific Proceedings Series on Computer Engineering and Information Science},
	title = {A constrained optimization problem under uncertainty},
	year = {2010}
}

@article{Quaeghebeur-Shariatmadar-DeCooman-2012-FSS,
	author = {Erik Quaeghebeur and Keivan Shariatmadar and Gert {De Cooman}},
	journal = {Fuzzy Sets and Systems},
	title = {Constrained optimization problems under uncertainty with coherent lower previsions},
	year = {submitted}
}

@article{Charnes-Cooper-1959,
	abstract = {A new conceptual and analytical vehicle for problems of
temporal planning under uncertainty, involving determination of optimal
(sequential) stochastic decision rules is defined and illustrated by means of a
typical industrial example. The paper presents a method of attack which splits
the problem into two non-linear (or linear) programming parts, (i) determining
optimal probability distributions, (ii) approximating the optimal distributions
as closely as possible by decision rules of prescribed form.},
	author = {A. Charnes and W. W. Cooper},
	doi = {10.1287/mnsc.6.1.73},
	issn = {0025-1909},
	journal = {Management Science},
	localfile = {article/Charnes-Cooper-1959.pdf},
	month = {oct},
	number = {1},
	pages = {73–79},
	publisher = {INFORMS},
	title = {Chance-constrained programming},
	url = {http://www.jstor.org/stable/2627476},
	volume = {6},
	year = {1959}
}

@article{Dantzig-1955,
	author = {George B. Dantzig},
	doi = {10.1287/mnsc.1.3-4.197},
	issn = {0025-1909},
	journal = {Management Science},
	localfile = {article/Dantzig-1955.pdf},
	number = {3/4},
	pages = {197–206},
	publisher = {INFORMS},
	title = {Linear Programming under Uncertainty},
	url = {http://www.jstor.org/stable/2627159},
	volume = {1},
	year = {1955}
}

@article{Yu-Zeleny-1975,
	abstract = {In this note we are interested in the properties of, and
methods for locating the set of all nondominated solutions of multiple linear
criteria defined over a polyhedron. We first show that the set of all dominated
solutions is convex and that the set of all nondominated solutions is a subset
of the convex hull of the nondominated extreme points. When the domination cone
is polyhedral, we derive a necessary and sufficient condition for a point to be
nondominated. The condition is stronger than that of Ref. [1] and enables us to
give a simple proof that the set of all nondominated extreme points indeed is
connected. In order to locate the entire set of all nondominated extreme points,
we derive a generalized version of simplex method—multicriteria simplex method.
In addition to some useful results, a necessary and sufficient condition for an
extreme point to be nondominated is derived. Examples and computer experience
are also given. Finally, we focus on how to generate the entire set of all
nondominated solutions through the set of all nondominated extreme points. A
decomposition theorem and some necessary and sufficient conditions for a face to
be nondominated are derived. We then describe a systematic way to identify the
entire set of all nondominated solutions. Through examples, we show that in fact
our procedure is quite efficient.},
	author = {P. L. Yu and M. Zeleny},
	doi = {10.1016/0022-247X(75)90189-4},
	issn = {0022-247X},
	journal = {Journal of Mathematical Analysis and Applications},
	number = {2},
	pages = {430–468},
	title = {The set of all nondominated solutions in linear cases and a multicriteria simplex method},
	volume = {49},
	year = {1975}
}

@article{Evans-Steuer-1973,
	abstract = {For linear multiple-objective problems, a necessary and
sufficient condition for a point to be efficient is employed in the development
of a revised simplex algorithm for the enumeration of the set of efficient
extreme points. Five options within this algorithm were tested on a variety of
problems. Results of these tests provide indications for effective use of the
algorithm.},
	author = {J. P. Evans and R. E. Steuer},
	doi = {10.1007/BF01580111},
	issn = {0025-5610},
	journal = {Mathematical Programming},
	pages = {54–72},
	publisher = {Springer Berlin / Heidelberg},
	title = {A revised simplex method for linear multiple objective programs},
	volume = {5},
	year = {1973}
}

@book{Dubois-Prade-1988-en,
	address = {New York},
	author = {Didier Dubois and Henri Prade},
	publisher = {Plenum Press},
	title = {Possibility Theory: An Approach to Computerized Processing of Uncertainty},
	year = {1988}
}

@inproceedings{Huntley-etal-2012-FLINS,
	abstract = {We present a software implementation of the methods for
solving linear programming problems under uncertainty from previous work.
Uncertainties about constraint parameters can be expressed as intervals or
trapezoidal possibility distributions. The software computes the solutions for
the optimality criteria maximin and maximality. For maximality with possibility
distributions, only an approximate solution is obtained.},
	author = {Nathan Huntley and Rolando Quiñones and Keivan Shariatmadar and Erik Quaeghebeur and Gert {De Cooman} and Etienne E. Kerre},
	keywords = {linear programming; uncertainty; maximin; maximality; implementation},
	note = {Submitted to FLINS 2012},
	title = {Implementation of maximin and maximal solutions for linear programming problems under uncertainty},
	year = {2012}
}

@inproceedings{Quaeghebeur-etal-2012-IPMU,
	abstract = {We consider linear programming problems with uncertain
constraint coefficients described by intervals or, more generally, possibility
distributions. The uncertainty is given a behavioral interpretation using
coherent lower previsions from the theory of imprecise probabilities. We give a
meaning to the linear programming problems by reformulating them as decision
problems under such imprecise-probabilistic uncertainty. We provide expressions
for and illustrations of the maximin and maximal solutions of these decision
problems and present computational approaches for dealing with them.},
	author = {Erik Quaeghebeur and Nathan Huntley and Keivan Shariatmadar and Gert {De Cooman}},
	keywords = {linear program; interval uncertainty; vacuous lower prevision; possibility distribution; coherent lower prevision; imprecise probabilities; decision making; maximinity; maximality},
	note = {Submitted to IPMU 2012},
	title = {Maximin and Maximal Solutions for Linear Programming Problems with Possibilistic Uncertainty},
	year = {2012}
}

@book{Dantzig-Thapa-2003,
	author = {George B. Dantzig and Mukund N. Thapa},
	booktitle = {Linear Programming:2: Theory and Extensions},
	doi = {10.1007/b97283},
	localfile = {book/Dantzig-Thapa-2003.pdf},
	publisher = {Springer},
	series = {Springer Series in Operations Research},
	year = {2003}
}

@inproceedings{Bagnara-etal-2002,
	author = {Roberto Bagnara and Elisa Ricci and Enea Zaffanella and Patricia M. Hill},
	booktitle = {Static Analysis: Proceedings of the 9th International Symposium},
	doi = {10.1007/3-540-45789-5_17},
	editor = {Manuel V. Hermenegildo and Germán Puebla},
	isbn = {3-540-44235-9},
	localfile = {inproceedings/Bagnara-etal-2002.pdf},
	pages = {213–229},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	title = {Possibly Not Closed Convex Polyhedra and the Parma Polyhedra Library.},
	url = {http://bugseng.com/products/ppl/documentation/BagnaraRZH02.pdf},
	volume = 2477,
	year = 2002
}

